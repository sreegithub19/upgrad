{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "\n",
    "### Upgrad notebooks:\n",
    "1. [Intro](#Intro)<br>\n",
    "2. [Exercise_1](#Exercise_1)<br>\n",
    "3. [Case_study_Cricket_tournament](#Case_study_Cricket_tournament)<br>\n",
    "4. [Creating_numpy_arrays](#Creating_numpy_arrays)<br>\n",
    "5. [Operations_on_NumPy_Arrays](#Operations_on_NumPy_Arrays)<br>\n",
    "6. [Compare_Computation_Times_in_NumPy_and_Standard_Python_Lists](#Compare_Computation_Times_in_NumPy_and_Standard_Python_Lists)<br>\n",
    "7. [Numpy_practice_Udemy](#Numpy_practice_Udemy)<br>\n",
    "7. [Numpy_practice](#Numpy_practice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upgrad notebooks:\n",
    "<a id='Intro'>Intro</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6qb81h2NYHb"
   },
   "source": [
    "<h2 style = \"color : Brown\"> Case Study - Cricket Tournament </h2>\n",
    "\n",
    "A panel wants to select players for an upcoming league match based on their fitness. Players from all significant cricket clubs have participated in a practice match, and their data is collected. Let us now explore NumPy features using the player's data.</font>\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 1</h4>  \n",
    "\n",
    "#### Heights of the players is stored as a regular Python list: height_in. The height is expressed in inches. Can you make a numpy array out of it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syM7L-HrNYHe"
   },
   "outputs": [],
   "source": [
    "# Define list\n",
    "heights = [74, 74, 72, 72, 73, 69, 69, 71, 76, 71, 73, 73, 74, 74, 69, 70, 73, 75, 78, 79, 76, 74, 76, 72, 71, 75, 77, 74, 73, 74, 78, 73, 75, 73, 75, 75, 74, 69, 71, 74, 73, 73, 76, 74, 74, 70, 72, 77, 74, 70, 73, 75, 76, 76, 78, 74, 74, 76, 77, 81, 78, 75, 77, 75, 76, 74, 72, 72, 75, 73, 73, 73, 70, 70, 70, 76, 68, 71, 72, 75, 75, 75, 75, 68, 74, 78, 71, 73, 76, 74, 74, 79, 75, 73, 76, 74, 74, 73, 72, 74, 73, 74, 72, 73, 69, 72, 73, 75, 75, 73, 72, 72, 76, 74, 72, 77, 74, 77, 75, 76, 80, 74, 74, 75, 78, 73, 73, 74, 75, 76, 71, 73, 74, 76, 76, 74, 73, 74, 70, 72, 73, 73, 73, 73, 71, 74, 74, 72, 74, 71, 74, 73, 75, 75, 79, 73, 75, 76, 74, 76, 78, 74, 76, 72, 74, 76, 74, 75, 78, 75, 72, 74, 72, 74, 70, 71, 70, 75, 71, 71, 73, 72, 71, 73, 72, 75, 74, 74, 75, 73, 77, 73, 76, 75, 74, 76, 75, 73, 71, 76, 75, 72, 71, 77, 73, 74, 71, 72, 74, 75, 73, 72, 75, 75, 74, 72, 74, 71, 70, 74, 77, 77, 75, 75, 78, 75, 76, 73, 75, 75, 79, 77, 76, 71, 75, 74, 69, 71, 76, 72, 72, 70, 72, 73, 71, 72, 71, 73, 72, 73, 74, 74, 72, 75, 74, 74, 77, 75, 73, 72, 71, 74, 77, 75, 75, 75, 78, 78, 74, 76, 78, 76, 70, 72, 80, 74, 74, 71, 70, 72, 71, 74, 71, 72, 71, 74, 69, 76, 75, 75, 76, 73, 76, 73, 77, 73, 72, 72, 77, 77, 71, 74, 74, 73, 78, 75, 73, 70, 74, 72, 73, 73, 75, 75, 74, 76, 73, 74, 75, 75, 72, 73, 73, 72, 74, 78, 76, 73, 74, 75, 70, 75, 71, 72, 78, 75, 73, 73, 71, 75, 77, 72, 69, 73, 74, 72, 70, 75, 70, 72, 72, 74, 73, 74, 76, 75, 80, 72, 75, 73, 74, 74, 73, 75, 75, 71, 73, 75, 74, 74, 72, 74, 74, 74, 73, 76, 75, 72, 73, 73, 73, 72, 72, 72, 72, 71, 75, 75, 74, 73, 75, 79, 74, 76, 73, 74, 74, 72, 74, 74, 75, 78, 74, 74, 74, 77, 70, 73, 74, 73, 71, 75, 71, 72, 77, 74, 70, 77, 73, 72, 76, 71, 76, 78, 75, 73, 78, 74, 79, 75, 76, 72, 75, 75, 70, 72, 70, 74, 71, 76, 73, 76, 71, 69, 72, 72, 69, 73, 69, 73, 74, 74, 72, 71, 72, 72, 76, 76, 76, 74, 76, 75, 71, 72, 71, 73, 75, 76, 75, 71, 75, 74, 72, 73, 73, 73, 73, 76, 72, 76, 73, 73, 73, 75, 75, 77, 73, 72, 75, 70, 74, 72, 80, 71, 71, 74, 74, 73, 75, 76, 73, 77, 72, 73, 77, 76, 71, 75, 73, 74, 77, 71, 72, 73, 69, 73, 70, 74, 76, 73, 73, 75, 73, 79, 74, 73, 74, 77, 75, 74, 73, 77, 73, 77, 74, 74, 73, 77, 74, 77, 75, 77, 75, 71, 74, 70, 79, 72, 72, 70, 74, 74, 72, 73, 72, 74, 74, 76, 82, 74, 74, 70, 73, 73, 74, 77, 72, 76, 73, 73, 72, 74, 74, 71, 72, 75, 74, 74, 77, 70, 71, 73, 76, 71, 75, 74, 72, 76, 79, 76, 73, 76, 78, 75, 76, 72, 72, 73, 73, 75, 71, 76, 70, 75, 74, 75, 73, 71, 71, 72, 73, 73, 72, 69, 73, 78, 71, 73, 75, 76, 70, 74, 77, 75, 79, 72, 77, 73, 75, 75, 75, 73, 73, 76, 77, 75, 70, 71, 71, 75, 74, 69, 70, 75, 72, 75, 73, 72, 72, 72, 76, 75, 74, 69, 73, 72, 72, 75, 77, 76, 80, 77, 76, 79, 71, 75, 73, 76, 77, 73, 76, 70, 75, 73, 75, 70, 69, 71, 72, 72, 73, 70, 70, 73, 76, 75, 72, 73, 79, 71, 72, 74, 74, 74, 72, 76, 76, 72, 72, 71, 72, 72, 70, 77, 74, 72, 76, 71, 76, 71, 73, 70, 73, 73, 72, 71, 71, 71, 72, 72, 74, 74, 74, 71, 72, 75, 72, 71, 72, 72, 72, 72, 74, 74, 77, 75, 73, 75, 73, 76, 72, 77, 75, 72, 71, 71, 75, 72, 73, 73, 71, 70, 75, 71, 76, 73, 68, 71, 72, 74, 77, 72, 76, 78, 81, 72, 73, 76, 72, 72, 74, 76, 73, 76, 75, 70, 71, 74, 72, 73, 76, 76, 73, 71, 68, 71, 71, 74, 77, 69, 72, 76, 75, 76, 75, 76, 72, 74, 76, 74, 72, 75, 78, 77, 70, 72, 79, 74, 71, 68, 77, 75, 71, 72, 70, 72, 72, 73, 72, 74, 72, 72, 75, 72, 73, 74, 72, 78, 75, 72, 74, 75, 75, 76, 74, 74, 73, 74, 71, 74, 75, 76, 74, 76, 76, 73, 75, 75, 74, 68, 72, 75, 71, 70, 72, 73, 72, 75, 74, 70, 76, 71, 82, 72, 73, 74, 71, 75, 77, 72, 74, 72, 73, 78, 77, 73, 73, 73, 73, 73, 76, 75, 70, 73, 72, 73, 75, 74, 73, 73, 76, 73, 75, 70, 77, 72, 77, 74, 75, 75, 75, 75, 72, 74, 71, 76, 71, 75, 76, 83, 75, 74, 76, 72, 72, 75, 75, 72, 77, 73, 72, 70, 74, 72, 74, 72, 71, 70, 71, 76, 74, 76, 74, 74, 74, 75, 75, 71, 71, 74, 77, 71, 74, 75, 77, 76, 74, 76, 72, 71, 72, 75, 73, 68, 72, 69, 73, 73, 75, 70, 70, 74, 75, 74, 74, 73, 74, 75, 77, 73, 74, 76, 74, 75, 73, 76, 78, 75, 73, 77, 74, 72, 74, 72, 71, 73, 75, 73, 67, 67, 76, 74, 73, 70, 75, 70, 72, 77, 79, 78, 74, 75, 75, 78, 76, 75, 69, 75, 72, 75, 73, 74, 75, 75, 73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuS3VSD-NYHr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "heights_in = np.array(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2l7qL67NYH2",
    "outputId": "a69b3b52-e221-4970-82f8-43ba3074d2ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74, 74, 72, ..., 75, 75, 73])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-__JqvMaNYIC",
    "outputId": "2d122803-98a9-4e9c-f401-1aca4f0df8c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(heights_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ANNW-OJNYIM"
   },
   "source": [
    "<h4 style = \"color : Sky blue\"> Example - 2</h4>\n",
    "\n",
    "#### Count the number of pariticipants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPtkkx_kNYIO",
    "outputId": "0501f98b-3038-4764-99e2-e3e7b01145d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utC2FQZnNYIb",
    "outputId": "01d007f7-71d4-48d4-ecf6-58c7186ba93c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHlC4G1_NYIk",
    "outputId": "bfb7e252-0792-41c6-c85d-56a748136ca3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_ykFKOrNYIv"
   },
   "source": [
    "<h4 style = \"color : Sky blue\"> Example - 3</h4>\n",
    "\n",
    "####  Convert the heights from inches to meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFh-kO3jNYIx",
    "outputId": "8553f4db-3f0e-4a21-ff9f-0c9a61325975"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8796, 1.8796, 1.8288, ..., 1.905 , 1.905 , 1.8542])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights_m = heights * 0.0254\n",
    "\n",
    "heights_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5BbAFY1NYI6"
   },
   "source": [
    "<h4 style = \"color : Sky blue\"> Example - 4</h4>\n",
    "\n",
    "#### A list of weights (in lbs) of the players is provided. Convert it to kg and calculate BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WB52h6LbNYI7"
   },
   "outputs": [],
   "source": [
    "weights_lb = [180, 215, 210, 210, 188, 176, 209, 200, 231, 180, 188, 180, 185, 160, 180, 185, 189, 185, 219, 230, 205, 230, 195, 180, 192, 225, 203, 195, 182, 188, 200, 180, 200, 200, 245, 240, 215, 185, 175, 199, 200, 215, 200, 205, 206, 186, 188, 220, 210, 195, 200, 200, 212, 224, 210, 205, 220, 195, 200, 260, 228, 270, 200, 210, 190, 220, 180, 205, 210, 220, 211, 200, 180, 190, 170, 230, 155, 185, 185, 200, 225, 225, 220, 160, 205, 235, 250, 210, 190, 160, 200, 205, 222, 195, 205, 220, 220, 170, 185, 195, 220, 230, 180, 220, 180, 180, 170, 210, 215, 200, 213, 180, 192, 235, 185, 235, 210, 222, 210, 230, 220, 180, 190, 200, 210, 194, 180, 190, 240, 200, 198, 200, 195, 210, 220, 190, 210, 225, 180, 185, 170, 185, 185, 180, 178, 175, 200, 204, 211, 190, 210, 190, 190, 185, 290, 175, 185, 200, 220, 170, 220, 190, 220, 205, 200, 250, 225, 215, 210, 215, 195, 200, 194, 220, 180, 180, 170, 195, 180, 170, 206, 205, 200, 225, 201, 225, 233, 180, 225, 180, 220, 180, 237, 215, 190, 235, 190, 180, 165, 195, 200, 190, 190, 185, 185, 205, 190, 205, 206, 220, 208, 170, 195, 210, 190, 211, 230, 170, 185, 185, 241, 225, 210, 175, 230, 200, 215, 198, 226, 278, 215, 230, 240, 184, 219, 170, 218, 190, 225, 220, 176, 190, 197, 204, 167, 180, 195, 220, 215, 185, 190, 205, 205, 200, 210, 215, 200, 205, 211, 190, 208, 200, 210, 232, 230, 210, 220, 210, 202, 212, 225, 170, 190, 200, 237, 220, 170, 193, 190, 150, 220, 200, 190, 185, 185, 200, 172, 220, 225, 190, 195, 219, 190, 197, 200, 195, 210, 177, 220, 235, 180, 195, 195, 190, 230, 190, 200, 190, 190, 200, 200, 184, 200, 180, 219, 187, 200, 220, 205, 190, 170, 160, 215, 175, 205, 200, 214, 200, 190, 180, 205, 220, 190, 215, 235, 191, 200, 181, 200, 210, 240, 185, 165, 190, 185, 175, 155, 210, 170, 175, 220, 210, 205, 200, 205, 195, 240, 150, 200, 215, 202, 200, 190, 205, 190, 160, 215, 185, 200, 190, 210, 185, 220, 190, 202, 205, 220, 175, 160, 190, 200, 229, 206, 220, 180, 195, 175, 188, 230, 190, 200, 190, 219, 235, 180, 180, 180, 200, 234, 185, 220, 223, 200, 210, 200, 210, 190, 177, 227, 180, 195, 199, 175, 185, 240, 210, 180, 194, 225, 180, 205, 193, 230, 230, 220, 200, 249, 190, 208, 245, 250, 160, 192, 220, 170, 197, 155, 190, 200, 220, 210, 228, 190, 160, 184, 180, 180, 200, 176, 160, 222, 211, 195, 200, 175, 206, 240, 185, 260, 185, 221, 205, 200, 170, 201, 205, 185, 205, 245, 220, 210, 220, 185, 175, 170, 180, 200, 210, 175, 220, 206, 180, 210, 195, 200, 200, 164, 180, 220, 195, 205, 170, 240, 210, 195, 200, 205, 192, 190, 170, 240, 200, 205, 175, 250, 220, 224, 210, 195, 180, 245, 175, 180, 215, 175, 180, 195, 230, 230, 205, 215, 195, 180, 205, 180, 190, 180, 190, 190, 220, 210, 255, 190, 230, 200, 205, 210, 225, 215, 220, 205, 200, 220, 197, 225, 187, 245, 185, 185, 175, 200, 180, 188, 225, 200, 210, 245, 213, 231, 165, 228, 210, 250, 191, 190, 200, 215, 254, 232, 180, 215, 220, 180, 200, 170, 195, 210, 200, 220, 165, 180, 200, 200, 170, 224, 220, 180, 198, 240, 239, 185, 210, 220, 200, 195, 220, 230, 170, 220, 230, 165, 205, 192, 210, 205, 200, 210, 185, 195, 202, 205, 195, 180, 200, 185, 240, 185, 220, 205, 205, 180, 201, 190, 208, 240, 180, 230, 195, 215, 190, 195, 215, 215, 220, 220, 230, 195, 190, 195, 209, 204, 170, 185, 205, 175, 210, 190, 180, 180, 160, 235, 200, 210, 180, 190, 197, 203, 205, 170, 200, 250, 200, 220, 200, 190, 170, 190, 220, 215, 206, 215, 185, 235, 188, 230, 195, 168, 190, 160, 200, 200, 189, 180, 190, 200, 220, 187, 240, 190, 180, 185, 210, 220, 219, 190, 193, 175, 180, 215, 210, 200, 190, 185, 220, 170, 195, 205, 195, 210, 190, 190, 180, 220, 190, 186, 185, 190, 180, 190, 170, 210, 240, 220, 180, 210, 210, 195, 160, 180, 205, 200, 185, 245, 190, 210, 200, 200, 222, 215, 240, 170, 220, 156, 190, 202, 221, 200, 190, 210, 190, 200, 165, 190, 185, 230, 208, 209, 175, 180, 200, 205, 200, 250, 210, 230, 244, 202, 240, 200, 215, 177, 210, 170, 215, 217, 198, 200, 220, 170, 200, 230, 231, 183, 192, 167, 190, 180, 180, 215, 160, 205, 223, 175, 170, 190, 240, 175, 230, 223, 196, 167, 195, 190, 250, 190, 190, 190, 170, 160, 150, 225, 220, 209, 210, 176, 260, 195, 190, 184, 180, 195, 195, 219, 225, 212, 202, 185, 200, 209, 200, 195, 228, 210, 190, 212, 190, 218, 220, 190, 235, 210, 200, 188, 210, 235, 188, 215, 216, 220, 180, 185, 200, 210, 220, 185, 231, 210, 195, 200, 205, 200, 190, 250, 185, 180, 170, 180, 208, 235, 215, 244, 220, 185, 230, 190, 200, 180, 190, 196, 180, 230, 224, 160, 178, 205, 185, 210, 180, 190, 200, 257, 190, 220, 165, 205, 200, 208, 185, 215, 170, 235, 210, 170, 180, 170, 190, 150, 230, 203, 260, 246, 186, 210, 198, 210, 215, 180, 200, 245, 200, 192, 192, 200, 192, 205, 190, 186, 170, 197, 219, 200, 220, 207, 225, 207, 212, 225, 170, 190, 210, 230, 210, 200, 238, 234, 222, 200, 190, 170, 220, 223, 210, 215, 196, 175, 175, 189, 205, 210, 180, 180, 197, 220, 228, 190, 204, 165, 216, 220, 208, 210, 215, 195, 200, 215, 229, 240, 207, 205, 208, 185, 190, 170, 208, 225, 190, 225, 185, 180, 165, 240, 220, 212, 163, 215, 175, 205, 210, 205, 208, 215, 180, 200, 230, 211, 230, 190, 220, 180, 205, 190, 180, 205, 190, 195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_qxn7VfNYJE",
    "outputId": "50804a29-64c2-4fbf-b5c7-44209056d23a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81.64656, 97.52228, 95.25432, ..., 92.98636, 86.18248, 88.45044])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting weights in lbs to kg\n",
    "\n",
    "weights_kg = np.array(weights_lb) * 0.453592\n",
    "\n",
    "weights_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hqst4kWvNYJM",
    "outputId": "db0ce3e1-4f4f-428f-9471-58cc1e62ef4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.11037639, 27.60406069, 28.48080465, ..., 25.62295933,\n",
       "       23.74810865, 25.72686361])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the BMI: bmi\n",
    "\n",
    "bmi = weights_kg / (heights_m ** 2)\n",
    "\n",
    "bmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hgOQonCNYJX"
   },
   "source": [
    "<h4 style = \"color : Sky blue\"> Sub-Setting Arrays</h4>\n",
    "\n",
    "##### Fetch the first element from the bmi array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxC_4zPqNYJZ",
    "outputId": "7428318b-84be-4e24-b73f-8baf6f9dfc6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.11037638875862"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJw_fIF_NYJh"
   },
   "source": [
    "##### Fetch the last element from the bmi array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMjBir0PNYJj",
    "outputId": "242da47a-5af9-4ac0-a875-dcf33ca53cb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.726863613607133"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLHhFzmONYJr"
   },
   "source": [
    "#####  Fetch the first 5 elements from the bmi array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qinJdYrONYJt",
    "outputId": "b7feaa6a-c71a-49e9-87e7-c72e8767b66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.11037639, 27.60406069, 28.48080465, 28.48080465, 24.80333518])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfBVk-mXNYJ5"
   },
   "source": [
    "##### Fetch the last 5 elements from the bmi array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTUtXBuZNYJ7",
    "outputId": "9d9120a0-6caf-4da7-a73b-0b88346ebf38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.06720044, 23.11037639, 25.62295933, 23.74810865, 25.72686361])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwWgOMiLNYKE"
   },
   "source": [
    "<h4 style = \"color : Sky blue\"> Conditional Sub-Setting Arrays</h4>\n",
    "\n",
    "##### Count the number of pariticipants who are underweight i.e. bmi < 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JT-qmbkPNYKG",
    "outputId": "7347c038-039e-4907-c5bf-61fade4b2fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi < 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9NNHZ8wNYKO",
    "outputId": "336085a3-7911-4904-99f3-afd0d23e5bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.54255679, 20.54255679, 20.69282047, 20.69282047, 20.34343189,\n",
       "       20.34343189, 20.69282047, 20.15883472, 19.4984471 , 20.69282047,\n",
       "       20.9205219 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi [ bmi<21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUejHbbONYKX",
    "outputId": "8f9d6cc9-445e-4737-e729-2cdd0e81a830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.54255679, 20.54255679, 20.69282047, 20.69282047, 20.34343189,\n",
       "       20.34343189, 20.69282047, 20.15883472, 19.4984471 , 20.69282047,\n",
       "       20.9205219 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underweight_players = bmi [ bmi<21]\n",
    "underweight_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_eRWB_YNYKf",
    "outputId": "e65c1510-27a3-4ea6-da73-93b1c7db62ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underweight_players.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoLIRApcNYKn"
   },
   "source": [
    "<h4 style = \"color : Sky blue\"> NumPy Functions</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2D3fJra3NYKo"
   },
   "source": [
    "##### Find the largest BMI value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbRRch9INYKq",
    "outputId": "5171b37a-7b12-4b8f-e469-68dd54267909"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.26194861031698"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fumC4BybNYKy",
    "outputId": "26d6ce11-fbe5-4dd3-f4b7-1c4140407be7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.26194861031698"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGQBQcs9NYK6"
   },
   "source": [
    "##### Find lowest BMI value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHkzhfynNYK8",
    "outputId": "44eecd88-a98c-4342-c114-a279e6af3a89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.498447103560874"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_wKFbq0NYLG"
   },
   "source": [
    "##### Find average BMI value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xC4IdjdHNYLJ",
    "outputId": "99485c01-7c47-4b7e-e1e6-45074a4e5938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.05684565448554"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmi.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Exercise_1'>Exercise_1</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FyQIRnWNYLR"
   },
   "outputs": [],
   "source": [
    "# Exercise Background\n",
    "\n",
    "This small application based coding exercise is ment to expose you to the use of the numpy library as well as give you a taste of tasks that you might be needed to perform during machine learning. \n",
    "\n",
    "Usually, machine learning involves working on large data sets. This notebook will walk you through normalising the data and then dividing the data set into smaller subsets. It is recommended that while attempting each of the tasks visit the NumPy library to find the most appropriate function which can help you achieve the desired result. More often than not you will find the functions which you require prewritten in the library. The **numpy library** can be found [here.](https://numpy.org/doc/stable/) \n",
    "\n",
    "Without further ado, the first task is to mean normalise a data set. Mean normalising is a data transformation done to reduce the variations in the data set. For example, consider a data set which has integers between 0 and 10000. That is a lot of variation, and it becomes difficult to build ML algorithms on this data. So mean normalisation is done on such data, after the transformation, the mean of the data will be zero, and standard deviation will be 1.  Even though the actual values of data will change a lot, but the overall variation is still kept intact. If the concept of normalisation feels a bit unclear dont worry all of this will be covered in the future sections of this program. For now, let’s concentrate on the tasks at hand. \n",
    "\n",
    "\n",
    "# Task 1: Mean Normalisation: \n",
    "\n",
    "**Question 1.1** Create a 2D of random integers between 0 and 10,000 (including both 0 and 10,000) with 25000 rows and 15 columns. This will be the dataset you will use in the notebook. \n",
    "\n",
    "# import NumPy into Python\n",
    "import numpy as np\n",
    "\n",
    "# Create a 25000 x 15 ndarray with random integers in the interval [0, 10000].\n",
    "X = np.random.randint(0,high = 10001, size = (25000,15))\n",
    "\n",
    "# print the shape of X\n",
    "X.shape\n",
    "\n",
    "# print the first row of X\n",
    "X[1]\n",
    "\n",
    "Now that you created the array we will mean normalize it. The equation for normalisaing the data is given below:\n",
    "\n",
    "$\\mbox{Norm_Col}_i = \\frac{\\mbox{Col}_i - \\mu_i}{\\sigma_i}$\n",
    "\n",
    "where $\\mbox{Col}_i$ is the $i$th column of $X$, $\\mu_i$ is average of the values in the $i$th column of $X$, and $\\sigma_i$ is the standard deviation of the values in the $i$th column of $X$. To put it simply, to find the new value of each element, you have to subtract the mean of respective column form that value and divide the result with the standard deviation of that columns. Now the question is, Why are these operations being done column-wise? That is because usually all the procedures in ML are done column-wise. So it will be beneficial for us to develop the habit of thinking about data column-wise.   \n",
    "\n",
    "**Question 1.2** Find the mean and the standard deviation of each of the columns in the dataset. The result will be two 1D arrays with 15 elements each, representing the mean and standard deviation for each of the columns in the dataset.  \n",
    "\n",
    "# Average of the values in each column of X\n",
    "ave_cols = np.average(X, axis = 0)\n",
    "\n",
    "# print ave_cols  \n",
    "print (\"ave_cols: \\n\", ave_cols,\"\\n\")\n",
    "\n",
    "# Standard Deviation of the values in each column of X\n",
    "std_cols = np.std(X, axis = 0)\n",
    "\n",
    "# print std_cols  \n",
    "print (\"std_cols: \\n\", std_cols,\"\\n\")\n",
    "\n",
    "**Question 1.3** Print the shape of each both the arrays, they should have 15 elements each.  \n",
    "\n",
    "# Print the shape of ave_cols\n",
    "print(ave_cols.shape)\n",
    "\n",
    "# Print the shape of std_cols\n",
    "print(std_cols.shape)\n",
    "\n",
    "**Question 1.4** Now that you have mean and standard deviation calculated, it is time to apply the transformation to the dataset. \n",
    " \n",
    "**HINT** The broadcast property of NumPy can make this a lot easier. You can read about it [here](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
    "All you have to do is create one row of transformation values and repeat them through all the values.\n",
    "\n",
    "# Mean normalize X\n",
    "X_norm = (X - ave_cols)/std_cols\n",
    "\n",
    "**Explanation of the solution:**\n",
    " \n",
    "The shape X is (25000, 15), and the shape of  ave_cols is (15,). If their shapes are different, then how is it possible that the operation can be executed? It is because of the property of broadcasting. Broadcasting repeats the smaller arrays over and over until it becomes the same size as the larger array. In this case, it is straightforward to figure out how the repetition would have worked. The ave_cols will be repeated 25000 times, and the operation is executed. \n",
    " \n",
    "But be careful broadcasting also has some limitations, you can read about them in the link above. \n",
    "\n",
    "**Question 1.5** If the transformation has been performed correctly, the mean of elements in each column will be approximately 0. Also, the average of the **minimum** value in each column of X_norm and the average of the **maximum** value in each column of X_norm will have almost the same face value with opposite signs. Let’s confirm if the transformation has happened correctly. \n",
    "\n",
    "# Print the average of all the values of X_norm\n",
    "print (np.average(X_norm))\n",
    "\n",
    "# Print the average of the minimum value in each column of X_norm\n",
    "print (np.average(X_norm.min(axis = 0)))\n",
    "\n",
    "# Print the average of the maximum value in each column of X_norm\n",
    "print (np.average(X_norm.max(axis = 0)))\n",
    "\n",
    "Be mindful that the exact values might not match since the dataset was initialized using the random function. \n",
    "\n",
    "# Data Spliting \n",
    "\n",
    "After data processing, it is a regular practice in ML to split the dataset into three datasets. \n",
    "\n",
    "1. A Training Set\n",
    "2. A Cross Validation Set\n",
    "3. A Test Set\n",
    "\n",
    "The ratios in which the data is split varies a bit from case to case. But the accepted standard 6:2:2 for train, test, and validation respectively. That is 60% for training data and so on. Again why is the data split or what is the signification of these smaller data sets? These questions are better left unanswered for now. \n",
    "The tanks assigned to you is to split the data in the given proportions randomly. \n",
    "For instance, if the data set had ten elements, this is how you would do it. \n",
    "\n",
    "# We create a random permutation of integers 0 to 9\n",
    "np.random.permutation(10)\n",
    "\n",
    "1. training set = 8,3,7,5,2,6\n",
    "2. Cross Validation Set = 1,9\n",
    "3. Test Set = 0,4\n",
    "\n",
    "Notice, the propostion of the split has stayed the same, and the data points are randomply seleccted and data points do not repeat in different sets. \n",
    "\n",
    "**Question 2.1** Similarly, create a 1D array representing the indexes of the rows in the dataset X_norm. U can use the   `np.random.permutation()` function for randomising the indexes. \n",
    "\n",
    "# Create a rank 1 ndarray that contains a random permutation of the row indices of `X_norm`\n",
    "row_indices = np.random.permutation(np.arange(0,25000))\n",
    "\n",
    "# Print the shape of row_indices\n",
    "row_indices.shape\n",
    "\n",
    "**Question 2.2** Split the row indexes in the needed proportions. You can use the slicing methods you have learnt in this session to make the job easier.  \n",
    "\n",
    "25000*0.6\n",
    "\n",
    "25000*0.2\n",
    "\n",
    "# Make any necessary calculations.\n",
    "# You can save your calculations into variables to use later.\n",
    "train = row_indices[:15000]\n",
    "test = row_indices[15000:20000]\n",
    "val  = row_indices[20000:]\n",
    "\n",
    "**Question 2.4** Now make use of the indexes that you made to split the data also similarly once the data is split print the shape of each of the smaller data sets. `X_train` should have 15000 rows and 15 columns. `X_test` should have 5000 rows and 15 columns. `X_val` should have 5000 rows and 15 columns. \n",
    "\n",
    "# Create a Training Set\n",
    "X_train = X_norm[train]\n",
    "\n",
    "# Create a Cross Validation Set\n",
    "X_Val = X_norm[test]\n",
    "\n",
    "# Create a Test Set\n",
    "X_test = X_norm[val]\n",
    "\n",
    "# Print the shape of X_train\n",
    "print(X_train.shape)\n",
    "\n",
    "# Print the shape of X_crossVal\n",
    "print(X_Val.shape)\n",
    "\n",
    "# Print the shape of X_test\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Case_study_Cricket_tournament'>Case_study_Cricket_tournament</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2 style = \"color : Brown\"> Case Study - Cricket Tournament </h2>\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 1</h4>  \n",
    "\n",
    "##### Players list contain the height(inches) and weight(lbs) data for all the players\n",
    "\n",
    "# list of height and weight of the players.\n",
    "players = [(74, 180), (74, 215), (72, 210), (72, 210), (73, 188), (69, 176), (69, 209), (71, 200), (76, 231), (71, 180), (73, 188), (73, 180), (74, 185), (74, 160), (69, 180), (70, 185), (73, 189), (75, 185), (78, 219), (79, 230), (76, 205), (74, 230), (76, 195), (72, 180), (71, 192), (75, 225), (77, 203), (74, 195), (73, 182), (74, 188), (78, 200), (73, 180), (75, 200), (73, 200), (75, 245), (75, 240), (74, 215), (69, 185), (71, 175), (74, 199), (73, 200), (73, 215), (76, 200), (74, 205), (74, 206), (70, 186), (72, 188), (77, 220), (74, 210), (70, 195), (73, 200), (75, 200), (76, 212), (76, 224), (78, 210), (74, 205), (74, 220), (76, 195), (77, 200), (81, 260), (78, 228), (75, 270), (77, 200), (75, 210), (76, 190), (74, 220), (72, 180), (72, 205), (75, 210), (73, 220), (73, 211), (73, 200), (70, 180), (70, 190), (70, 170), (76, 230), (68, 155), (71, 185), (72, 185), (75, 200), (75, 225), (75, 225), (75, 220), (68, 160), (74, 205), (78, 235), (71, 250), (73, 210), (76, 190), (74, 160), (74, 200), (79, 205), (75, 222), (73, 195), (76, 205), (74, 220), (74, 220), (73, 170), (72, 185), (74, 195), (73, 220), (74, 230), (72, 180), (73, 220), (69, 180), (72, 180), (73, 170), (75, 210), (75, 215), (73, 200), (72, 213), (72, 180), (76, 192), (74, 235), (72, 185), (77, 235), (74, 210), (77, 222), (75, 210), (76, 230), (80, 220), (74, 180), (74, 190), (75, 200), (78, 210), (73, 194), (73, 180), (74, 190), (75, 240), (76, 200), (71, 198), (73, 200), (74, 195), (76, 210), (76, 220), (74, 190), (73, 210), (74, 225), (70, 180), (72, 185), (73, 170), (73, 185), (73, 185), (73, 180), (71, 178), (74, 175), (74, 200), (72, 204), (74, 211), (71, 190), (74, 210), (73, 190), (75, 190), (75, 185), (79, 290), (73, 175), (75, 185), (76, 200), (74, 220), (76, 170), (78, 220), (74, 190), (76, 220), (72, 205), (74, 200), (76, 250), (74, 225), (75, 215), (78, 210), (75, 215), (72, 195), (74, 200), (72, 194), (74, 220), (70, 180), (71, 180), (70, 170), (75, 195), (71, 180), (71, 170), (73, 206), (72, 205), (71, 200), (73, 225), (72, 201), (75, 225), (74, 233), (74, 180), (75, 225), (73, 180), (77, 220), (73, 180), (76, 237), (75, 215), (74, 190), (76, 235), (75, 190), (73, 180), (71, 165), (76, 195), (75, 200), (72, 190), (71, 190), (77, 185), (73, 185), (74, 205), (71, 190), (72, 205), (74, 206), (75, 220), (73, 208), (72, 170), (75, 195), (75, 210), (74, 190), (72, 211), (74, 230), (71, 170), (70, 185), (74, 185), (77, 241), (77, 225), (75, 210), (75, 175), (78, 230), (75, 200), (76, 215), (73, 198), (75, 226), (75, 278), (79, 215), (77, 230), (76, 240), (71, 184), (75, 219), (74, 170), (69, 218), (71, 190), (76, 225), (72, 220), (72, 176), (70, 190), (72, 197), (73, 204), (71, 167), (72, 180), (71, 195), (73, 220), (72, 215), (73, 185), (74, 190), (74, 205), (72, 205), (75, 200), (74, 210), (74, 215), (77, 200), (75, 205), (73, 211), (72, 190), (71, 208), (74, 200), (77, 210), (75, 232), (75, 230), (75, 210), (78, 220), (78, 210), (74, 202), (76, 212), (78, 225), (76, 170), (70, 190), (72, 200), (80, 237), (74, 220), (74, 170), (71, 193), (70, 190), (72, 150), (71, 220), (74, 200), (71, 190), (72, 185), (71, 185), (74, 200), (69, 172), (76, 220), (75, 225), (75, 190), (76, 195), (73, 219), (76, 190), (73, 197), (77, 200), (73, 195), (72, 210), (72, 177), (77, 220), (77, 235), (71, 180), (74, 195), (74, 195), (73, 190), (78, 230), (75, 190), (73, 200), (70, 190), (74, 190), (72, 200), (73, 200), (73, 184), (75, 200), (75, 180), (74, 219), (76, 187), (73, 200), (74, 220), (75, 205), (75, 190), (72, 170), (73, 160), (73, 215), (72, 175), (74, 205), (78, 200), (76, 214), (73, 200), (74, 190), (75, 180), (70, 205), (75, 220), (71, 190), (72, 215), (78, 235), (75, 191), (73, 200), (73, 181), (71, 200), (75, 210), (77, 240), (72, 185), (69, 165), (73, 190), (74, 185), (72, 175), (70, 155), (75, 210), (70, 170), (72, 175), (72, 220), (74, 210), (73, 205), (74, 200), (76, 205), (75, 195), (80, 240), (72, 150), (75, 200), (73, 215), (74, 202), (74, 200), (73, 190), (75, 205), (75, 190), (71, 160), (73, 215), (75, 185), (74, 200), (74, 190), (72, 210), (74, 185), (74, 220), (74, 190), (73, 202), (76, 205), (75, 220), (72, 175), (73, 160), (73, 190), (73, 200), (72, 229), (72, 206), (72, 220), (72, 180), (71, 195), (75, 175), (75, 188), (74, 230), (73, 190), (75, 200), (79, 190), (74, 219), (76, 235), (73, 180), (74, 180), (74, 180), (72, 200), (74, 234), (74, 185), (75, 220), (78, 223), (74, 200), (74, 210), (74, 200), (77, 210), (70, 190), (73, 177), (74, 227), (73, 180), (71, 195), (75, 199), (71, 175), (72, 185), (77, 240), (74, 210), (70, 180), (77, 194), (73, 225), (72, 180), (76, 205), (71, 193), (76, 230), (78, 230), (75, 220), (73, 200), (78, 249), (74, 190), (79, 208), (75, 245), (76, 250), (72, 160), (75, 192), (75, 220), (70, 170), (72, 197), (70, 155), (74, 190), (71, 200), (76, 220), (73, 210), (76, 228), (71, 190), (69, 160), (72, 184), (72, 180), (69, 180), (73, 200), (69, 176), (73, 160), (74, 222), (74, 211), (72, 195), (71, 200), (72, 175), (72, 206), (76, 240), (76, 185), (76, 260), (74, 185), (76, 221), (75, 205), (71, 200), (72, 170), (71, 201), (73, 205), (75, 185), (76, 205), (75, 245), (71, 220), (75, 210), (74, 220), (72, 185), (73, 175), (73, 170), (73, 180), (73, 200), (76, 210), (72, 175), (76, 220), (73, 206), (73, 180), (73, 210), (75, 195), (75, 200), (77, 200), (73, 164), (72, 180), (75, 220), (70, 195), (74, 205), (72, 170), (80, 240), (71, 210), (71, 195), (74, 200), (74, 205), (73, 192), (75, 190), (76, 170), (73, 240), (77, 200), (72, 205), (73, 175), (77, 250), (76, 220), (71, 224), (75, 210), (73, 195), (74, 180), (77, 245), (71, 175), (72, 180), (73, 215), (69, 175), (73, 180), (70, 195), (74, 230), (76, 230), (73, 205), (73, 215), (75, 195), (73, 180), (79, 205), (74, 180), (73, 190), (74, 180), (77, 190), (75, 190), (74, 220), (73, 210), (77, 255), (73, 190), (77, 230), (74, 200), (74, 205), (73, 210), (77, 225), (74, 215), (77, 220), (75, 205), (77, 200), (75, 220), (71, 197), (74, 225), (70, 187), (79, 245), (72, 185), (72, 185), (70, 175), (74, 200), (74, 180), (72, 188), (73, 225), (72, 200), (74, 210), (74, 245), (76, 213), (82, 231), (74, 165), (74, 228), (70, 210), (73, 250), (73, 191), (74, 190), (77, 200), (72, 215), (76, 254), (73, 232), (73, 180), (72, 215), (74, 220), (74, 180), (71, 200), (72, 170), (75, 195), (74, 210), (74, 200), (77, 220), (70, 165), (71, 180), (73, 200), (76, 200), (71, 170), (75, 224), (74, 220), (72, 180), (76, 198), (79, 240), (76, 239), (73, 185), (76, 210), (78, 220), (75, 200), (76, 195), (72, 220), (72, 230), (73, 170), (73, 220), (75, 230), (71, 165), (76, 205), (70, 192), (75, 210), (74, 205), (75, 200), (73, 210), (71, 185), (71, 195), (72, 202), (73, 205), (73, 195), (72, 180), (69, 200), (73, 185), (78, 240), (71, 185), (73, 220), (75, 205), (76, 205), (70, 180), (74, 201), (77, 190), (75, 208), (79, 240), (72, 180), (77, 230), (73, 195), (75, 215), (75, 190), (75, 195), (73, 215), (73, 215), (76, 220), (77, 220), (75, 230), (70, 195), (71, 190), (71, 195), (75, 209), (74, 204), (69, 170), (70, 185), (75, 205), (72, 175), (75, 210), (73, 190), (72, 180), (72, 180), (72, 160), (76, 235), (75, 200), (74, 210), (69, 180), (73, 190), (72, 197), (72, 203), (75, 205), (77, 170), (76, 200), (80, 250), (77, 200), (76, 220), (79, 200), (71, 190), (75, 170), (73, 190), (76, 220), (77, 215), (73, 206), (76, 215), (70, 185), (75, 235), (73, 188), (75, 230), (70, 195), (69, 168), (71, 190), (72, 160), (72, 200), (73, 200), (70, 189), (70, 180), (73, 190), (76, 200), (75, 220), (72, 187), (73, 240), (79, 190), (71, 180), (72, 185), (74, 210), (74, 220), (74, 219), (72, 190), (76, 193), (76, 175), (72, 180), (72, 215), (71, 210), (72, 200), (72, 190), (70, 185), (77, 220), (74, 170), (72, 195), (76, 205), (71, 195), (76, 210), (71, 190), (73, 190), (70, 180), (73, 220), (73, 190), (72, 186), (71, 185), (71, 190), (71, 180), (72, 190), (72, 170), (74, 210), (74, 240), (74, 220), (71, 180), (72, 210), (75, 210), (72, 195), (71, 160), (72, 180), (72, 205), (72, 200), (72, 185), (74, 245), (74, 190), (77, 210), (75, 200), (73, 200), (75, 222), (73, 215), (76, 240), (72, 170), (77, 220), (75, 156), (72, 190), (71, 202), (71, 221), (75, 200), (72, 190), (73, 210), (73, 190), (71, 200), (70, 165), (75, 190), (71, 185), (76, 230), (73, 208), (68, 209), (71, 175), (72, 180), (74, 200), (77, 205), (72, 200), (76, 250), (78, 210), (81, 230), (72, 244), (73, 202), (76, 240), (72, 200), (72, 215), (74, 177), (76, 210), (73, 170), (76, 215), (75, 217), (70, 198), (71, 200), (74, 220), (72, 170), (73, 200), (76, 230), (76, 231), (73, 183), (71, 192), (68, 167), (71, 190), (71, 180), (74, 180), (77, 215), (69, 160), (72, 205), (76, 223), (75, 175), (76, 170), (75, 190), (76, 240), (72, 175), (74, 230), (76, 223), (74, 196), (72, 167), (75, 195), (78, 190), (77, 250), (70, 190), (72, 190), (79, 190), (74, 170), (71, 160), (68, 150), (77, 225), (75, 220), (71, 209), (72, 210), (70, 176), (72, 260), (72, 195), (73, 190), (72, 184), (74, 180), (72, 195), (72, 195), (75, 219), (72, 225), (73, 212), (74, 202), (72, 185), (78, 200), (75, 209), (72, 200), (74, 195), (75, 228), (75, 210), (76, 190), (74, 212), (74, 190), (73, 218), (74, 220), (71, 190), (74, 235), (75, 210), (76, 200), (74, 188), (76, 210), (76, 235), (73, 188), (75, 215), (75, 216), (74, 220), (68, 180), (72, 185), (75, 200), (71, 210), (70, 220), (72, 185), (73, 231), (72, 210), (75, 195), (74, 200), (70, 205), (76, 200), (71, 190), (82, 250), (72, 185), (73, 180), (74, 170), (71, 180), (75, 208), (77, 235), (72, 215), (74, 244), (72, 220), (73, 185), (78, 230), (77, 190), (73, 200), (73, 180), (73, 190), (73, 196), (73, 180), (76, 230), (75, 224), (70, 160), (73, 178), (72, 205), (73, 185), (75, 210), (74, 180), (73, 190), (73, 200), (76, 257), (73, 190), (75, 220), (70, 165), (77, 205), (72, 200), (77, 208), (74, 185), (75, 215), (75, 170), (75, 235), (75, 210), (72, 170), (74, 180), (71, 170), (76, 190), (71, 150), (75, 230), (76, 203), (83, 260), (75, 246), (74, 186), (76, 210), (72, 198), (72, 210), (75, 215), (75, 180), (72, 200), (77, 245), (73, 200), (72, 192), (70, 192), (74, 200), (72, 192), (74, 205), (72, 190), (71, 186), (70, 170), (71, 197), (76, 219), (74, 200), (76, 220), (74, 207), (74, 225), (74, 207), (75, 212), (75, 225), (71, 170), (71, 190), (74, 210), (77, 230), (71, 210), (74, 200), (75, 238), (77, 234), (76, 222), (74, 200), (76, 190), (72, 170), (71, 220), (72, 223), (75, 210), (73, 215), (68, 196), (72, 175), (69, 175), (73, 189), (73, 205), (75, 210), (70, 180), (70, 180), (74, 197), (75, 220), (74, 228), (74, 190), (73, 204), (74, 165), (75, 216), (77, 220), (73, 208), (74, 210), (76, 215), (74, 195), (75, 200), (73, 215), (76, 229), (78, 240), (75, 207), (73, 205), (77, 208), (74, 185), (72, 190), (74, 170), (72, 208), (71, 225), (73, 190), (75, 225), (73, 185), (67, 180), (67, 165), (76, 240), (74, 220), (73, 212), (70, 163), (75, 215), (70, 175), (72, 205), (77, 210), (79, 205), (78, 208), (74, 215), (75, 180), (75, 200), (78, 230), (76, 211), (75, 230), (69, 190), (75, 220), (72, 180), (75, 205), (73, 190), (74, 180), (75, 205), (75, 190), (73, 195)]\n",
    "\n",
    "len(players)\n",
    "\n",
    "players[1][1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np_players = np.array(players)\n",
    "\n",
    "np_players\n",
    "\n",
    "type(np_players)\n",
    "\n",
    "\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 2 (Numpy Attributes)</h4>\n",
    "\n",
    "##### Print the structure of the 2-D Array\n",
    "\n",
    "np_players.shape\n",
    "\n",
    "##### Print the dimensions of the array\n",
    "\n",
    "np_players.ndim\n",
    "\n",
    "##### Print the data type of elements in the array\n",
    "\n",
    "np_players.dtype\n",
    "\n",
    "##### Print the size of a single item of the array\n",
    "\n",
    "np_players.itemsize\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 3</h4>\n",
    "\n",
    "##### Convert the heights to meters and weights to kg \n",
    "\n",
    "players_converted = np_players * [0.0254, 0.453592]\n",
    "\n",
    "players_converted\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Sub-Setting 2-D Arrays</h4>\n",
    "\n",
    "##### Fetch the first row from the array \n",
    "\n",
    "players_converted[0]\n",
    "\n",
    "##### Fetch the first row 2nd element from the array \n",
    "\n",
    "players_converted[0][1]\n",
    "\n",
    "##### Fetch the first column from the array \n",
    "\n",
    "players_converted[:, 0]\n",
    "\n",
    "##### Fetch the height (1st column) of 125th player from the array \n",
    "\n",
    "players_converted[124][0]\n",
    "\n",
    "players_converted[124,0]\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Conditional Sub-Setting Arrays</h4>\n",
    "\n",
    "##### Fetch height and weight of players with height above 1.8m\n",
    "\n",
    "\n",
    "tall_players = players_converted[players_converted[:,0] > 1.8]\n",
    "\n",
    "players_converted.shape\n",
    "\n",
    "tall_players.shape\n",
    "\n",
    "##### Skills Array - holds the player key skills.\n",
    "\n",
    "skills = np.array(['Keeper', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Bowler', 'Bowler', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Keeper', 'Keeper', 'Keeper', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Keeper', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Bowler', 'Bowler', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper', 'Batsman', 'Keeper', 'Bowler', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Bowler', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Batsman', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Bowler', 'Bowler', 'Bowler', 'Batsman', 'Keeper', 'Bowler', 'Bowler', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Batsman', 'Bowler', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper', 'Keeper', 'Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Batsman', 'Bowler', 'Keeper', 'Keeper', 'Batsman', 'Bowler', 'Bowler', 'Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Bowler', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Batsman', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Batsman', 'Batsman', 'Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Keeper', 'Bowler', 'Keeper', 'Bowler', 'Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Batsman', 'Keeper', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Batsman', 'Bowler', 'Keeper', 'Bowler', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Bowler', 'Bowler', 'Bowler', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Batsman', 'Bowler', 'Keeper', 'Bowler', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Batsman', 'Batsman', 'Keeper', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Bowler', 'Batsman', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Bowler', 'Bowler', 'Keeper', 'Bowler', 'Keeper', 'Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Bowler', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Batsman', 'Batsman', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Batsman', 'Batsman', 'Bowler', 'Batsman', 'Keeper', 'Bowler', 'Bowler', 'Keeper', 'Bowler', 'Bowler', 'Keeper', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Keeper-Batsman', 'Keeper-Batsman', 'Bowler', 'Bowler', 'Bowler', 'Batsman', 'Bowler', 'Keeper-Batsman', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper', 'Keeper-Batsman', 'Batsman', 'Bowler', 'Keeper-Batsman'])\n",
    "skills\n",
    "\n",
    "##### Fetch Heights of the Batsmen\n",
    "\n",
    "batsmen = players_converted[skills == 'Batsman']\n",
    "\n",
    "batsmen.shape\n",
    "\n",
    "batsmen[:, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Creating_numpy_arrays'>Creating_numpy_arrays</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2 style = \"color : Brown\"> Creating NumPy Arrays </h2 >\n",
    "\n",
    "\n",
    "\n",
    " The following ways are commonly used when you know the size of the array beforehand:\n",
    "* ```np.ones()```: Create array of 1s\n",
    "* ```np.zeros()```: Create array of 0s\n",
    "* ```np.random.random()```: Create array of random numbers\n",
    "* ```np.arange()```: Create array with increments of a fixed step size\n",
    "* ```np.linspace()```: Create array of fixed length\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "##### Tip: Use help to see the syntax when required\n",
    "\n",
    "help(np.ones)\n",
    "\n",
    "##### Creating a 1 D array of ones\n",
    "\n",
    "arr = np.ones(5)\n",
    "arr\n",
    "\n",
    "##### Notice that, by default, numpy creates data type = float64\n",
    "\n",
    "\n",
    "\n",
    "arr.dtype\n",
    "\n",
    "##### Can provide dtype explicitly using dtype\n",
    "\n",
    "\n",
    "arr = np.ones(5, dtype=int)\n",
    "arr\n",
    "\n",
    "arr.dtype\n",
    "\n",
    "##### Creating a 5  x 3 array of ones\n",
    "\n",
    "\n",
    "np.ones((5,3))\n",
    "\n",
    "##### Creating array of zeros\n",
    "\n",
    "np.zeros(5)\n",
    "\n",
    "\n",
    "# convert the type into integer.\n",
    "np.zeros(5, dtype=int)\n",
    "\n",
    "# Create a list of integers range between 1 to 5.\n",
    "list(range(1,5))\n",
    "\n",
    "np.arange(3)\n",
    "\n",
    "np.arange(3.0)\n",
    "\n",
    "##### Notice that 3 is included, 35 is not, as in standard python lists\n",
    "\n",
    "From 3 to 35 with a step of 2\n",
    "\n",
    "np.arange(3,35,2)\n",
    "\n",
    "##### Array of random numbers \n",
    "\n",
    "\n",
    "np.random.randint(2, size=10)\n",
    "\n",
    "np.random.randint(3,5, size=10)\n",
    "\n",
    "##### 2D Array of random numbers \n",
    "\n",
    "\n",
    "np.random.random([3,4])\n",
    "\n",
    "###### Sometimes, you know the length of the array, not the step size\n",
    "\n",
    "Array of length 20 between 1 and 10\n",
    "\n",
    "np.linspace(1,10,20)\n",
    "\n",
    "<h2 style = \"color : Sky blue\"> Exercises </h2>\n",
    "\n",
    "\n",
    "\n",
    "Apart from the methods mentioned above, there are a few more NumPy functions that you can use to create special NumPy arrays:\n",
    "\n",
    "-  `np.full()`: Create a constant array of any number ‘n’\n",
    "-  `np.tile()`: Create a new array by repeating an existing array for a particular number of times\n",
    "-  `np.eye()`: Create an identity matrix of any dimension\n",
    "-  `np.random.randint()`: Create a random array of integers within a particular range\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Operations_on_NumPy_Arrays'>Operations_on_NumPy_Arrays</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2 style = \"color : Brown\"> Operations on NumPy Arrays</h2>\n",
    "\n",
    "The learning objectives of this section are:\n",
    "\n",
    "* Manipulate arrays\n",
    "    * Reshape arrays\n",
    "    * Stack arrays\n",
    "* Perform operations on arrays\n",
    "    * Perform basic mathematical operations\n",
    "    * Apply built-in functions \n",
    "    * Apply your own functions \n",
    "    * Apply basic linear algebra operations \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 1 (Arithmatric Operations)</h4>  \n",
    "\n",
    "\n",
    "array1 = np.array([10,20,30,40,50])\n",
    "array2 = np.arange(5)\n",
    "\n",
    "array1\n",
    "\n",
    "array2\n",
    "\n",
    "# Add array1 and array2.\n",
    "array3 = array1 + array2\n",
    "\n",
    "array3\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 2</h4>  \n",
    "\n",
    "array4 = np.array([1,2,3,4])\n",
    "\n",
    "array4 + array1\n",
    "\n",
    "print (array1.shape)\n",
    "\n",
    "print (array4.shape)\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 3</h4>  \n",
    "\n",
    "array = np.linspace(1, 10, 5)\n",
    "array\n",
    "\n",
    "array*2\n",
    "\n",
    "array**2\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Stacking Arrays</h4> \n",
    "\n",
    "####  ```np.hstack()``` and ```n.vstack()```\n",
    "\n",
    "Stacking is done using the ```np.hstack()``` and ```np.vstack()``` methods. For horizontal stacking, the number of rows should be the same, while for vertical stacking, the number of columns should be the same.\n",
    "\n",
    "# Note that np.hstack(a, b) throws an error - you need to pass the arrays as a list\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "\n",
    "np.hstack((a,b))\n",
    "\n",
    "np.vstack((a,b))\n",
    "\n",
    "np.arange(12)\n",
    "\n",
    "np.arange(12).reshape(3,4)\n",
    "\n",
    "array1 = np.arange(12).reshape(3,4) #3x4\n",
    "array2 = np.arange(20).reshape(5,4) #5x4\n",
    "\n",
    "print (array1, '\\n', array2)\n",
    "\n",
    "np.vstack((array1,array2))\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 4 (Numpy Built-in functions)</h4>  \n",
    "\n",
    "array1\n",
    "\n",
    "np.power(array1, 3)\n",
    "\n",
    "np.arange(9).reshape(3,3)\n",
    "\n",
    "x = np.array([-2,-1, 0, 1,2])\n",
    "x\n",
    "\n",
    "abs(x)\n",
    "\n",
    "np.absolute(x)\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 5 (Trignometric functions)</h4>  \n",
    "\n",
    "np.pi\n",
    "\n",
    "theta = np.linspace(0, np.pi, 5)\n",
    "\n",
    "theta\n",
    "\n",
    "np.sin(theta)\n",
    "\n",
    "np.cos(theta)\n",
    "\n",
    "np.tan(theta)\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 6 (Exponential and logarithmic functions)</h4>  \n",
    "\n",
    "x = [1, 2, 3, 10]\n",
    "x = np.array(x)\n",
    "\n",
    "np.exp(x) # e=2.718...\n",
    "\n",
    "# 2^1, 2^2, 2^3, 2^10\n",
    "np.exp2(x)\n",
    "\n",
    "np.power(x,3)\n",
    "\n",
    "np.log(x)\n",
    "\n",
    "np.log2(x)\n",
    "\n",
    "np.log10(x)\n",
    "\n",
    "np.log\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 7</h4>  \n",
    "\n",
    "x = np.arange(5)\n",
    "x\n",
    "\n",
    "y = x * 10\n",
    "y\n",
    "\n",
    "y = np.empty(5)\n",
    "y\n",
    "\n",
    "np.multiply(x, 12, out=y)\n",
    "\n",
    "y\n",
    "\n",
    "y = np.zeros(10)\n",
    "y\n",
    "\n",
    "np.power(2, x, out=y[::2])\n",
    "\n",
    "y\n",
    "\n",
    "<h4 style = \"color : Sky blue\"> Example - 8 (Aggregates)</h4>  \n",
    "\n",
    "x = np.arange(1,6)\n",
    "x\n",
    "\n",
    "sum(x)\n",
    "\n",
    "np.add.reduce(x)\n",
    "\n",
    "np.add.accumulate(x)\n",
    "\n",
    "np.multiply.accumulate(x)\n",
    "\n",
    "\n",
    "\n",
    "#### Apply Basic Linear Algebra Operations\n",
    "\n",
    "NumPy provides the ```np.linalg``` package to apply common linear algebra operations, such as:\n",
    "* ```np.linalg.inv```: Inverse of a matrix\n",
    "* ```np.linalg.det```: Determinant of a matrix\n",
    "* ```np.linalg.eig```: Eigenvalues and eigenvectors of a matrix\n",
    "    \n",
    "Also, you can multiple matrices using ```np.dot(a, b)```. \n",
    "\n",
    "\n",
    "# np.linalg documentation\n",
    "help(np.linalg)\n",
    "\n",
    "A = np.array([[6, 1, 1],\n",
    "              [4, -2, 5],\n",
    "              [2, 8, 7]])\n",
    "\n",
    "A\n",
    "\n",
    "##### Rank of a matrix\n",
    "\n",
    "np.linalg.matrix_rank(A)\n",
    "\n",
    "##### Trace of matrix A\n",
    "\n",
    "np.trace(A)\n",
    "\n",
    "##### Determinant of a matrix\n",
    "\n",
    "np.linalg.det(A)\n",
    "\n",
    "##### Inverse of matrix A\n",
    "\n",
    "A\n",
    "\n",
    "np.linalg.inv(A)\n",
    "\n",
    "B = np.linalg.inv(A)\n",
    "\n",
    "np.matmul(A,B) #actual matrix multiplication\n",
    "\n",
    "A * B\n",
    "\n",
    "##### Matrix A raised to power 3\n",
    "\n",
    "np.linalg.matrix_power(A,3) # matrix multiplication A A A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Compare_Computation_Times_in_NumPy_and_Standard_Python_Lists'>Compare_Computation_Times_in_NumPy_and_Standard_Python_Lists</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare Computation Times in NumPy and Standard Python Lists\n",
    "\n",
    "Now that we know how to use numpy, let us see code and witness the key advantages of numpy i.e. convenience and speed of computation. \n",
    "\n",
    "In the data science landscape, you'll often work with extremely large datasets, and thus it is important point for you to understand how much computation time (and memory) you can save using numpy, compared to standard python lists.   \n",
    "\n",
    "Let's compare the computation times of arrays and lists for a simple task of calculating the element-wise product of numbers. \n",
    "\n",
    "import time\n",
    "\n",
    "## Comparing time taken for computation\n",
    "list_1 = [i for i in range(1000000)]\n",
    "list_2 = [j**2 for j in range(1000000)]\n",
    "\n",
    "t0 = time.time()\n",
    "product_list = list(map(lambda x, y: x*y, list_1, list_2))\n",
    "t1 = time.time()\n",
    "list_time = t1 - t0\n",
    "print (t1-t0)\n",
    "\n",
    "# numpy array \n",
    "array_1 = np.array(list_1)\n",
    "array_2 = np.array(list_2)\n",
    "\n",
    "t0 = time.time()\n",
    "product_numpy = array_1 * array_2\n",
    "t1 = time.time()\n",
    "numpy_time = t1 - t0\n",
    "print (t1-t0)\n",
    "\n",
    "print(\"The ratio of time taken is {}\".format(list_time//numpy_time))\n",
    "\n",
    "\n",
    "\n",
    "In this case, numpy is **an order of magnitude faster** than lists. This is with arrays of size in millions, but you may work on much larger arrays of sizes in order of billions. Then, the difference is even larger.\n",
    "\n",
    "Some reasons for such difference in speed are:\n",
    "* NumPy is written in C, which is basically being executed behind the scenes\n",
    "* NumPy arrays are more compact than lists, i.e. they take much lesser storage space than lists\n",
    "\n",
    "\n",
    "The following discussions demonstrate the differences in speeds of NumPy and standard python:\n",
    "1. https://stackoverflow.com/questions/8385602/why-are-numpy-arrays-so-fast\n",
    "2. https://stackoverflow.com/questions/993984/why-numpy-instead-of-python-lists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Numpy_practice_Udemy'>Numpy_practice_Udemy</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Links:\n",
    "1. Questions: https://www.w3resource.com/python-exercises/numpy/index.php\n",
    "2. Numpy reference: https://numpy.org/doc/stable/reference/\n",
    "3. Udemy course: https://rakuten.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/learn/lecture/5733184#overview\n",
    "'''\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "from IPython.core.display import display,HTML\n",
    "print(np.array(ast.literal_eval(input())))\n",
    "\n",
    "import sys\n",
    "sys.stdout.write(\"Hello\")\n",
    "\n",
    "\n",
    "\n",
    "# NumPy Basic [59 exercises with solution]\n",
    "\n",
    "# 1\n",
    "print(np.__version__)\n",
    "    #print(np.show_config())\n",
    "\n",
    "#2\n",
    "    #print(np.info(np.add))\n",
    "\n",
    "#3 to #8\n",
    "li = [1,2,3,4,5]\n",
    "arr = np.array(li)\n",
    "print(np.array(li)[np.array(li)==0])\n",
    "print(np.all(arr<4))\n",
    "print(np.any(arr<4))\n",
    "print(-np.inf>2)  # false\n",
    "print(np.inf>2)  # true\n",
    "\n",
    "li = np.array([1+1j, 1+0j, 4.5, 3, 2, 2j]) \n",
    "print(li)  # all elements as complex numbers\n",
    "print(np.isreal(li))\n",
    "print(np.iscomplex(li))\n",
    "print(np.isscalar(li))\n",
    "\n",
    "# 10\n",
    "li1 = [1,2,3,4,50]\n",
    "arr1 = np.array(li1)\n",
    "print(np.greater(arr,arr1))\n",
    "print(np.less(arr,arr1))\n",
    "print(np.greater_equal(arr,arr1))\n",
    "print(np.less_equal(arr,arr1))\n",
    "\n",
    "#12\n",
    "print(\"%d bytes\" % (arr1.size * arr1.itemsize))\n",
    "\n",
    "#13\n",
    "print(np.zeros(5), np.ones(4), np.ones(4)*4)\n",
    "\n",
    "\n",
    "#14 \n",
    "print(np.array([i for i in range(30,71) if(i%2==0)]))   #or\n",
    "print(np.arange(30,71,2))\n",
    "\n",
    "#16 , 17\n",
    "# Numpy matrices: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
    "matrix = np.matrix('1 2;3 4')\n",
    "print(np.identity(3))\n",
    "\n",
    "print(np.random.randint(3, 500,1))\n",
    "print(np.random.randn(3, 500))\n",
    "print(np.random.rand(3, 500))\n",
    "print(np.random.normal(3, 500))\n",
    "print(np.random.rand(3, 500,1))\n",
    "\n",
    "print(np.arange(1,30,2))  # range with intervals\n",
    "print(np.linspace(1,30,2)) # range (equally distributed) with length \n",
    "\n",
    "y = np.zeros((2, 3, 4,4))\n",
    "print(y)\n",
    "print(y.ndim)  # 4\n",
    "\n",
    "print(np.identity(4))\n",
    "print(np.eye(4))\n",
    "print(np.identity(4)==np.eye(4))   # true\n",
    "\n",
    "arr = np.random.randint(4,50,300)\n",
    "print(arr.argmax())  # Returns the indices of the maximum values along an axis.\n",
    "print(arr[arr.argmax()])\n",
    "print(arr[arr.argmin()])\n",
    "print(arr.dtype)\n",
    "print(arr)\n",
    "arr[:] = 99\n",
    "print(arr)\n",
    "\n",
    "# numpy array indexing\n",
    "\n",
    "print(np.linspace(0,11))  #Its default value is 50\n",
    "print(np.linspace(0,11,2))\n",
    "arr = arr.reshape((30,10))\n",
    "print(arr)  # product should be equal to the total size\n",
    "print(arr[:23,:23])\n",
    "\n",
    "# numpy operations\n",
    "\n",
    "arr = np.array([1,2,3,4,5],dtype=\"float64\")\n",
    "print(arr + arr)\n",
    "print(arr * arr)\n",
    "print(arr **----- arr)  # if dtype is \"int\", then: ValueError: Integers to negative integer powers are not allowed.\n",
    "\n",
    "\n",
    "print(np.exp(arr))\n",
    "\n",
    "# numpy exercises\n",
    "\n",
    "print(np.zeros(10) + np.ones(10))\n",
    "print(np.ones(10)*5)\n",
    "print(np.arange(8).reshape(2,4))\n",
    "\n",
    "print(np.random.rand(1,2,1))  # dimensions\n",
    "print(np.random.randn(25))\n",
    "\n",
    "# numpy exercises\n",
    "\n",
    "print(np.arange(1,101).reshape(10,10)/100)\n",
    "#print(np.linspace(1,101).reshape(10,10)/100)  -> error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Numpy_practice'>Numpy_practice</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Data Science</h3>\n",
       "<h4>Numpy</h4>\n",
       "<h4>Pandas</h4>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.54  5.08  7.62 10.16 12.7  15.24]\n",
      "[1.         0.17194273 0.06139203 0.0295643  0.01677321 0.01055591]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Numpy arithmetic operations</h3>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 7, 8, 9, 0]\n",
      "[ 8 10 12  4]\n",
      "[0.0254 0.0508 0.0762 0.1016]\n",
      "4\n",
      "4\n",
      "(4,)\n",
      "[0.5 1.  1.5 2. ]\n",
      "[0 1 1 2]\n",
      "[ 1  4  9 16]\n",
      "[3 4 5 6]\n",
      "[-1  0  1  2]\n",
      "[1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Trigonometric:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42289408 0.27865714 0.83598528 0.55323453]\n",
      "[ 1.18839511  1.09975017  7.0861674  -1.32134871]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Median,mean,std</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5 2.5\n",
      "1 4\n",
      "1.118033988749895\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Indexing, splitting, slicing</h3>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "[1 2 3]\n",
      "[1 2 3 4]\n",
      "[4 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Conditional sub-setting arrays</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False]\n",
      "[1 2]\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h4>Multiple-Conditioned sub-setting arrays</h4>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Numpy functions</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "10\n",
      "4\n",
      "1\n",
      "1\n",
      "4\n",
      "2.5\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Multidimensional arrays</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 23]\n",
      " [23 34]\n",
      " [34 45]\n",
      " [45 56]\n",
      " [56 67]\n",
      " [67 78]\n",
      " [78 89]]\n",
      "[[12 23]\n",
      " [23 34]\n",
      " [34 45]\n",
      " [45 56]\n",
      " [56 67]\n",
      " [67 78]\n",
      " [78 89]]\n",
      "[{12, 23} {34, 23} {34, 45} {56, 45} {56, 67} {67, 78} {89, 78}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Multidimensional arrays (continued)</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Padmaja\\AppData\\Local\\Temp\\ipykernel_7388\\4180433849.py:115: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(players1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{12, 23} {34, 23} (34, 45) (45, 56) (56, 67) (67, 78) (78, 89)]\n",
      "2\n",
      "int32\n",
      "int64\n",
      "4\n",
      "[[12 23]\n",
      " [23 34]\n",
      " [34 45]\n",
      " [45 56]\n",
      " [56 67]\n",
      " [67 78]\n",
      " [78 89]]\n",
      "8\n",
      "[[ 24 115]\n",
      " [ 46 170]\n",
      " [ 68 225]\n",
      " [ 90 280]\n",
      " [112 335]\n",
      " [134 390]\n",
      " [156 445]]\n",
      "[[12 23]\n",
      " [23 34]\n",
      " [34 45]\n",
      " [45 56]\n",
      " [56 67]\n",
      " [67 78]\n",
      " [78 89]]\n",
      "[[False  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n",
      "[[False False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas\n",
    "\n",
    "#Classes overview (instance methods, class methods, static methods)\n",
    "from IPython.display import display,HTML\n",
    "display(HTML('''\n",
    "<h3>Data Science</h3>\n",
    "<h4>Numpy</h4>\n",
    "<h4>Pandas</h4>\n",
    "''')) \n",
    "\n",
    "\n",
    "'''\n",
    "Important numpy links and references:\n",
    "1. https://numpy.org/doc/stable/reference/\n",
    "'''\n",
    "\n",
    "heights = [1,2,3,4,5,6]\n",
    "#print(heights*2.54)   # TypeError: can't multiply sequence by non-int of type 'float'\n",
    "\n",
    "print(np.array(heights)*2.54) # homogeneous\n",
    "print(np.array(heights)**---------------------------------------------2.54)\n",
    "\n",
    "\n",
    "# Numpy arithmetic operations\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Numpy arithmetic operations</h3>\n",
    "\n",
    "''')) \n",
    "\n",
    "list_1 = [1,2,3,4]\n",
    "list_2 = [7,8,9,0]\n",
    "\n",
    "print(list_1 + list_2)\n",
    "print(np.array(list_1) + np.array(list_2))\n",
    "np_1 = np.array(list_1)\n",
    "\n",
    "print(np_1*0.0254)\n",
    "print(len(np_1))\n",
    "print(np_1.size)\n",
    "print(np_1.shape)\n",
    "print(np_1/2)\n",
    "print(np_1//2)\n",
    "print(np_1**2)\n",
    "print(np_1+2)\n",
    "print(np_1-2)\n",
    "print(np_1)\n",
    "\n",
    "display(HTML(\"<h3>Trigonometric:</h3>\"))\n",
    "print(np.sin(np.cos(np.tan(np.sin(np_1)))))\n",
    "print(1/np.sin(np_1))   # cosec\n",
    "\n",
    "\n",
    "display(HTML(\"<h3>Median,mean,std</h3>\"))\n",
    "print(np.mean(list_1), np.median(list_1))\n",
    "print(np.min(list_1), np.max(list_1))\n",
    "print(np.std(list_1))\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Indexing, splitting, slicing</h3>\n",
    "\n",
    "''')) \n",
    "print(np_1[0])\n",
    "print(np_1[-0])\n",
    "print(np_1[-----1])\n",
    "print(np_1[---+---1])\n",
    "print(np_1[-4:-1])\n",
    "print(np_1[-4:])\n",
    "print(np_1[::-2])\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Conditional sub-setting arrays</h3>\n",
    "''')) \n",
    "\n",
    "print(np_1<3)\n",
    "print(np_1[np_1<3])\n",
    "print((np_1<3).size)\n",
    "\n",
    "display(HTML('''\n",
    "<h4>Multiple-Conditioned sub-setting arrays</h4>\n",
    "''')) \n",
    "print(np_1[np.where((np_1%2==0) & (np_1<4))])\n",
    "print(np_1[np.where((np_1%2==0) | (np_1<4))])\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Numpy functions</h3>\n",
    "''')) \n",
    "\n",
    "print(np_1)\n",
    "print(sum(np_1))\n",
    "print(max(np_1))\n",
    "print(min(np_1))\n",
    "print((np_1).min())\n",
    "print((np_1).max())\n",
    "print((np_1).mean())\n",
    "print(type(np_1))\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Multidimensional arrays</h3>\n",
    "''')) \n",
    "\n",
    "players = [[12,23],[23,34],[34,45],[45,56],[56,67],[67,78],[78,89]]\n",
    "print(np.array(players))\n",
    "players = [(12,23),(23,34),(34,45),(45,56),(56,67),(67,78),(78,89)]\n",
    "print(np.array(players))\n",
    "players0 = [{12,23},{23,34},{34,45},{45,56},{56,67},{67,78},{78,89}]\n",
    "print(np.array(players0))\n",
    "\n",
    "display(HTML('''\n",
    "<h3>Multidimensional arrays (continued)</h3>\n",
    "''')) \n",
    "\n",
    "\n",
    "players1 = [{12,23},{23,34},(34,45),(45,56),(56,67),(67,78),(78,89)]\n",
    "print(np.array(players1))\n",
    "\n",
    "np_players = np.array(players)\n",
    "print(np_players.ndim)  # dimension\n",
    "print(np_players.dtype)\n",
    "print(np_players.astype('int64').dtype)  # to change dtype\n",
    "print(np_players.itemsize)\n",
    "print(np_players)\n",
    "np_players_64 = np_players.astype('int64')\n",
    "print(np_players_64.itemsize)\n",
    "\n",
    "print(np_players_64*[2,5])\n",
    "\n",
    "print(np_players_64[:,:2]) \n",
    "print(np_players_64[:,:2]>22) \n",
    "print(np_players_64[:,:2]>[22,32]) \n",
    "\n",
    "# Extracting elements from an array\n",
    "\n",
    "import ast, numpy as np\n",
    "input_array=np.array(ast.literal_eval(input()))\n",
    "m=int(input())\n",
    "n=int(input())\n",
    "print(input_array[np.where((input_array<n)&(input_array>m))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session - 1_part_1-Teaching.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
