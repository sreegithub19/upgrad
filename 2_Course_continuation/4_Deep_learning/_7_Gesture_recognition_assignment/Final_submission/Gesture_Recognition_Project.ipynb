{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    " - Developers: Sreedhar K and Munirathinam Duraisamy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Problem Statement](#Problem_Statement)\n",
    "- [Generator](#Generator)\n",
    "- [Models](#Model)\n",
    "    - Conv3D:\n",
    "    -- [Model 1: No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10](#Model_1)\n",
    "    -- [Model 2: No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6](#Model_2)\n",
    "    -- [Model 3: No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10](#Model_3)\n",
    "    -- [Model 4: No of Epochs = 25 , batch_size = 50 ,shape = (120,120) , no of frames = 10](#Model_4)\n",
    "    -- [Model 5: No of Epochs = 25 , batch_size = 50 ,shape = (70,70) , no of frames = 18](#Model_5)\n",
    "    - CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
    "    -- [Model 6: No of Epochs = 25 , batch_size = 50 ,shape = (70,70), no of frames = 18](#Model_6)\n",
    "    -- [Model 7: No of Epochs = 20 , number of batches=20 ,shape = (50,50), number of frames=10](#Model_7)\n",
    "    - CONV2D + GRU\n",
    "    -- [Model 8: No of frames are 18 , image_height and image_witdth = (50,50) , batch_size 20 , no of epochs = 20](#Model_8)\n",
    "    - Transfer Learning Using MobileNet\n",
    "    -- [Model 9:  No of epochs = 15; batch_size = 5; shape (120,120); no of frames = 18](#Model_9)\n",
    "- [Conclusion](#Conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Introduction\">Introduction</a></h2>\n",
    "\n",
    "In this group project, we are going to build a different model that will be able to predict the 5 gestures correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Problem_Statement\">Problem Statement</a></h2>\n",
    "\n",
    "    - We want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
    "    - The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "        -- Thumbs up:  Increase the volume\n",
    "        -- Thumbs down: Decrease the volume\n",
    "        -- Left swipe: 'Jump' backwards 10 seconds\n",
    "        -- Right swipe: 'Jump' forward 10 seconds  \n",
    "        -- Stop: Pause the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following libraries to get started.\n",
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread, imresize\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('../datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('../datasets/Project_data/val.csv').readlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Generator\">Generator</a></h2>\n",
    "\n",
    "This is one of the most important parts of the code. In the generator, we are going to pre-process the images as we have images of different dimensions (50 x 50, 70 x 70 and 120 x 120) as well as create a batch of video frames. The generator should be able to take a batch of videos as input without any error. Steps like cropping/resizing and normalization should be performed successfully.  We have to experiment with `img_idx`, `y`,`z` and normalization such that we get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#!pip install scikit-image\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #img_idx = #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        #Shuffle the list of the folders in csv\n",
    "        t = np.random.permutation(folder_list)\n",
    "         #Exact batches of the batch size\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "         #Left over batches which should be handled separately\n",
    "        leftover_batches = len(t) - num_batches * batch_size\n",
    "        \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                   \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    image = resize(image, (shape_h,shape_w))\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
    "\n",
    "                #Fill the one hot encoding stuff where we maintain the label\n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if leftover_batches != 0:\n",
    "            for batch in range(num_batches): \n",
    "                # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "                batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) \n",
    "                # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
    "                batch_labels = np.zeros((batch_size,5)) \n",
    "                for folder in range(batch_size): # iterate over the batch_size\n",
    "                    imgs = os.listdir(source_path +'/'+t[batch * batch_size + folder].split(';')[0])\n",
    "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                        \n",
    "                        image = imageio.imread(source_path +'/'+t[batch * batch_size + folder].split(';')[0] +'/'+imgs[item]).astype(np.float32)\n",
    "                        image = resize(image, (shape_h,shape_w))\n",
    "\n",
    "                        batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
    "                        batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
    "                        batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
    "                        \n",
    "                    #Fill the one hot encoding stuff where we maintain the label\n",
    "                    batch_labels[folder, int(t[batch * batch_size + folder].split(';')[2])] = 1\n",
    "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video is represented above in the generator as (number of images, height, width, number of channels). We take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '../datasets/Project_data/train'\n",
    "val_path = '../datasets/Project_data/val'\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model\">Model</a></h2>\n",
    "\n",
    "Here we make the model using different functionalities that Keras provides. We must use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. We would also use `TimeDistributed` while building a Conv2D + RNN model. Also, the last layer is the softmax. We design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,  Dropout, LSTM, ConvLSTM2D\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "#write your model here\n",
    "class Conv3DModel():\n",
    "    \n",
    "    def Model3D(self,frames_to_sample,image_height,image_width):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(frames_to_sample,image_height,image_width,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "        model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        # model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        # model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation='elu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        \n",
    "        #write your optimizer TRY OUT WITH ADAM AND SGD\n",
    "        '''\n",
    "        Classes\n",
    "        class Adadelta: Optimizer that implements the Adadelta algorithm.\n",
    "\n",
    "        class Adagrad: Optimizer that implements the Adagrad algorithm.\n",
    "\n",
    "        class Adam: Optimizer that implements the Adam algorithm.\n",
    "\n",
    "        class Adamax: Optimizer that implements the Adamax algorithm.\n",
    "\n",
    "        class Ftrl: Optimizer that implements the FTRL algorithm.\n",
    "\n",
    "        class Nadam: Optimizer that implements the NAdam algorithm.\n",
    "\n",
    "        class Optimizer: Base class for Keras optimizers.\n",
    "\n",
    "        class RMSprop: Optimizer that implements the RMSprop algorithm.\n",
    "\n",
    "        class SGD: Gradient descent (with momentum) optimizer.\n",
    "        '''\n",
    "        \n",
    "        optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have written the model, the next step is to `compile` the model. When we print the `summary` of the model, we can see the total number of parameters we have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global vars\n",
    "def global_vars(img_idx,shape_h,shape_w,batch_size,num_epochs):\n",
    "    print(\"the number of images we will be feeding in the input for a video {}\".format(len(img_idx)))\n",
    "    return img_idx,shape_h,shape_w,batch_size,num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_1\">Model 1:</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 10, 120, 120, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 120, 120, 64)  256      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10, 120, 120, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 5, 60, 120, 64)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 5, 60, 120, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 60, 120, 128)  512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5, 60, 120, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 30, 60, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 30, 60, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 30, 60, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 30, 60, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 15, 30, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 15, 30, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 15, 30, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 15, 30, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 8, 15, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30720)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30720)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               15729152  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,615,813\n",
      "Trainable params: 18,614,405\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 23:10:25.215886: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-01-07 23:10:25.215953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14802 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1c:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Model 1: No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([6,8,10,12,14,16,20,22,24,26],120,120,64,15)\n",
    "conv_model1=Conv3DModel()\n",
    "conv_model1=conv_model1.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "#Fix the file path        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "#Callback to save the Keras model or model weights at some frequency.\n",
    "#ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights.\n",
    "#path to save the model file.\n",
    "#\"val_loss\" to monitor the model's total loss in validation.\n",
    "#saves when the model is considered the \"best\"\n",
    "#the model's weights will be saved\n",
    "#the minimization of the monitored quantity\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Reduce learning rate when a metric has stopped improving.\n",
    "#LR = ReduceLROnPlateau(monitor, factor, aptience, min_lr)\n",
    "#monitor: quantity to be monitored.\n",
    "#factor: factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "#patience: number of epochs with no improvement after which learning rate will be reduced.\n",
    "#min_lr: lower bound on the learning rate.\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=6 )\n",
    "# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 64\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 23:13:08.765537: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n",
      "2023-01-07 23:13:11.436186: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2023-01-07 23:13:12.320896: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2023-01-07 23:13:13.224921: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2023-01-07 23:13:24.583270: W tensorflow/core/common_runtime/bfc_allocator.cc:463] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.10GiB (rounded to 1179648000)requested by op gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-01-07 23:13:24.583371: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2023-01-07 23:13:24.583411: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 48, Chunks in use: 47. 12.0KiB allocated for chunks. 11.8KiB in use in bin. 2.7KiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583436: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 10, Chunks in use: 10. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 5.0KiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583461: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 19, Chunks in use: 17. 19.8KiB allocated for chunks. 17.2KiB in use in bin. 17.0KiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583482: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 3, Chunks in use: 2. 6.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583504: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583529: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 20.0KiB allocated for chunks. 20.0KiB in use in bin. 20.0KiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583552: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 2, Chunks in use: 2. 40.5KiB allocated for chunks. 40.5KiB in use in bin. 40.5KiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583571: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583591: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583610: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583629: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583650: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 2, Chunks in use: 2. 1.69MiB allocated for chunks. 1.69MiB in use in bin. 1.69MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583672: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 1.05MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583694: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 3, Chunks in use: 2. 9.91MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583721: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 2, Chunks in use: 1. 11.38MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583746: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 12.62MiB allocated for chunks. 12.62MiB in use in bin. 6.75MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583768: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 0. 25.25MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583791: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 1. 60.00MiB allocated for chunks. 60.00MiB in use in bin. 60.00MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583816: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 1. 237.47MiB allocated for chunks. 68.00MiB in use in bin. 60.00MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583847: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 3, Chunks in use: 3. 488.03MiB allocated for chunks. 488.03MiB in use in bin. 442.97MiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583873: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 12, Chunks in use: 9. 13.63GiB allocated for chunks. 12.08GiB in use in bin. 12.08GiB client-requested in use in bin.\n",
      "2023-01-07 23:13:24.583895: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 1.10GiB was 256.00MiB, Chunk State: \n",
      "2023-01-07 23:13:24.583929: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 383.50MiB | Requested Size: 1.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 168.75MiB | Requested Size: 168.75MiB | in_use: 1 | bin_num: -1\n",
      "2023-01-07 23:13:24.583955: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 475.44MiB | Requested Size: 168.75MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.10GiB | Requested Size: 1.10GiB | in_use: 1 | bin_num: -1\n",
      "2023-01-07 23:13:24.583981: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 721.00MiB | Requested Size: 337.50MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.10GiB | Requested Size: 1.10GiB | in_use: 1 | bin_num: -1\n",
      "2023-01-07 23:13:24.583998: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 6396772352\n",
      "2023-01-07 23:13:24.584022: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f3f8c000000 of size 2359296000 next 100\n",
      "2023-01-07 23:13:24.584040: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4018a00000 of size 1179648000 next 104\n",
      "2023-01-07 23:13:24.584058: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f405ef00000 of size 1179648000 next 21\n",
      "2023-01-07 23:13:24.584074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f40a5400000 of size 1179648000 next 112\n",
      "2023-01-07 23:13:24.584091: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f40eb900000 of size 498532352 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584107: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4294967296\n",
      "2023-01-07 23:13:24.584124: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f415c000000 of size 2359296000 next 95\n",
      "2023-01-07 23:13:24.584141: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f41e8a00000 of size 589824000 next 101\n",
      "2023-01-07 23:13:24.584157: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f420bc80000 of size 589824000 next 102\n",
      "2023-01-07 23:13:24.584174: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f422ef00000 of size 176947200 next 108\n",
      "2023-01-07 23:13:24.584191: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f42397c0000 of size 176947200 next 119\n",
      "2023-01-07 23:13:24.584208: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4244080000 of size 402128896 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584223: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4294967296\n",
      "2023-01-07 23:13:24.584240: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f42bc000000 of size 2359296000 next 94\n",
      "2023-01-07 23:13:24.584286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4348a00000 of size 1179648000 next 103\n",
      "2023-01-07 23:13:24.584303: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f438ef00000 of size 756023296 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584319: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 268435456\n",
      "2023-01-07 23:13:24.584336: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f442c000000 of size 110592000 next 89\n",
      "2023-01-07 23:13:24.584353: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4432978000 of size 157843456 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584776: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 134217728\n",
      "2023-01-07 23:13:24.584795: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4458000000 of size 62914560 next 47\n",
      "2023-01-07 23:13:24.584813: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f445bc00000 of size 71303168 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584831: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 67108864\n",
      "2023-01-07 23:13:24.584847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4460000000 of size 67108864 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584862: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 33554432\n",
      "2023-01-07 23:13:24.584880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44a0000000 of size 7077888 next 36\n",
      "2023-01-07 23:13:24.584897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44a06c0000 of size 26476544 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584912: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 16777216\n",
      "2023-01-07 23:13:24.584928: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d0000000 of size 3538944 next 28\n",
      "2023-01-07 23:13:24.584946: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d0360000 of size 13238272 next 18446744073709551615\n",
      "2023-01-07 23:13:24.584963: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2097152\n",
      "2023-01-07 23:13:24.584980: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400000 of size 256 next 1\n",
      "2023-01-07 23:13:24.584997: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400100 of size 1280 next 2\n",
      "2023-01-07 23:13:24.585014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400600 of size 256 next 3\n",
      "2023-01-07 23:13:24.585031: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400700 of size 256 next 4\n",
      "2023-01-07 23:13:24.585046: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400800 of size 256 next 5\n",
      "2023-01-07 23:13:24.585062: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400900 of size 256 next 6\n",
      "2023-01-07 23:13:24.585079: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400a00 of size 256 next 9\n",
      "2023-01-07 23:13:24.585096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400b00 of size 256 next 10\n",
      "2023-01-07 23:13:24.585112: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400c00 of size 256 next 11\n",
      "2023-01-07 23:13:24.585129: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400d00 of size 512 next 13\n",
      "2023-01-07 23:13:24.585145: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8400f00 of size 256 next 14\n",
      "2023-01-07 23:13:24.585161: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401000 of size 256 next 15\n",
      "2023-01-07 23:13:24.585176: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401100 of size 512 next 16\n",
      "2023-01-07 23:13:24.585192: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401300 of size 512 next 19\n",
      "2023-01-07 23:13:24.585209: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401500 of size 512 next 20\n",
      "2023-01-07 23:13:24.585227: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401700 of size 1024 next 22\n",
      "2023-01-07 23:13:24.585244: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401b00 of size 256 next 23\n",
      "2023-01-07 23:13:24.585261: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401c00 of size 256 next 24\n",
      "2023-01-07 23:13:24.585278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8401d00 of size 1024 next 26\n",
      "2023-01-07 23:13:24.585293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8402100 of size 1024 next 29\n",
      "2023-01-07 23:13:24.585554: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8402500 of size 1024 next 30\n",
      "2023-01-07 23:13:24.585572: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8402900 of size 1024 next 114\n",
      "2023-01-07 23:13:24.585588: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8402d00 of size 1024 next 32\n",
      "2023-01-07 23:13:24.585605: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8403100 of size 256 next 33\n",
      "2023-01-07 23:13:24.585622: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8403200 of size 256 next 34\n",
      "2023-01-07 23:13:24.585638: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8403300 of size 1024 next 37\n",
      "2023-01-07 23:13:24.585654: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8403700 of size 1024 next 38\n",
      "2023-01-07 23:13:24.585670: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8403b00 of size 1024 next 39\n",
      "2023-01-07 23:13:24.585686: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d8403f00 of size 2048 next 41\n",
      "2023-01-07 23:13:24.585702: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8404700 of size 256 next 42\n",
      "2023-01-07 23:13:24.585719: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8404800 of size 256 next 43\n",
      "2023-01-07 23:13:24.585737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8404900 of size 2048 next 45\n",
      "2023-01-07 23:13:24.585754: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405100 of size 256 next 48\n",
      "2023-01-07 23:13:24.585770: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405200 of size 256 next 49\n",
      "2023-01-07 23:13:24.585787: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405300 of size 256 next 50\n",
      "2023-01-07 23:13:24.585803: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405400 of size 256 next 51\n",
      "2023-01-07 23:13:24.585818: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405500 of size 256 next 53\n",
      "2023-01-07 23:13:24.585834: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405600 of size 256 next 54\n",
      "2023-01-07 23:13:24.585851: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405700 of size 256 next 56\n",
      "2023-01-07 23:13:24.585868: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405800 of size 256 next 57\n",
      "2023-01-07 23:13:24.585884: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405900 of size 256 next 58\n",
      "2023-01-07 23:13:24.585899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405a00 of size 256 next 59\n",
      "2023-01-07 23:13:24.585916: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405b00 of size 256 next 60\n",
      "2023-01-07 23:13:24.585932: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405c00 of size 256 next 62\n",
      "2023-01-07 23:13:24.585949: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405d00 of size 256 next 63\n",
      "2023-01-07 23:13:24.585964: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405e00 of size 256 next 64\n",
      "2023-01-07 23:13:24.585982: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8405f00 of size 512 next 66\n",
      "2023-01-07 23:13:24.585998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8406100 of size 512 next 67\n",
      "2023-01-07 23:13:24.586013: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8406300 of size 512 next 68\n",
      "2023-01-07 23:13:24.586029: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8406500 of size 1024 next 70\n",
      "2023-01-07 23:13:24.586046: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8406900 of size 1024 next 71\n",
      "2023-01-07 23:13:24.586063: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8406d00 of size 1024 next 72\n",
      "2023-01-07 23:13:24.586167: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8407100 of size 1024 next 73\n",
      "2023-01-07 23:13:24.586184: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8407500 of size 1024 next 74\n",
      "2023-01-07 23:13:24.586201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8407900 of size 1024 next 75\n",
      "2023-01-07 23:13:24.586217: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8407d00 of size 2048 next 76\n",
      "2023-01-07 23:13:24.586232: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408500 of size 256 next 78\n",
      "2023-01-07 23:13:24.586248: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408600 of size 256 next 79\n",
      "2023-01-07 23:13:24.586265: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408700 of size 256 next 80\n",
      "2023-01-07 23:13:24.586282: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408800 of size 256 next 81\n",
      "2023-01-07 23:13:24.586298: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408900 of size 256 next 82\n",
      "2023-01-07 23:13:24.586315: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408a00 of size 256 next 83\n",
      "2023-01-07 23:13:24.586331: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408b00 of size 256 next 84\n",
      "2023-01-07 23:13:24.586347: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408c00 of size 256 next 85\n",
      "2023-01-07 23:13:24.586362: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408d00 of size 256 next 86\n",
      "2023-01-07 23:13:24.586378: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8408e00 of size 256 next 87\n",
      "2023-01-07 23:13:24.586395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d8408f00 of size 256 next 127\n",
      "2023-01-07 23:13:24.586412: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409000 of size 256 next 122\n",
      "2023-01-07 23:13:24.586428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409100 of size 256 next 123\n",
      "2023-01-07 23:13:24.586444: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d8409200 of size 1024 next 90\n",
      "2023-01-07 23:13:24.586461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409600 of size 256 next 91\n",
      "2023-01-07 23:13:24.586477: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409700 of size 256 next 92\n",
      "2023-01-07 23:13:24.586493: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409800 of size 256 next 97\n",
      "2023-01-07 23:13:24.586508: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409900 of size 256 next 98\n",
      "2023-01-07 23:13:24.586525: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409a00 of size 256 next 99\n",
      "2023-01-07 23:13:24.586542: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409b00 of size 512 next 105\n",
      "2023-01-07 23:13:24.586559: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409d00 of size 512 next 106\n",
      "2023-01-07 23:13:24.586575: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8409f00 of size 512 next 107\n",
      "2023-01-07 23:13:24.586591: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d840a100 of size 1024 next 111\n",
      "2023-01-07 23:13:24.586608: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d840a500 of size 1536 next 7\n",
      "2023-01-07 23:13:24.586625: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d840ab00 of size 20736 next 8\n",
      "2023-01-07 23:13:24.586642: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d840fc00 of size 10240 next 52\n",
      "2023-01-07 23:13:24.586659: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8412400 of size 20736 next 61\n",
      "2023-01-07 23:13:24.586677: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8417500 of size 884736 next 65\n",
      "2023-01-07 23:13:24.586773: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d84ef500 of size 10240 next 77\n",
      "2023-01-07 23:13:24.586791: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d84f1d00 of size 1106688 next 18446744073709551615\n",
      "2023-01-07 23:13:24.586809: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 4194304\n",
      "2023-01-07 23:13:24.586826: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8600000 of size 884736 next 18\n",
      "2023-01-07 23:13:24.586842: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d86d8000 of size 3309568 next 18446744073709551615\n",
      "2023-01-07 23:13:24.586858: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 8388608\n",
      "2023-01-07 23:13:24.586875: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f44d8a00000 of size 3538944 next 69\n",
      "2023-01-07 23:13:24.586891: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f44d8d60000 of size 4849664 next 18446744073709551615\n",
      "2023-01-07 23:13:24.586906: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2023-01-07 23:13:24.586929: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 47 Chunks of size 256 totalling 11.8KiB\n",
      "2023-01-07 23:13:24.586949: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 10 Chunks of size 512 totalling 5.0KiB\n",
      "2023-01-07 23:13:24.586968: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 16 Chunks of size 1024 totalling 16.0KiB\n",
      "2023-01-07 23:13:24.586987: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-01-07 23:13:24.587005: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2048 totalling 4.0KiB\n",
      "2023-01-07 23:13:24.587024: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 10240 totalling 20.0KiB\n",
      "2023-01-07 23:13:24.587043: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 20736 totalling 40.5KiB\n",
      "2023-01-07 23:13:24.587060: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 884736 totalling 1.69MiB\n",
      "2023-01-07 23:13:24.587079: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 3538944 totalling 6.75MiB\n",
      "2023-01-07 23:13:24.587097: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7077888 totalling 6.75MiB\n",
      "2023-01-07 23:13:24.587117: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 13238272 totalling 12.62MiB\n",
      "2023-01-07 23:13:24.587136: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 62914560 totalling 60.00MiB\n",
      "2023-01-07 23:13:24.587154: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 71303168 totalling 68.00MiB\n",
      "2023-01-07 23:13:24.587174: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 157843456 totalling 150.53MiB\n",
      "2023-01-07 23:13:24.587193: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 176947200 totalling 337.50MiB\n",
      "2023-01-07 23:13:24.587211: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 589824000 totalling 1.10GiB\n",
      "2023-01-07 23:13:24.587229: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 1179648000 totalling 4.39GiB\n",
      "2023-01-07 23:13:24.587247: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 2359296000 totalling 6.59GiB\n",
      "2023-01-07 23:13:24.587267: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 12.71GiB\n",
      "2023-01-07 23:13:24.587285: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 15521480704 memory_limit_: 15521480704 available bytes: 0 curr_region_allocation_bytes_: 17179869184\n",
      "2023-01-07 23:13:24.587312: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                     15521480704\n",
      "InUse:                     13651347968\n",
      "MaxInUse:                  14987203584\n",
      "NumAllocs:                         285\n",
      "MaxAllocSize:               2376073216\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-01-07 23:13:24.587395: W tensorflow/core/common_runtime/bfc_allocator.cc:475] ***************************************__**************************_************************_____***\n",
      "2023-01-07 23:13:24.587503: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_pooling_gpu.cc:153 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[64,128,5,60,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[64,128,5,60,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1426]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad:\nIn[0] sequential/activation_1/Elu (defined at /usr/local/lib/python3.8/dist-packages/keras/backend.py:4893)\t\nIn[1] sequential/max_pooling3d_1/MaxPool3D (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/pooling.py:699)\t\nIn[2] gradient_tape/sequential/conv3d_2/Conv3D/Conv3DBackpropInputV2:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_420/3678496989.py\", line 1, in <module>\n>>>     conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_420/3678496989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,128,5,60,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1426]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential/max_pooling3d_1/MaxPool3D/MaxPool3DGrad:\nIn[0] sequential/activation_1/Elu (defined at /usr/local/lib/python3.8/dist-packages/keras/backend.py:4893)\t\nIn[1] sequential/max_pooling3d_1/MaxPool3D (defined at /usr/local/lib/python3.8/dist-packages/keras/layers/pooling.py:699)\t\nIn[2] gradient_tape/sequential/conv3d_2/Conv3D/Conv3DBackpropInputV2:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_420/3678496989.py\", line 1, in <module>\n>>>     conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> "
     ]
    }
   ],
   "source": [
    "conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 1 is giving the out of memory error with batch size 64. We try with less batch size and shapes to further improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_2\">Model 2:</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 6\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 6, 50, 50, 64)     5248      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 6, 50, 50, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6, 50, 50, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 3, 25, 50, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 3, 25, 50, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 3, 25, 50, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 3, 25, 50, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 2, 13, 25, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 2, 13, 25, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 2, 13, 25, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 2, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 1, 7, 13, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 1, 7, 13, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 1, 7, 13, 256)    1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1, 7, 13, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 4, 7, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               3670528   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,557,189\n",
      "Trainable params: 6,555,781\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2: No of Epochs = 20; batch_size = 20; shape = (50,50); no of frames = 6\n",
    "\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,5)),50,50,20,20)\n",
    "conv_model2=Conv3DModel()\n",
    "conv_model2=conv_model2.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.5354 - categorical_accuracy: 0.3529Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.67804, saving model to model_init_2023-01-0723_07_08.490689/model-00001-2.53544-0.35294-6.67804-0.23000.h5\n",
      "34/34 [==============================] - 38s 1s/step - loss: 2.5354 - categorical_accuracy: 0.3529 - val_loss: 6.6780 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5022 - categorical_accuracy: 0.5426\n",
      "Epoch 00002: val_loss improved from 6.67804 to 4.88644, saving model to model_init_2023-01-0723_07_08.490689/model-00002-1.50220-0.54265-4.88644-0.27000.h5\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.5022 - categorical_accuracy: 0.5426 - val_loss: 4.8864 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0338 - categorical_accuracy: 0.6471\n",
      "Epoch 00003: val_loss improved from 4.88644 to 2.60596, saving model to model_init_2023-01-0723_07_08.490689/model-00003-1.03379-0.64706-2.60596-0.34000.h5\n",
      "34/34 [==============================] - 36s 1s/step - loss: 1.0338 - categorical_accuracy: 0.6471 - val_loss: 2.6060 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9011 - categorical_accuracy: 0.6809\n",
      "Epoch 00004: val_loss improved from 2.60596 to 0.87393, saving model to model_init_2023-01-0723_07_08.490689/model-00004-0.90113-0.68088-0.87393-0.67000.h5\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.9011 - categorical_accuracy: 0.6809 - val_loss: 0.8739 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7251 - categorical_accuracy: 0.7588\n",
      "Epoch 00005: val_loss improved from 0.87393 to 0.71362, saving model to model_init_2023-01-0723_07_08.490689/model-00005-0.72513-0.75882-0.71362-0.76000.h5\n",
      "34/34 [==============================] - 37s 1s/step - loss: 0.7251 - categorical_accuracy: 0.7588 - val_loss: 0.7136 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4915 - categorical_accuracy: 0.8250\n",
      "Epoch 00006: val_loss improved from 0.71362 to 0.47442, saving model to model_init_2023-01-0723_07_08.490689/model-00006-0.49153-0.82500-0.47442-0.79000.h5\n",
      "34/34 [==============================] - 37s 1s/step - loss: 0.4915 - categorical_accuracy: 0.8250 - val_loss: 0.4744 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4816 - categorical_accuracy: 0.8382\n",
      "Epoch 00007: val_loss improved from 0.47442 to 0.39590, saving model to model_init_2023-01-0723_07_08.490689/model-00007-0.48159-0.83824-0.39590-0.84000.h5\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.4816 - categorical_accuracy: 0.8382 - val_loss: 0.3959 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3858 - categorical_accuracy: 0.8574\n",
      "Epoch 00008: val_loss did not improve from 0.39590\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.3858 - categorical_accuracy: 0.8574 - val_loss: 0.5811 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3397 - categorical_accuracy: 0.8632\n",
      "Epoch 00009: val_loss did not improve from 0.39590\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.3397 - categorical_accuracy: 0.8632 - val_loss: 0.4868 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2606 - categorical_accuracy: 0.8912\n",
      "Epoch 00010: val_loss improved from 0.39590 to 0.37009, saving model to model_init_2023-01-0723_07_08.490689/model-00010-0.26057-0.89118-0.37009-0.86000.h5\n",
      "34/34 [==============================] - 37s 1s/step - loss: 0.2606 - categorical_accuracy: 0.8912 - val_loss: 0.3701 - val_categorical_accuracy: 0.8600 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2273 - categorical_accuracy: 0.9206\n",
      "Epoch 00011: val_loss did not improve from 0.37009\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.2273 - categorical_accuracy: 0.9206 - val_loss: 0.4149 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1983 - categorical_accuracy: 0.9221\n",
      "Epoch 00012: val_loss did not improve from 0.37009\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1983 - categorical_accuracy: 0.9221 - val_loss: 0.4122 - val_categorical_accuracy: 0.8800 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2076 - categorical_accuracy: 0.9191\n",
      "Epoch 00013: val_loss did not improve from 0.37009\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.2076 - categorical_accuracy: 0.9191 - val_loss: 0.3972 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1590 - categorical_accuracy: 0.9294\n",
      "Epoch 00014: val_loss did not improve from 0.37009\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1590 - categorical_accuracy: 0.9294 - val_loss: 0.3718 - val_categorical_accuracy: 0.8800 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1342 - categorical_accuracy: 0.9588\n",
      "Epoch 00015: val_loss did not improve from 0.37009\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1342 - categorical_accuracy: 0.9588 - val_loss: 0.3925 - val_categorical_accuracy: 0.8800 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1336 - categorical_accuracy: 0.9485\n",
      "Epoch 00016: val_loss improved from 0.37009 to 0.35341, saving model to model_init_2023-01-0723_07_08.490689/model-00016-0.13360-0.94853-0.35341-0.89000.h5\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1336 - categorical_accuracy: 0.9485 - val_loss: 0.3534 - val_categorical_accuracy: 0.8900 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1253 - categorical_accuracy: 0.9574\n",
      "Epoch 00017: val_loss improved from 0.35341 to 0.32473, saving model to model_init_2023-01-0723_07_08.490689/model-00017-0.12526-0.95735-0.32473-0.89000.h5\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1253 - categorical_accuracy: 0.9574 - val_loss: 0.3247 - val_categorical_accuracy: 0.8900 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1311 - categorical_accuracy: 0.9544\n",
      "Epoch 00018: val_loss improved from 0.32473 to 0.28034, saving model to model_init_2023-01-0723_07_08.490689/model-00018-0.13112-0.95441-0.28034-0.90000.h5\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1311 - categorical_accuracy: 0.9544 - val_loss: 0.2803 - val_categorical_accuracy: 0.9000 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1515 - categorical_accuracy: 0.9441\n",
      "Epoch 00019: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 36s 1s/step - loss: 0.1515 - categorical_accuracy: 0.9441 - val_loss: 0.3437 - val_categorical_accuracy: 0.8900 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1476 - categorical_accuracy: 0.9397\n",
      "Epoch 00020: val_loss did not improve from 0.28034\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "34/34 [==============================] - 37s 1s/step - loss: 0.1476 - categorical_accuracy: 0.9397 - val_loss: 0.3662 - val_categorical_accuracy: 0.8800 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45b187f430>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    - Number of Epochs =20; Batch size=20; Number of frames=6\n",
    "    - Taking the Frames with the step size 5 and taking 6 frames with shape (50,50) have increased the performance tremendously for both the training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_3\">Model 3: </a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_8 (Conv3D)           (None, 10, 50, 50, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 10, 50, 50, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 10, 50, 50, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 5, 25, 50, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 5, 25, 50, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 5, 25, 50, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 5, 25, 50, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_9 (MaxPooling  (None, 3, 13, 25, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 3, 13, 25, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 3, 13, 25, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 3, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_10 (MaxPoolin  (None, 2, 7, 13, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 2, 7, 13, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 2, 7, 13, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2, 7, 13, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_11 (MaxPoolin  (None, 1, 4, 7, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               3670528   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,557,189\n",
      "Trainable params: 6,555,781\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 20; batch_size = 30; shape = (50,50); no of frames = 10 \n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)\n",
    "conv_model3=Conv3DModel()\n",
    "conv_model3=conv_model3.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 3.0971 - categorical_accuracy: 0.3250Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 3.0971 - categorical_accuracy: 0.3250 - val_loss: 8.7157 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5894 - categorical_accuracy: 0.4912\n",
      "Epoch 00002: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.5894 - categorical_accuracy: 0.4912 - val_loss: 7.2110 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3993 - categorical_accuracy: 0.5471\n",
      "Epoch 00003: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.3993 - categorical_accuracy: 0.5471 - val_loss: 2.8719 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0730 - categorical_accuracy: 0.6250\n",
      "Epoch 00004: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.0730 - categorical_accuracy: 0.6250 - val_loss: 1.2454 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9504 - categorical_accuracy: 0.6647\n",
      "Epoch 00005: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.9504 - categorical_accuracy: 0.6647 - val_loss: 0.8577 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7825 - categorical_accuracy: 0.7279\n",
      "Epoch 00006: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.7825 - categorical_accuracy: 0.7279 - val_loss: 0.7028 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7158 - categorical_accuracy: 0.7588\n",
      "Epoch 00007: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.7158 - categorical_accuracy: 0.7588 - val_loss: 0.6154 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6091 - categorical_accuracy: 0.7882\n",
      "Epoch 00008: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.6091 - categorical_accuracy: 0.7882 - val_loss: 0.6203 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5401 - categorical_accuracy: 0.8000\n",
      "Epoch 00009: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5401 - categorical_accuracy: 0.8000 - val_loss: 0.5543 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4613 - categorical_accuracy: 0.8294\n",
      "Epoch 00010: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.4613 - categorical_accuracy: 0.8294 - val_loss: 0.4996 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4770 - categorical_accuracy: 0.8382\n",
      "Epoch 00011: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4770 - categorical_accuracy: 0.8382 - val_loss: 0.5351 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3641 - categorical_accuracy: 0.8412\n",
      "Epoch 00012: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3641 - categorical_accuracy: 0.8412 - val_loss: 0.3877 - val_categorical_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3424 - categorical_accuracy: 0.8897\n",
      "Epoch 00013: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3424 - categorical_accuracy: 0.8897 - val_loss: 0.4451 - val_categorical_accuracy: 0.8600 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3713 - categorical_accuracy: 0.8529\n",
      "Epoch 00014: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3713 - categorical_accuracy: 0.8529 - val_loss: 0.3205 - val_categorical_accuracy: 0.8900 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2626 - categorical_accuracy: 0.9059\n",
      "Epoch 00015: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2626 - categorical_accuracy: 0.9059 - val_loss: 0.3928 - val_categorical_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2525 - categorical_accuracy: 0.9029\n",
      "Epoch 00016: val_loss did not improve from 0.28034\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2525 - categorical_accuracy: 0.9029 - val_loss: 0.3938 - val_categorical_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2054 - categorical_accuracy: 0.9265\n",
      "Epoch 00017: val_loss did not improve from 0.28034\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2054 - categorical_accuracy: 0.9265 - val_loss: 0.3423 - val_categorical_accuracy: 0.9000 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2027 - categorical_accuracy: 0.9221\n",
      "Epoch 00018: val_loss did not improve from 0.28034\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2027 - categorical_accuracy: 0.9221 - val_loss: 0.3504 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1388 - categorical_accuracy: 0.9529\n",
      "Epoch 00019: val_loss improved from 0.28034 to 0.27185, saving model to model_init_2023-01-0723_07_08.490689/model-00019-0.13883-0.95294-0.27185-0.93000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1388 - categorical_accuracy: 0.9529 - val_loss: 0.2718 - val_categorical_accuracy: 0.9300 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1476 - categorical_accuracy: 0.9529\n",
      "Epoch 00020: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1476 - categorical_accuracy: 0.9529 - val_loss: 0.5089 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45b10d0700>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 3: Number of Epochs =20; Batch size=30; shape = (50,50); Number of frames=10\n",
    "    Keeping the same shape and increasing the number of frames we have observed that Validation Accuracy decreased and slightly seems to be overfitting as compared to Model-2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_4\">Model 4: </a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 12\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_24 (Conv3D)          (None, 12, 100, 100, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 12, 100, 100, 64)  256      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 12, 100, 100, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d_24 (MaxPoolin  (None, 6, 50, 100, 64)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_25 (Conv3D)          (None, 6, 50, 100, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 6, 50, 100, 128)  512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 6, 50, 100, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_25 (MaxPoolin  (None, 3, 25, 50, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_26 (Conv3D)          (None, 3, 25, 50, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 3, 25, 50, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 3, 25, 50, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_26 (MaxPoolin  (None, 2, 13, 25, 256)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 2, 13, 25, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 2, 13, 25, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 2, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_27 (MaxPoolin  (None, 1, 7, 13, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 23296)             0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 23296)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               11928064  \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,814,725\n",
      "Trainable params: 14,813,317\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 25 , batch_size = 50 ,shape = (100,100) , no of frames = 10\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(5,28,2)),100,100,50,25)\n",
    "conv_model4=Conv3DModel()\n",
    "conv_model4=conv_model4.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 4.1673 - categorical_accuracy: 0.3000Source path =  ../datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 68s 5s/step - loss: 4.1673 - categorical_accuracy: 0.3000 - val_loss: 9.0604 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5627 - categorical_accuracy: 0.5100\n",
      "Epoch 00002: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 1.5627 - categorical_accuracy: 0.5100 - val_loss: 8.1171 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2017 - categorical_accuracy: 0.5786\n",
      "Epoch 00003: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 1.2017 - categorical_accuracy: 0.5786 - val_loss: 5.4213 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0963 - categorical_accuracy: 0.6143\n",
      "Epoch 00004: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 1.0963 - categorical_accuracy: 0.6143 - val_loss: 4.5302 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9388 - categorical_accuracy: 0.6643\n",
      "Epoch 00005: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.9388 - categorical_accuracy: 0.6643 - val_loss: 4.6850 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8365 - categorical_accuracy: 0.6886\n",
      "Epoch 00006: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 64s 5s/step - loss: 0.8365 - categorical_accuracy: 0.6886 - val_loss: 2.6657 - val_categorical_accuracy: 0.3800 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7370 - categorical_accuracy: 0.7343\n",
      "Epoch 00007: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.7370 - categorical_accuracy: 0.7343 - val_loss: 2.4923 - val_categorical_accuracy: 0.3600 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6983 - categorical_accuracy: 0.7571\n",
      "Epoch 00008: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.6983 - categorical_accuracy: 0.7571 - val_loss: 1.8047 - val_categorical_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6619 - categorical_accuracy: 0.7643\n",
      "Epoch 00009: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.6619 - categorical_accuracy: 0.7643 - val_loss: 0.9130 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6447 - categorical_accuracy: 0.7800\n",
      "Epoch 00010: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.6447 - categorical_accuracy: 0.7800 - val_loss: 1.2452 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5089 - categorical_accuracy: 0.8014\n",
      "Epoch 00011: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.5089 - categorical_accuracy: 0.8014 - val_loss: 1.0548 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4324 - categorical_accuracy: 0.8414\n",
      "Epoch 00012: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.4324 - categorical_accuracy: 0.8414 - val_loss: 0.8577 - val_categorical_accuracy: 0.7100 - lr: 5.0000e-04\n",
      "Epoch 13/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3678 - categorical_accuracy: 0.8586\n",
      "Epoch 00013: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.3678 - categorical_accuracy: 0.8586 - val_loss: 0.6372 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 14/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3012 - categorical_accuracy: 0.8843\n",
      "Epoch 00014: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.3012 - categorical_accuracy: 0.8843 - val_loss: 0.6992 - val_categorical_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 15/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3410 - categorical_accuracy: 0.8771\n",
      "Epoch 00015: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 64s 5s/step - loss: 0.3410 - categorical_accuracy: 0.8771 - val_loss: 0.5773 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 16/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3158 - categorical_accuracy: 0.8871\n",
      "Epoch 00016: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.3158 - categorical_accuracy: 0.8871 - val_loss: 0.5564 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
      "Epoch 17/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2964 - categorical_accuracy: 0.8843\n",
      "Epoch 00017: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 64s 5s/step - loss: 0.2964 - categorical_accuracy: 0.8843 - val_loss: 0.5410 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2692 - categorical_accuracy: 0.9029\n",
      "Epoch 00018: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.2692 - categorical_accuracy: 0.9029 - val_loss: 0.6502 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2413 - categorical_accuracy: 0.8986\n",
      "Epoch 00019: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 63s 5s/step - loss: 0.2413 - categorical_accuracy: 0.8986 - val_loss: 0.4812 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 20/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2679 - categorical_accuracy: 0.9029\n",
      "Epoch 00020: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.2679 - categorical_accuracy: 0.9029 - val_loss: 0.3250 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 21/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1989 - categorical_accuracy: 0.9214\n",
      "Epoch 00021: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.1989 - categorical_accuracy: 0.9214 - val_loss: 0.4485 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 22/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2406 - categorical_accuracy: 0.9186\n",
      "Epoch 00022: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 65s 5s/step - loss: 0.2406 - categorical_accuracy: 0.9186 - val_loss: 0.4619 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 23/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2214 - categorical_accuracy: 0.9171\n",
      "Epoch 00023: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.2214 - categorical_accuracy: 0.9171 - val_loss: 0.4695 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
      "Epoch 24/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1960 - categorical_accuracy: 0.9300\n",
      "Epoch 00024: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.1960 - categorical_accuracy: 0.9300 - val_loss: 0.3490 - val_categorical_accuracy: 0.9000 - lr: 2.5000e-04\n",
      "Epoch 25/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2042 - categorical_accuracy: 0.9171\n",
      "Epoch 00025: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.2042 - categorical_accuracy: 0.9171 - val_loss: 0.4417 - val_categorical_accuracy: 0.8600 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45d3d99f40>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "Model 4: This model seems to be overfitting. Increasing the image size decreases the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_5\">Model 5: </a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_28 (Conv3D)          (None, 18, 70, 70, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 18, 70, 70, 64)   256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 18, 70, 70, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPoolin  (None, 9, 35, 70, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 9, 35, 70, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 9, 35, 70, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 9, 35, 70, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPoolin  (None, 5, 18, 35, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 5, 18, 35, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 5, 18, 35, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 5, 18, 35, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPoolin  (None, 3, 9, 18, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 3, 9, 18, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 3, 9, 18, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 3, 9, 18, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPoolin  (None, 2, 5, 9, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 23040)             0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 23040)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               11796992  \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,683,653\n",
      "Trainable params: 14,682,245\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 25 , batch_size = 50 ,shape = (70,70) , no of frames = 18 \n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],70,70,50,34)\n",
    "conv_model5=Conv3DModel()\n",
    "conv_model5=conv_model5.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model5.summary()\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 3.9453 - categorical_accuracy: 0.3343Source path =  ../datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 7s/step - loss: 3.9453 - categorical_accuracy: 0.3343 - val_loss: 10.0761 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5158 - categorical_accuracy: 0.5157\n",
      "Epoch 00002: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 99s 8s/step - loss: 1.5158 - categorical_accuracy: 0.5157 - val_loss: 9.8197 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1429 - categorical_accuracy: 0.6157\n",
      "Epoch 00003: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 1.1429 - categorical_accuracy: 0.6157 - val_loss: 7.2666 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9533 - categorical_accuracy: 0.6471\n",
      "Epoch 00004: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.9533 - categorical_accuracy: 0.6471 - val_loss: 5.1850 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8597 - categorical_accuracy: 0.6771\n",
      "Epoch 00005: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.8597 - categorical_accuracy: 0.6771 - val_loss: 3.8072 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7933 - categorical_accuracy: 0.7243\n",
      "Epoch 00006: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.7933 - categorical_accuracy: 0.7243 - val_loss: 2.5675 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6117 - categorical_accuracy: 0.7671\n",
      "Epoch 00007: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.6117 - categorical_accuracy: 0.7671 - val_loss: 1.7383 - val_categorical_accuracy: 0.4600 - lr: 0.0010\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5189 - categorical_accuracy: 0.8071\n",
      "Epoch 00008: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.5189 - categorical_accuracy: 0.8071 - val_loss: 1.7888 - val_categorical_accuracy: 0.5200 - lr: 0.0010\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4875 - categorical_accuracy: 0.7986\n",
      "Epoch 00009: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.4875 - categorical_accuracy: 0.7986 - val_loss: 1.3131 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4756 - categorical_accuracy: 0.8200\n",
      "Epoch 00010: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.4756 - categorical_accuracy: 0.8200 - val_loss: 0.8968 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4075 - categorical_accuracy: 0.8486\n",
      "Epoch 00011: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.4075 - categorical_accuracy: 0.8486 - val_loss: 0.5999 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3427 - categorical_accuracy: 0.8643\n",
      "Epoch 00012: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.3427 - categorical_accuracy: 0.8643 - val_loss: 0.8899 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3406 - categorical_accuracy: 0.8557\n",
      "Epoch 00013: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.3406 - categorical_accuracy: 0.8557 - val_loss: 0.5281 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2789 - categorical_accuracy: 0.8986\n",
      "Epoch 00014: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.2789 - categorical_accuracy: 0.8986 - val_loss: 0.4352 - val_categorical_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3106 - categorical_accuracy: 0.8757\n",
      "Epoch 00015: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.3106 - categorical_accuracy: 0.8757 - val_loss: 0.4521 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2112 - categorical_accuracy: 0.9129\n",
      "Epoch 00016: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.2112 - categorical_accuracy: 0.9129 - val_loss: 0.6101 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2312 - categorical_accuracy: 0.9286\n",
      "Epoch 00017: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.2312 - categorical_accuracy: 0.9286 - val_loss: 0.4063 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1695 - categorical_accuracy: 0.9243\n",
      "Epoch 00018: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.1695 - categorical_accuracy: 0.9243 - val_loss: 0.4865 - val_categorical_accuracy: 0.8300 - lr: 5.0000e-04\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1945 - categorical_accuracy: 0.9414\n",
      "Epoch 00019: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1945 - categorical_accuracy: 0.9414 - val_loss: 0.4059 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1421 - categorical_accuracy: 0.9429\n",
      "Epoch 00020: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1421 - categorical_accuracy: 0.9429 - val_loss: 0.4053 - val_categorical_accuracy: 0.8200 - lr: 5.0000e-04\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1534 - categorical_accuracy: 0.9443\n",
      "Epoch 00021: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.1534 - categorical_accuracy: 0.9443 - val_loss: 0.4475 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1326 - categorical_accuracy: 0.9529\n",
      "Epoch 00022: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.1326 - categorical_accuracy: 0.9529 - val_loss: 0.6519 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1445 - categorical_accuracy: 0.9471\n",
      "Epoch 00023: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.1445 - categorical_accuracy: 0.9471 - val_loss: 0.4104 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1094 - categorical_accuracy: 0.9514\n",
      "Epoch 00024: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1094 - categorical_accuracy: 0.9514 - val_loss: 0.3967 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1460 - categorical_accuracy: 0.9514\n",
      "Epoch 00025: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.1460 - categorical_accuracy: 0.9514 - val_loss: 0.3925 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1304 - categorical_accuracy: 0.9600\n",
      "Epoch 00026: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1304 - categorical_accuracy: 0.9600 - val_loss: 0.3850 - val_categorical_accuracy: 0.8400 - lr: 2.5000e-04\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.0993 - categorical_accuracy: 0.9657\n",
      "Epoch 00027: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.0993 - categorical_accuracy: 0.9657 - val_loss: 0.3926 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1134 - categorical_accuracy: 0.9629\n",
      "Epoch 00028: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.1134 - categorical_accuracy: 0.9629 - val_loss: 0.5006 - val_categorical_accuracy: 0.8400 - lr: 2.5000e-04\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1206 - categorical_accuracy: 0.9514\n",
      "Epoch 00029: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.1206 - categorical_accuracy: 0.9514 - val_loss: 0.3833 - val_categorical_accuracy: 0.8900 - lr: 1.2500e-04\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.0947 - categorical_accuracy: 0.9657\n",
      "Epoch 00030: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.0947 - categorical_accuracy: 0.9657 - val_loss: 0.4078 - val_categorical_accuracy: 0.8700 - lr: 1.2500e-04\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1020 - categorical_accuracy: 0.9686\n",
      "Epoch 00031: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1020 - categorical_accuracy: 0.9686 - val_loss: 0.3902 - val_categorical_accuracy: 0.8800 - lr: 1.2500e-04\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1192 - categorical_accuracy: 0.9543\n",
      "Epoch 00032: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1192 - categorical_accuracy: 0.9543 - val_loss: 0.3097 - val_categorical_accuracy: 0.9100 - lr: 6.2500e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1025 - categorical_accuracy: 0.9686\n",
      "Epoch 00033: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1025 - categorical_accuracy: 0.9686 - val_loss: 0.3842 - val_categorical_accuracy: 0.8800 - lr: 6.2500e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1183 - categorical_accuracy: 0.9571\n",
      "Epoch 00034: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1183 - categorical_accuracy: 0.9571 - val_loss: 0.3356 - val_categorical_accuracy: 0.8700 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45b0eea3d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 5 is clearly an overfit model can see that increasing in number of frames and epochs causing the noise to be learned also from all the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Insights for Model 1 to 5:\n",
    "    Based on our experiment the final model will be model 2 - Less no of frames and reducing image size to 50,50 giving good results\n",
    "    Model 2 No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_6\">Model 6 <br></a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking image_height and image_width as 70,70 , batch size 50 and no of epochs 25\n",
    "#Switching Model architecture to Conv2D+LSTM\n",
    "# Conv2D_18, 70, 70, 16\n",
    "# LSTM_512\n",
    "# Dense_512_5\n",
    "\n",
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(512),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_2d_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_2d_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 18, 70, 70, 16)   448       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 18, 70, 70, 16)   64        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 18, 35, 35, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 18, 35, 35, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 18, 35, 35, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 18, 17, 17, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 18, 17, 17, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 18, 17, 17, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 18, 8, 8, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 18, 8, 8, 128)    73856     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 18, 8, 8, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 18, 4, 4, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 18, 4, 4, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 18, 4, 4, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 18, 2, 2, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               3147776   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,807,589\n",
      "Trainable params: 3,806,597\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, 20)\n",
    "val_generator = generator(val_path, val_doc, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6210 - categorical_accuracy: 0.2714Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.6210 - categorical_accuracy: 0.2714 - val_loss: 1.6206 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5329 - categorical_accuracy: 0.2857\n",
      "Epoch 00002: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.5329 - categorical_accuracy: 0.2857 - val_loss: 1.5486 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3947 - categorical_accuracy: 0.4393\n",
      "Epoch 00003: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.3947 - categorical_accuracy: 0.4393 - val_loss: 1.5646 - val_categorical_accuracy: 0.2250 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3099 - categorical_accuracy: 0.5393\n",
      "Epoch 00004: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.3099 - categorical_accuracy: 0.5393 - val_loss: 1.5713 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2381 - categorical_accuracy: 0.5643\n",
      "Epoch 00005: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.2381 - categorical_accuracy: 0.5643 - val_loss: 1.4654 - val_categorical_accuracy: 0.2750 - lr: 5.0000e-04\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1554 - categorical_accuracy: 0.6464\n",
      "Epoch 00006: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 38s 3s/step - loss: 1.1554 - categorical_accuracy: 0.6464 - val_loss: 1.4448 - val_categorical_accuracy: 0.2250 - lr: 5.0000e-04\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1095 - categorical_accuracy: 0.6714\n",
      "Epoch 00007: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.1095 - categorical_accuracy: 0.6714 - val_loss: 1.4399 - val_categorical_accuracy: 0.3250 - lr: 5.0000e-04\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0646 - categorical_accuracy: 0.6964\n",
      "Epoch 00008: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.0646 - categorical_accuracy: 0.6964 - val_loss: 1.3928 - val_categorical_accuracy: 0.4250 - lr: 5.0000e-04\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9903 - categorical_accuracy: 0.7357\n",
      "Epoch 00009: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9903 - categorical_accuracy: 0.7357 - val_loss: 1.2595 - val_categorical_accuracy: 0.5750 - lr: 5.0000e-04\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0392 - categorical_accuracy: 0.6607\n",
      "Epoch 00010: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.0392 - categorical_accuracy: 0.6607 - val_loss: 1.3867 - val_categorical_accuracy: 0.4000 - lr: 5.0000e-04\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9650 - categorical_accuracy: 0.7500\n",
      "Epoch 00011: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9650 - categorical_accuracy: 0.7500 - val_loss: 1.4001 - val_categorical_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9058 - categorical_accuracy: 0.7500\n",
      "Epoch 00012: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.9058 - categorical_accuracy: 0.7500 - val_loss: 1.2503 - val_categorical_accuracy: 0.5250 - lr: 2.5000e-04\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8874 - categorical_accuracy: 0.8107\n",
      "Epoch 00013: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.8874 - categorical_accuracy: 0.8107 - val_loss: 1.2658 - val_categorical_accuracy: 0.5750 - lr: 2.5000e-04\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8603 - categorical_accuracy: 0.8179\n",
      "Epoch 00014: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.8603 - categorical_accuracy: 0.8179 - val_loss: 1.1885 - val_categorical_accuracy: 0.6750 - lr: 2.5000e-04\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9051 - categorical_accuracy: 0.7536\n",
      "Epoch 00015: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 37s 3s/step - loss: 0.9051 - categorical_accuracy: 0.7536 - val_loss: 1.1962 - val_categorical_accuracy: 0.6500 - lr: 2.5000e-04\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8328 - categorical_accuracy: 0.8107\n",
      "Epoch 00016: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.8328 - categorical_accuracy: 0.8107 - val_loss: 1.1896 - val_categorical_accuracy: 0.6250 - lr: 2.5000e-04\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8348 - categorical_accuracy: 0.8036\n",
      "Epoch 00017: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.8348 - categorical_accuracy: 0.8036 - val_loss: 1.1456 - val_categorical_accuracy: 0.6500 - lr: 1.2500e-04\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8199 - categorical_accuracy: 0.8250\n",
      "Epoch 00018: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.8199 - categorical_accuracy: 0.8250 - val_loss: 1.0921 - val_categorical_accuracy: 0.6750 - lr: 1.2500e-04\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8065 - categorical_accuracy: 0.8214\n",
      "Epoch 00019: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.8065 - categorical_accuracy: 0.8214 - val_loss: 1.1181 - val_categorical_accuracy: 0.6750 - lr: 1.2500e-04\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7832 - categorical_accuracy: 0.8036\n",
      "Epoch 00020: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.7832 - categorical_accuracy: 0.8036 - val_loss: 1.1422 - val_categorical_accuracy: 0.6750 - lr: 1.2500e-04\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8083 - categorical_accuracy: 0.8357\n",
      "Epoch 00021: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.8083 - categorical_accuracy: 0.8357 - val_loss: 1.0570 - val_categorical_accuracy: 0.6750 - lr: 6.2500e-05\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8026 - categorical_accuracy: 0.8214\n",
      "Epoch 00022: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.8026 - categorical_accuracy: 0.8214 - val_loss: 1.0971 - val_categorical_accuracy: 0.6250 - lr: 6.2500e-05\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7980 - categorical_accuracy: 0.8036\n",
      "Epoch 00023: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.7980 - categorical_accuracy: 0.8036 - val_loss: 0.9883 - val_categorical_accuracy: 0.7500 - lr: 6.2500e-05\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8046 - categorical_accuracy: 0.8393\n",
      "Epoch 00024: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.8046 - categorical_accuracy: 0.8393 - val_loss: 1.1594 - val_categorical_accuracy: 0.6250 - lr: 6.2500e-05\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7838 - categorical_accuracy: 0.8393\n",
      "Epoch 00025: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7838 - categorical_accuracy: 0.8393 - val_loss: 1.0430 - val_categorical_accuracy: 0.7000 - lr: 6.2500e-05\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7967 - categorical_accuracy: 0.7857\n",
      "Epoch 00026: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.7967 - categorical_accuracy: 0.7857 - val_loss: 1.1232 - val_categorical_accuracy: 0.6000 - lr: 3.1250e-05\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7939 - categorical_accuracy: 0.8107\n",
      "Epoch 00027: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7939 - categorical_accuracy: 0.8107 - val_loss: 1.0665 - val_categorical_accuracy: 0.6250 - lr: 3.1250e-05\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8209 - categorical_accuracy: 0.8000\n",
      "Epoch 00028: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.8209 - categorical_accuracy: 0.8000 - val_loss: 1.1065 - val_categorical_accuracy: 0.6500 - lr: 1.5625e-05\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8088 - categorical_accuracy: 0.7964\n",
      "Epoch 00029: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.8088 - categorical_accuracy: 0.7964 - val_loss: 0.9371 - val_categorical_accuracy: 0.7250 - lr: 1.5625e-05\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7839 - categorical_accuracy: 0.7964\n",
      "Epoch 00030: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.7839 - categorical_accuracy: 0.7964 - val_loss: 1.0492 - val_categorical_accuracy: 0.6250 - lr: 1.5625e-05\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8163 - categorical_accuracy: 0.7750\n",
      "Epoch 00031: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.8163 - categorical_accuracy: 0.7750 - val_loss: 1.0662 - val_categorical_accuracy: 0.5500 - lr: 1.5625e-05\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7593 - categorical_accuracy: 0.8607\n",
      "Epoch 00032: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 38s 3s/step - loss: 0.7593 - categorical_accuracy: 0.8607 - val_loss: 1.0563 - val_categorical_accuracy: 0.7250 - lr: 1.0000e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7999 - categorical_accuracy: 0.8071\n",
      "Epoch 00033: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7999 - categorical_accuracy: 0.8071 - val_loss: 0.9414 - val_categorical_accuracy: 0.7250 - lr: 1.0000e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7748 - categorical_accuracy: 0.8179\n",
      "Epoch 00034: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.7748 - categorical_accuracy: 0.8179 - val_loss: 1.0715 - val_categorical_accuracy: 0.6000 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45b0eff2b0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model-6 is clearly overfitting.\n",
    "    We will change the number of frames, image size and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_7\">Model 7:</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    No of Epochs = 20 , number of batches=20 ,shape = (50,50), number of frames=10\n",
    "    img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of images we will be feeding in the input for a video 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching Model architecture to Conv2D+LSTM\n",
    "\n",
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(512),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_2d_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_2d_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_48 (TimeDi  (None, 18, 70, 70, 16)   448       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_49 (TimeDi  (None, 18, 70, 70, 16)   64        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_50 (TimeDi  (None, 18, 35, 35, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 18, 35, 35, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  (None, 18, 35, 35, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_53 (TimeDi  (None, 18, 17, 17, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_54 (TimeDi  (None, 18, 17, 17, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_55 (TimeDi  (None, 18, 17, 17, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_56 (TimeDi  (None, 18, 8, 8, 64)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_57 (TimeDi  (None, 18, 8, 8, 128)    73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_58 (TimeDi  (None, 18, 8, 8, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_59 (TimeDi  (None, 18, 4, 4, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_60 (TimeDi  (None, 18, 4, 4, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_61 (TimeDi  (None, 18, 4, 4, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_62 (TimeDi  (None, 18, 2, 2, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_63 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 512)               3147776   \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,807,589\n",
      "Trainable params: 3,806,597\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6400 - categorical_accuracy: 0.2257Source path =  ../datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 1.6400 - categorical_accuracy: 0.2257 - val_loss: 1.6119 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4621 - categorical_accuracy: 0.3857\n",
      "Epoch 00002: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 98s 7s/step - loss: 1.4621 - categorical_accuracy: 0.3857 - val_loss: 1.6539 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3164 - categorical_accuracy: 0.5343\n",
      "Epoch 00003: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 100s 8s/step - loss: 1.3164 - categorical_accuracy: 0.5343 - val_loss: 1.6457 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2054 - categorical_accuracy: 0.6114\n",
      "Epoch 00004: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 1.2054 - categorical_accuracy: 0.6114 - val_loss: 1.6614 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1580 - categorical_accuracy: 0.6157\n",
      "Epoch 00005: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 99s 8s/step - loss: 1.1580 - categorical_accuracy: 0.6157 - val_loss: 1.6448 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1065 - categorical_accuracy: 0.6857\n",
      "Epoch 00006: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 1.1065 - categorical_accuracy: 0.6857 - val_loss: 1.6563 - val_categorical_accuracy: 0.2200 - lr: 2.5000e-04\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0909 - categorical_accuracy: 0.6657\n",
      "Epoch 00007: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 1.0909 - categorical_accuracy: 0.6657 - val_loss: 1.5775 - val_categorical_accuracy: 0.2400 - lr: 2.5000e-04\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0595 - categorical_accuracy: 0.6843\n",
      "Epoch 00008: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 1.0595 - categorical_accuracy: 0.6843 - val_loss: 1.4758 - val_categorical_accuracy: 0.3100 - lr: 2.5000e-04\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0313 - categorical_accuracy: 0.7071\n",
      "Epoch 00009: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 1.0313 - categorical_accuracy: 0.7071 - val_loss: 1.5000 - val_categorical_accuracy: 0.2700 - lr: 2.5000e-04\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0105 - categorical_accuracy: 0.7314\n",
      "Epoch 00010: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 1.0105 - categorical_accuracy: 0.7314 - val_loss: 1.4192 - val_categorical_accuracy: 0.3200 - lr: 2.5000e-04\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9833 - categorical_accuracy: 0.7471\n",
      "Epoch 00011: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.9833 - categorical_accuracy: 0.7471 - val_loss: 1.4257 - val_categorical_accuracy: 0.3100 - lr: 2.5000e-04\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9577 - categorical_accuracy: 0.7543\n",
      "Epoch 00012: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.9577 - categorical_accuracy: 0.7543 - val_loss: 1.4093 - val_categorical_accuracy: 0.3300 - lr: 2.5000e-04\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9693 - categorical_accuracy: 0.7314\n",
      "Epoch 00013: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.9693 - categorical_accuracy: 0.7314 - val_loss: 1.3389 - val_categorical_accuracy: 0.3700 - lr: 2.5000e-04\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9253 - categorical_accuracy: 0.7829\n",
      "Epoch 00014: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.9253 - categorical_accuracy: 0.7829 - val_loss: 1.2844 - val_categorical_accuracy: 0.4600 - lr: 2.5000e-04\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9045 - categorical_accuracy: 0.7800\n",
      "Epoch 00015: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.9045 - categorical_accuracy: 0.7800 - val_loss: 1.2700 - val_categorical_accuracy: 0.5000 - lr: 2.5000e-04\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8999 - categorical_accuracy: 0.7671\n",
      "Epoch 00016: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.8999 - categorical_accuracy: 0.7671 - val_loss: 1.1800 - val_categorical_accuracy: 0.5300 - lr: 2.5000e-04\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8713 - categorical_accuracy: 0.7971\n",
      "Epoch 00017: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.8713 - categorical_accuracy: 0.7971 - val_loss: 1.1952 - val_categorical_accuracy: 0.5400 - lr: 2.5000e-04\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8499 - categorical_accuracy: 0.8029\n",
      "Epoch 00018: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.8499 - categorical_accuracy: 0.8029 - val_loss: 1.1857 - val_categorical_accuracy: 0.4800 - lr: 2.5000e-04\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8325 - categorical_accuracy: 0.8086\n",
      "Epoch 00019: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.8325 - categorical_accuracy: 0.8086 - val_loss: 1.1377 - val_categorical_accuracy: 0.5900 - lr: 1.2500e-04\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8276 - categorical_accuracy: 0.8129\n",
      "Epoch 00020: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.8276 - categorical_accuracy: 0.8129 - val_loss: 1.2055 - val_categorical_accuracy: 0.5300 - lr: 1.2500e-04\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8010 - categorical_accuracy: 0.8371\n",
      "Epoch 00021: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.8010 - categorical_accuracy: 0.8371 - val_loss: 1.1004 - val_categorical_accuracy: 0.6100 - lr: 1.2500e-04\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8092 - categorical_accuracy: 0.8143\n",
      "Epoch 00022: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.8092 - categorical_accuracy: 0.8143 - val_loss: 1.1514 - val_categorical_accuracy: 0.5700 - lr: 1.2500e-04\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7993 - categorical_accuracy: 0.8200\n",
      "Epoch 00023: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.7993 - categorical_accuracy: 0.8200 - val_loss: 1.0687 - val_categorical_accuracy: 0.6400 - lr: 1.2500e-04\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7837 - categorical_accuracy: 0.8314\n",
      "Epoch 00024: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.7837 - categorical_accuracy: 0.8314 - val_loss: 1.1181 - val_categorical_accuracy: 0.6100 - lr: 1.2500e-04\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7866 - categorical_accuracy: 0.8300\n",
      "Epoch 00025: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.7866 - categorical_accuracy: 0.8300 - val_loss: 1.0461 - val_categorical_accuracy: 0.6400 - lr: 1.2500e-04\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7656 - categorical_accuracy: 0.8271\n",
      "Epoch 00026: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.7656 - categorical_accuracy: 0.8271 - val_loss: 1.0303 - val_categorical_accuracy: 0.6300 - lr: 1.2500e-04\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7751 - categorical_accuracy: 0.8143\n",
      "Epoch 00027: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.7751 - categorical_accuracy: 0.8143 - val_loss: 1.0303 - val_categorical_accuracy: 0.6700 - lr: 1.2500e-04\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7494 - categorical_accuracy: 0.8457\n",
      "Epoch 00028: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 98s 8s/step - loss: 0.7494 - categorical_accuracy: 0.8457 - val_loss: 1.0734 - val_categorical_accuracy: 0.6300 - lr: 1.2500e-04\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7587 - categorical_accuracy: 0.8371\n",
      "Epoch 00029: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.7587 - categorical_accuracy: 0.8371 - val_loss: 1.0190 - val_categorical_accuracy: 0.6400 - lr: 6.2500e-05\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7298 - categorical_accuracy: 0.8543\n",
      "Epoch 00030: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.7298 - categorical_accuracy: 0.8543 - val_loss: 1.0555 - val_categorical_accuracy: 0.6300 - lr: 6.2500e-05\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7513 - categorical_accuracy: 0.8286\n",
      "Epoch 00031: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.7513 - categorical_accuracy: 0.8286 - val_loss: 1.0120 - val_categorical_accuracy: 0.6400 - lr: 6.2500e-05\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7468 - categorical_accuracy: 0.8271\n",
      "Epoch 00032: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 104s 8s/step - loss: 0.7468 - categorical_accuracy: 0.8271 - val_loss: 0.9752 - val_categorical_accuracy: 0.6600 - lr: 6.2500e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7230 - categorical_accuracy: 0.8629\n",
      "Epoch 00033: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.7230 - categorical_accuracy: 0.8629 - val_loss: 1.0061 - val_categorical_accuracy: 0.6400 - lr: 6.2500e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7335 - categorical_accuracy: 0.8471\n",
      "Epoch 00034: val_loss did not improve from 0.27185\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.7335 - categorical_accuracy: 0.8471 - val_loss: 0.9672 - val_categorical_accuracy: 0.6700 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45b03949a0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 7 is also clearly overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_8\">Model 8: </a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONV2D + GRU Changed the no of layers , no of frames are 18 , image_height and image_witdth = (50,50) , batch_size 20 , no of epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],50,50,20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential()    \n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_77 (TimeDi  (None, 18, 50, 50, 16)   448       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_78 (TimeDi  (None, 18, 50, 50, 16)   64        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_79 (TimeDi  (None, 18, 25, 25, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_80 (TimeDi  (None, 18, 25, 25, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_81 (TimeDi  (None, 18, 25, 25, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_82 (TimeDi  (None, 18, 12, 12, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_83 (TimeDi  (None, 18, 12, 12, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_84 (TimeDi  (None, 18, 12, 12, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_85 (TimeDi  (None, 18, 6, 6, 64)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_86 (TimeDi  (None, 18, 6, 6, 128)    73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_87 (TimeDi  (None, 18, 6, 6, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_88 (TimeDi  (None, 18, 3, 3, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_89 (TimeDi  (None, 18, 1152)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                233856    \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,741\n",
      "Trainable params: 336,261\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7170 - categorical_accuracy: 0.2765Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 112s 3s/step - loss: 1.7170 - categorical_accuracy: 0.2765 - val_loss: 2.0161 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4107 - categorical_accuracy: 0.4044\n",
      "Epoch 00002: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 113s 3s/step - loss: 1.4107 - categorical_accuracy: 0.4044 - val_loss: 1.9672 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2799 - categorical_accuracy: 0.4632\n",
      "Epoch 00003: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 110s 3s/step - loss: 1.2799 - categorical_accuracy: 0.4632 - val_loss: 1.8540 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1197 - categorical_accuracy: 0.5794\n",
      "Epoch 00004: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 110s 3s/step - loss: 1.1197 - categorical_accuracy: 0.5794 - val_loss: 1.6509 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9924 - categorical_accuracy: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.9924 - categorical_accuracy: 0.6471 - val_loss: 1.3881 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8911 - categorical_accuracy: 0.6735\n",
      "Epoch 00006: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 107s 3s/step - loss: 0.8911 - categorical_accuracy: 0.6735 - val_loss: 1.2953 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8097 - categorical_accuracy: 0.7147\n",
      "Epoch 00007: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.8097 - categorical_accuracy: 0.7147 - val_loss: 1.0631 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7100 - categorical_accuracy: 0.7838\n",
      "Epoch 00008: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 105s 3s/step - loss: 0.7100 - categorical_accuracy: 0.7838 - val_loss: 1.0127 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6576 - categorical_accuracy: 0.8132\n",
      "Epoch 00009: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 107s 3s/step - loss: 0.6576 - categorical_accuracy: 0.8132 - val_loss: 0.8816 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6046 - categorical_accuracy: 0.8118\n",
      "Epoch 00010: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 107s 3s/step - loss: 0.6046 - categorical_accuracy: 0.8118 - val_loss: 0.9023 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5448 - categorical_accuracy: 0.8426\n",
      "Epoch 00011: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.5448 - categorical_accuracy: 0.8426 - val_loss: 0.8734 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5243 - categorical_accuracy: 0.8441\n",
      "Epoch 00012: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 110s 3s/step - loss: 0.5243 - categorical_accuracy: 0.8441 - val_loss: 0.8999 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4616 - categorical_accuracy: 0.8838\n",
      "Epoch 00013: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 109s 3s/step - loss: 0.4616 - categorical_accuracy: 0.8838 - val_loss: 0.8339 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4002 - categorical_accuracy: 0.9059\n",
      "Epoch 00014: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.4002 - categorical_accuracy: 0.9059 - val_loss: 0.6949 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4001 - categorical_accuracy: 0.8941\n",
      "Epoch 00015: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 109s 3s/step - loss: 0.4001 - categorical_accuracy: 0.8941 - val_loss: 0.8364 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3278 - categorical_accuracy: 0.9265\n",
      "Epoch 00016: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 113s 3s/step - loss: 0.3278 - categorical_accuracy: 0.9265 - val_loss: 0.8014 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3062 - categorical_accuracy: 0.9368\n",
      "Epoch 00017: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 110s 3s/step - loss: 0.3062 - categorical_accuracy: 0.9368 - val_loss: 0.7121 - val_categorical_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2671 - categorical_accuracy: 0.9647\n",
      "Epoch 00018: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 104s 3s/step - loss: 0.2671 - categorical_accuracy: 0.9647 - val_loss: 0.7583 - val_categorical_accuracy: 0.7000 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2752 - categorical_accuracy: 0.9456\n",
      "Epoch 00019: val_loss did not improve from 0.27185\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.2752 - categorical_accuracy: 0.9456 - val_loss: 0.7569 - val_categorical_accuracy: 0.7500 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2732 - categorical_accuracy: 0.9426\n",
      "Epoch 00020: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.2732 - categorical_accuracy: 0.9426 - val_loss: 0.7896 - val_categorical_accuracy: 0.7200 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45a0cc27f0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "    Model 8 is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Model_9\">Model 9 Using Transfer Learning - MobileNet</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],120,120,5,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "from keras.applications import mobilenet\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(TimeDistributed(mobilenet_transfer,input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_90 (TimeDi  (None, 18, 3, 3, 1024)   3228864   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_91 (TimeDi  (None, 18, 3, 3, 1024)   4096      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_92 (TimeDi  (None, 18, 1, 1, 1024)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_93 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               443136    \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 5\n",
      "Epoch 1/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1403 - categorical_accuracy: 0.5353Source path =  ../datasets/Project_data/val ; batch size = 5\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27185\n",
      "133/133 [==============================] - 98s 713ms/step - loss: 1.1403 - categorical_accuracy: 0.5353 - val_loss: 0.6832 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5782 - categorical_accuracy: 0.8030\n",
      "Epoch 00002: val_loss did not improve from 0.27185\n",
      "133/133 [==============================] - 95s 719ms/step - loss: 0.5782 - categorical_accuracy: 0.8030 - val_loss: 0.6508 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4820 - categorical_accuracy: 0.8301\n",
      "Epoch 00003: val_loss did not improve from 0.27185\n",
      "133/133 [==============================] - 94s 712ms/step - loss: 0.4820 - categorical_accuracy: 0.8301 - val_loss: 2.4063 - val_categorical_accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3634 - categorical_accuracy: 0.8842\n",
      "Epoch 00004: val_loss did not improve from 0.27185\n",
      "133/133 [==============================] - 95s 720ms/step - loss: 0.3634 - categorical_accuracy: 0.8842 - val_loss: 0.3145 - val_categorical_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3953 - categorical_accuracy: 0.8827\n",
      "Epoch 00005: val_loss did not improve from 0.27185\n",
      "133/133 [==============================] - 93s 706ms/step - loss: 0.3953 - categorical_accuracy: 0.8827 - val_loss: 0.8556 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.3404 - categorical_accuracy: 0.9053\n",
      "Epoch 00006: val_loss did not improve from 0.27185\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "133/133 [==============================] - 94s 711ms/step - loss: 0.3404 - categorical_accuracy: 0.9053 - val_loss: 0.4994 - val_categorical_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1999 - categorical_accuracy: 0.9429\n",
      "Epoch 00007: val_loss did not improve from 0.27185\n",
      "133/133 [==============================] - 94s 712ms/step - loss: 0.1999 - categorical_accuracy: 0.9429 - val_loss: 0.3003 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0773 - categorical_accuracy: 0.9789\n",
      "Epoch 00008: val_loss improved from 0.27185 to 0.20716, saving model to model_init_2023-01-0723_07_08.490689/model-00008-0.07730-0.97895-0.20716-0.92000.h5\n",
      "133/133 [==============================] - 94s 715ms/step - loss: 0.0773 - categorical_accuracy: 0.9789 - val_loss: 0.2072 - val_categorical_accuracy: 0.9200 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0859 - categorical_accuracy: 0.9714\n",
      "Epoch 00009: val_loss did not improve from 0.20716\n",
      "133/133 [==============================] - 93s 705ms/step - loss: 0.0859 - categorical_accuracy: 0.9714 - val_loss: 0.2469 - val_categorical_accuracy: 0.9500 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0514 - categorical_accuracy: 0.9865\n",
      "Epoch 00010: val_loss improved from 0.20716 to 0.18679, saving model to model_init_2023-01-0723_07_08.490689/model-00010-0.05142-0.98647-0.18679-0.90000.h5\n",
      "133/133 [==============================] - 95s 717ms/step - loss: 0.0514 - categorical_accuracy: 0.9865 - val_loss: 0.1868 - val_categorical_accuracy: 0.9000 - lr: 5.0000e-04\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0466 - categorical_accuracy: 0.9880\n",
      "Epoch 00011: val_loss did not improve from 0.18679\n",
      "133/133 [==============================] - 94s 709ms/step - loss: 0.0466 - categorical_accuracy: 0.9880 - val_loss: 0.4315 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0376 - categorical_accuracy: 0.9895\n",
      "Epoch 00012: val_loss did not improve from 0.18679\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "133/133 [==============================] - 96s 726ms/step - loss: 0.0376 - categorical_accuracy: 0.9895 - val_loss: 0.5255 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0246 - categorical_accuracy: 0.9940\n",
      "Epoch 00013: val_loss improved from 0.18679 to 0.17304, saving model to model_init_2023-01-0723_07_08.490689/model-00013-0.02464-0.99398-0.17304-0.96000.h5\n",
      "133/133 [==============================] - 96s 723ms/step - loss: 0.0246 - categorical_accuracy: 0.9940 - val_loss: 0.1730 - val_categorical_accuracy: 0.9600 - lr: 2.5000e-04\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0156 - categorical_accuracy: 0.9970\n",
      "Epoch 00014: val_loss did not improve from 0.17304\n",
      "133/133 [==============================] - 94s 711ms/step - loss: 0.0156 - categorical_accuracy: 0.9970 - val_loss: 0.2625 - val_categorical_accuracy: 0.9300 - lr: 2.5000e-04\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0123 - categorical_accuracy: 0.9955\n",
      "Epoch 00015: val_loss did not improve from 0.17304\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "133/133 [==============================] - 94s 712ms/step - loss: 0.0123 - categorical_accuracy: 0.9955 - val_loss: 0.1887 - val_categorical_accuracy: 0.9500 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45a053d8e0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id=\"Conclusion\">Conclusion</a></h2> \n",
    "\n",
    "- # Model Statistics\n",
    "\n",
    "- # Conv3D\n",
    "\n",
    "- Model 1 : No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "- - - - Model 1 is giving the out of memory error with batch size 64. We try with less batch size and shapes to further improve the performance and accuracy\n",
    "\n",
    "- Model 2 : No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
    "\n",
    "- - - - Training Accuracy : 95.74% , Validation Accuracy : 89% , \n",
    "- - - - Model Analysis : Training and validation Accuracy are good so that we can conclude that with above set of parameters model is giving good results\n",
    "\n",
    "- Model 3 : No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10\n",
    "\n",
    "- - - - Training Accuracy : 95.29% , Validation Accuracy : 87% \n",
    "- - - - Model Analysis : Keeping the same shape and increasing the number of frames we have observed that validation accuracy decreased and seems to be overfitting as compared to Model-2\n",
    "\n",
    "- Model 4 : No of Epochs = 25 , batch_size = 50 ,shape = (100,100) , no of frames = 10\n",
    "\n",
    "- - - - Training Accuracy : 91.71% , Validation Accuracy : 86% \n",
    "- - - - Model Analysis : Increasing the image size decreases the accuracy. Also, this model seems to be overfitting.\n",
    "\n",
    "- Model 5 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "\n",
    "- - - - Training Accuracy : 95.71% , Validation Accuracy : 87% \n",
    "- - - - Model Analysis : This model is clearly an overfit model can see that increasing in number of frames and epochs causing the noise to be learned also from all the frames\n",
    "\n",
    "- # CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
    "\n",
    "- Model 6 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "\n",
    "- - - - Training Accuracy : 81.79% , Validation Accuracy : 60% \n",
    "- - - - Model Analysis : This model is clearly Overfitting\n",
    "\n",
    "- Model 7 : No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 10 \n",
    "\n",
    "- - - - Training Accuracy : 84.71% , Validation Accuracy : 67% \n",
    "- - - - Model Analysis : This model is clearly overfitting\n",
    "\n",
    "- # CONV2D + GRU\n",
    "\n",
    "- Model 8 : No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 18\n",
    "\n",
    "- - - - Training Accuracy : 94.26%, Validation Accuracy : 72% \n",
    "- - - - Model Analysis : This model is overfitting\n",
    "\n",
    "- # Transfer Learning Using MobileNet\n",
    "\n",
    "-  Model 9 : No of epochs = 15 , batch_size = 5 , shape  (120,120) , no of frames  = 18\n",
    "\n",
    "- - - - Training Accuracy : 99.55% , Validation Accuracy : 95% \n",
    "- - - - Model Analysis : This is so far the best model that we got with better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
