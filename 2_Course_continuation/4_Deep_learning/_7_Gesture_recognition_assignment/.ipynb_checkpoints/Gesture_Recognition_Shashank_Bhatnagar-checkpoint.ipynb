{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff73481",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "\n",
    "> Developers : Abhishek Sa and Shashank Bhatnagar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73a879",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "> The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
    "\n",
    "-  Thumbs up:  Increase the volume\n",
    "- Thumbs down: Decrease the volume\n",
    "- Left swipe: 'Jump' backwards 10 seconds\n",
    "- Right swipe: 'Jump' forward 10 seconds  \n",
    "- Stop: Pause the movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba0b7e",
   "metadata": {},
   "source": [
    "# Steps : \n",
    "  \n",
    "- Step 1 : Created Generator\n",
    "- Step 2 : Created different Models with different set of parameters and layers \n",
    "> ## Conv3D\n",
    "    \n",
    "- - - Model 1 : # Model 1 No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "- - - Model 2 : # Model 2 No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
    "    \n",
    "- - - Model 3 : # Model 3 No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10\n",
    "    \n",
    "- - - Model 4 : # Model 4 No of Epochs = 25 , batch_size = 50 ,shape = (120,120) , no of frames = 10\n",
    "- - - Model 5 : # Model 5 No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "    \n",
    "> ## CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
    "    \n",
    "- - - Model 6 : # Model 6 No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "    \n",
    "- - - Model 7 : # MOdel 7 No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 10 \n",
    "    \n",
    "> ## CONV2D + GRU\n",
    "    \n",
    "- - - Model 8 : # MOdel 8 No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 18\n",
    "    \n",
    ">  ## Transfer Learning Using MobileNet\n",
    "    \n",
    "- - - Model 9 : # MOdel 9 No of epochs = 15 , batch_size = 5 , shape  (120,120) , no of frames  = 18\n",
    "\n",
    "\n",
    "- Step 3 : Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0929f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#!python -m pip uninstall scipy --yes\n",
    "#!pip in1stall --upgrade scipy==1.2.1\n",
    "#from scipy.misc import imread, imresize\n",
    "\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecbf19f",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230c55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e9fdb",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfb910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('../datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('../datasets/Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec3c1c",
   "metadata": {},
   "source": [
    "### Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69154831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (0.19.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.19.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.19.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2022.5.4)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.6.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->scikit-image) (3.0.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "!pip install scikit-image\n",
    "from skimage.transform import resize\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        #Shuffle the list of the folders in csv- all the folders\n",
    "        videos_list = np.random.permutation(folder_list)\n",
    "        #Exact batches of the batch size\n",
    "        num_batches = int(len(videos_list)/batch_size)\n",
    "        #Left over batches which should be handled separately\n",
    "        leftover_batches = len(videos_list) - num_batches * batch_size\n",
    "        # we iterate over the number of batches\n",
    "        for batch in range(num_batches): \n",
    "            # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) \n",
    "            # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
    "            batch_labels = np.zeros((batch_size,5)) \n",
    "            #print(batch_data)\n",
    "            #print(batch_labels)\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                # determine the folder and read all images out of it\n",
    "                img_folder = os.listdir(source_path +'/'+videos_list[batch * batch_size + folder].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    \n",
    "                    image = imageio.imread(source_path +'/'+videos_list[batch * batch_size + folder].split(';')[0] +'/'+img_folder[item]).astype(np.float32)\n",
    "                    image = resize(image, (shape_h,shape_w))\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
    "                    \n",
    "                #Fill the one hot encoding stuff where we maintain the label\n",
    "                batch_labels[folder, int(videos_list[batch * batch_size + folder].split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if leftover_batches != 0:\n",
    "            for batch in range(num_batches): \n",
    "                # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "                batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) \n",
    "                # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
    "                batch_labels = np.zeros((batch_size,5)) \n",
    "                for folder in range(batch_size): # iterate over the batch_size\n",
    "                    img_folder = os.listdir(source_path +'/'+videos_list[batch * batch_size + folder].split(';')[0])\n",
    "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                        \n",
    "                        image = imageio.imread(source_path +'/'+videos_list[batch * batch_size + folder].split(';')[0] +'/'+img_folder[item]).astype(np.float32)\n",
    "                        image = resize(image, (shape_h,shape_w))\n",
    "\n",
    "                        batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
    "                        batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
    "                        batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
    "                        \n",
    "                    #Fill the one hot encoding stuff where we maintain the label\n",
    "                    batch_labels[folder, int(videos_list[batch * batch_size + folder].split(';')[2])] = 1\n",
    "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866b5a8",
   "metadata": {},
   "source": [
    "video is represented above in the generator as (number of images, height, width, number of channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e256f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "#Current time \n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "#Train and validation paths\n",
    "train_path = '../datasets/Project_data/train'\n",
    "val_path = '../datasets/Project_data/val'\n",
    "\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences) # 663\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences) # 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cd032",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe86f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Reshape, Flatten, BatchNormalization, Activation, Dropout, LSTM, ConvLSTM2D\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "# Conv3D()\n",
    "\n",
    "#input_shape=(len(img_idx),shape_h,shape_w,3)\n",
    "class Conv3DModel():\n",
    "    \n",
    "    def Model3D(self,frames_to_sample,image_height,image_width):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(frames_to_sample,image_height,image_width,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "        model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        # model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        # model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation='elu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        \n",
    "        #write your optimizer TRY OUT WITH ADAM AND SGD\n",
    "        '''\n",
    "        Classes\n",
    "        class Adadelta: Optimizer that implements the Adadelta algorithm.\n",
    "\n",
    "        class Adagrad: Optimizer that implements the Adagrad algorithm.\n",
    "\n",
    "        class Adam: Optimizer that implements the Adam algorithm.\n",
    "\n",
    "        class Adamax: Optimizer that implements the Adamax algorithm.\n",
    "\n",
    "        class Ftrl: Optimizer that implements the FTRL algorithm.\n",
    "\n",
    "        class Nadam: Optimizer that implements the NAdam algorithm.\n",
    "\n",
    "        class Optimizer: Base class for Keras optimizers.\n",
    "\n",
    "        class RMSprop: Optimizer that implements the RMSprop algorithm.\n",
    "\n",
    "        class SGD: Gradient descent (with momentum) optimizer.\n",
    "        '''\n",
    "        \n",
    "        optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3084cd",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to compile the model. When you print the summary of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2720f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global vars\n",
    "def global_vars(img_idx,shape_h,shape_w,batch_size,num_epochs):\n",
    "    print(\"the number of images we will be feeding in the input for a video {}\".format(len(img_idx)))\n",
    "    return img_idx,shape_h,shape_w,batch_size,num_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afa51f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 10, 120, 120, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 10, 120, 120, 64)  256      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10, 120, 120, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 5, 60, 120, 64)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 5, 60, 120, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 5, 60, 120, 128)  512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5, 60, 120, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 30, 60, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 30, 60, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 3, 30, 60, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 30, 60, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 15, 30, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 15, 30, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 2, 15, 30, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 15, 30, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 8, 15, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 30720)             0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 30720)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               15729152  \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,615,813\n",
      "Trainable params: 18,614,405\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1 No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([6,8,10,12,14,16,20,22,24,26],120,120,64,15)\n",
    "conv_model1=Conv3DModel()\n",
    "conv_model1=conv_model1.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbc72507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34e070a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "#Fix the file path        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "#Callback to save the Keras model or model weights at some frequency.\n",
    "#ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights.\n",
    "#path to save the model file.\n",
    "#\"val_loss\" to monitor the model's total loss in validation.\n",
    "#saves when the model is considered the \"best\"\n",
    "#the model's weights will be saved\n",
    "#the minimization of the monitored quantity\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Reduce learning rate when a metric has stopped improving.\n",
    "#LR = ReduceLROnPlateau(monitor, factor, aptience, min_lr)\n",
    "#monitor: quantity to be monitored.\n",
    "#factor: factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "#patience: number of epochs with no improvement after which learning rate will be reduced.\n",
    "#min_lr: lower bound on the learning rate.\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "\n",
    "EarlyStop = EarlyStopping(monitor='val_loss', patience=6 )\n",
    "# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9408b83",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e87fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b16bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2f01e",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150207a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87bcc9",
   "metadata": {},
   "source": [
    "> - Model 1 is giving the out of memory error hence we are reducing the batch size instead of 64 we will experiment with less batch size \n",
    "\n",
    "> - Lets experiment further with different batch size , and shapes to further improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb8657",
   "metadata": {},
   "source": [
    "If your invocation of model.fit_generator is raising StopIteration at the same sample count everytime then double check if the generator that you've passed to the fit_generator method is producing the same amount of samples as specified in \"steps_per_epoch\" parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7ba53",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f9a6e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 6\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_28 (Conv3D)          (None, 6, 50, 50, 64)     5248      \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 6, 50, 50, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 6, 50, 50, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPoolin  (None, 3, 25, 50, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 3, 25, 50, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 3, 25, 50, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 3, 25, 50, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPoolin  (None, 2, 13, 25, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 2, 13, 25, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 2, 13, 25, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 2, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPoolin  (None, 1, 7, 13, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 1, 7, 13, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 1, 7, 13, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 1, 7, 13, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPoolin  (None, 1, 4, 7, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 7168)              0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 7168)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               3670528   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,557,189\n",
      "Trainable params: 6,555,781\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2 No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
    "\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,5)),50,50,20,20)\n",
    "conv_model2=Conv3DModel()\n",
    "conv_model2=conv_model2.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b5fec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88cfa72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.6485 - categorical_accuracy: 0.3676Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.60939\n",
      "34/34 [==============================] - 41s 1s/step - loss: 2.6485 - categorical_accuracy: 0.3676 - val_loss: 8.1520 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5405 - categorical_accuracy: 0.5529\n",
      "Epoch 00002: val_loss did not improve from 0.60939\n",
      "34/34 [==============================] - 39s 1s/step - loss: 1.5405 - categorical_accuracy: 0.5529 - val_loss: 5.0378 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0853 - categorical_accuracy: 0.6265\n",
      "Epoch 00003: val_loss did not improve from 0.60939\n",
      "34/34 [==============================] - 39s 1s/step - loss: 1.0853 - categorical_accuracy: 0.6265 - val_loss: 2.1787 - val_categorical_accuracy: 0.4100 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8253 - categorical_accuracy: 0.6985\n",
      "Epoch 00004: val_loss did not improve from 0.60939\n",
      "34/34 [==============================] - 39s 1s/step - loss: 0.8253 - categorical_accuracy: 0.6985 - val_loss: 0.6510 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7315 - categorical_accuracy: 0.7265\n",
      "Epoch 00005: val_loss improved from 0.60939 to 0.51132, saving model to model_init_2022-05-1507_50_08.147158/model-00005-0.73146-0.72647-0.51132-0.80000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.7315 - categorical_accuracy: 0.7265 - val_loss: 0.5113 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5803 - categorical_accuracy: 0.7838\n",
      "Epoch 00006: val_loss did not improve from 0.51132\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.5803 - categorical_accuracy: 0.7838 - val_loss: 0.5676 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4707 - categorical_accuracy: 0.8250\n",
      "Epoch 00007: val_loss improved from 0.51132 to 0.46308, saving model to model_init_2022-05-1507_50_08.147158/model-00007-0.47071-0.82500-0.46308-0.84000.h5\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.4707 - categorical_accuracy: 0.8250 - val_loss: 0.4631 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4197 - categorical_accuracy: 0.8426\n",
      "Epoch 00008: val_loss did not improve from 0.46308\n",
      "34/34 [==============================] - 38s 1s/step - loss: 0.4197 - categorical_accuracy: 0.8426 - val_loss: 0.4908 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2894 - categorical_accuracy: 0.8750\n",
      "Epoch 00009: val_loss did not improve from 0.46308\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.2894 - categorical_accuracy: 0.8750 - val_loss: 0.5752 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2508 - categorical_accuracy: 0.9103\n",
      "Epoch 00010: val_loss did not improve from 0.46308\n",
      "34/34 [==============================] - 38s 1s/step - loss: 0.2508 - categorical_accuracy: 0.9103 - val_loss: 0.4881 - val_categorical_accuracy: 0.8600 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2396 - categorical_accuracy: 0.9088\n",
      "Epoch 00011: val_loss did not improve from 0.46308\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 39s 1s/step - loss: 0.2396 - categorical_accuracy: 0.9088 - val_loss: 0.4789 - val_categorical_accuracy: 0.8500 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2443 - categorical_accuracy: 0.9088\n",
      "Epoch 00012: val_loss did not improve from 0.46308\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.2443 - categorical_accuracy: 0.9088 - val_loss: 0.4879 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1879 - categorical_accuracy: 0.9250\n",
      "Epoch 00013: val_loss improved from 0.46308 to 0.40618, saving model to model_init_2022-05-1507_50_08.147158/model-00013-0.18786-0.92500-0.40618-0.88000.h5\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.1879 - categorical_accuracy: 0.9250 - val_loss: 0.4062 - val_categorical_accuracy: 0.8800 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1805 - categorical_accuracy: 0.9279\n",
      "Epoch 00014: val_loss improved from 0.40618 to 0.32226, saving model to model_init_2022-05-1507_50_08.147158/model-00014-0.18047-0.92794-0.32226-0.88000.h5\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.1805 - categorical_accuracy: 0.9279 - val_loss: 0.3223 - val_categorical_accuracy: 0.8800 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1788 - categorical_accuracy: 0.9309\n",
      "Epoch 00015: val_loss did not improve from 0.32226\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.1788 - categorical_accuracy: 0.9309 - val_loss: 0.3568 - val_categorical_accuracy: 0.8900 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1664 - categorical_accuracy: 0.9456\n",
      "Epoch 00016: val_loss did not improve from 0.32226\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 37s 1s/step - loss: 0.1664 - categorical_accuracy: 0.9456 - val_loss: 0.3475 - val_categorical_accuracy: 0.8900 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1762 - categorical_accuracy: 0.9382\n",
      "Epoch 00017: val_loss improved from 0.32226 to 0.31969, saving model to model_init_2022-05-1507_50_08.147158/model-00017-0.17624-0.93824-0.31969-0.89000.h5\n",
      "34/34 [==============================] - 44s 1s/step - loss: 0.1762 - categorical_accuracy: 0.9382 - val_loss: 0.3197 - val_categorical_accuracy: 0.8900 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1246 - categorical_accuracy: 0.9574\n",
      "Epoch 00018: val_loss improved from 0.31969 to 0.24244, saving model to model_init_2022-05-1507_50_08.147158/model-00018-0.12461-0.95735-0.24244-0.93000.h5\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.1246 - categorical_accuracy: 0.9574 - val_loss: 0.2424 - val_categorical_accuracy: 0.9300 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1355 - categorical_accuracy: 0.9559\n",
      "Epoch 00019: val_loss did not improve from 0.24244\n",
      "34/34 [==============================] - 39s 1s/step - loss: 0.1355 - categorical_accuracy: 0.9559 - val_loss: 0.3783 - val_categorical_accuracy: 0.8600 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1374 - categorical_accuracy: 0.9456\n",
      "Epoch 00020: val_loss did not improve from 0.24244\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "34/34 [==============================] - 39s 1s/step - loss: 0.1374 - categorical_accuracy: 0.9456 - val_loss: 0.2979 - val_categorical_accuracy: 0.9200 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bacc482e0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e5467",
   "metadata": {},
   "source": [
    "> # Model 2 \n",
    "\n",
    "- No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
    "\n",
    "- Taking the Frames with the step size 5 and taking 6 frames with shape (50,50) have increased the performance tremendously for both the \n",
    "training and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae67a0",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "\n",
    "- No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba35425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 10, 50, 50, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 50, 50, 64)   256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10, 50, 50, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 5, 25, 50, 64)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 5, 25, 50, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 5, 25, 50, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5, 25, 50, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 3, 13, 25, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 3, 13, 25, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 3, 13, 25, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3, 13, 25, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 7, 13, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 7, 13, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 7, 13, 256)    1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 7, 13, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 4, 7, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7168)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7168)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3670528   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,557,189\n",
      "Trainable params: 6,555,781\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 11:44:34.047373: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-05-15 11:44:34.047447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46483 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1b:00.0, compute capability: 8.6\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)\n",
    "conv_model3=Conv3DModel()\n",
    "conv_model3=conv_model3.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9de1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca69664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 11:45:07.309110: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n",
      "2022-05-15 11:45:09.162049: I tensorflow/stream_executor/cuda/cuda_blas.cc:1792] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - ETA: 0s - loss: 3.2969 - categorical_accuracy: 0.3265Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.97417, saving model to model_init_2022-05-1511_43_15.303156/model-00001-3.29694-0.32647-9.97417-0.23000.h5\n",
      "34/34 [==============================] - 68s 2s/step - loss: 3.2969 - categorical_accuracy: 0.3265 - val_loss: 9.9742 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8959 - categorical_accuracy: 0.4750\n",
      "Epoch 00002: val_loss improved from 9.97417 to 5.05766, saving model to model_init_2022-05-1511_43_15.303156/model-00002-1.89586-0.47500-5.05766-0.22000.h5\n",
      "34/34 [==============================] - 62s 2s/step - loss: 1.8959 - categorical_accuracy: 0.4750 - val_loss: 5.0577 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3686 - categorical_accuracy: 0.5294\n",
      "Epoch 00003: val_loss improved from 5.05766 to 2.63396, saving model to model_init_2022-05-1511_43_15.303156/model-00003-1.36857-0.52941-2.63396-0.37000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.3686 - categorical_accuracy: 0.5294 - val_loss: 2.6340 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2707 - categorical_accuracy: 0.5676\n",
      "Epoch 00004: val_loss improved from 2.63396 to 2.22663, saving model to model_init_2022-05-1511_43_15.303156/model-00004-1.27073-0.56765-2.22663-0.40000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.2707 - categorical_accuracy: 0.5676 - val_loss: 2.2266 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9787 - categorical_accuracy: 0.6676\n",
      "Epoch 00005: val_loss improved from 2.22663 to 1.26329, saving model to model_init_2022-05-1511_43_15.303156/model-00005-0.97875-0.66765-1.26329-0.56000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.9787 - categorical_accuracy: 0.6676 - val_loss: 1.2633 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8432 - categorical_accuracy: 0.6882\n",
      "Epoch 00006: val_loss improved from 1.26329 to 0.70550, saving model to model_init_2022-05-1511_43_15.303156/model-00006-0.84319-0.68824-0.70550-0.72000.h5\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.8432 - categorical_accuracy: 0.6882 - val_loss: 0.7055 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7660 - categorical_accuracy: 0.7279\n",
      "Epoch 00007: val_loss improved from 0.70550 to 0.69015, saving model to model_init_2022-05-1511_43_15.303156/model-00007-0.76597-0.72794-0.69015-0.79000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.7660 - categorical_accuracy: 0.7279 - val_loss: 0.6901 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7016 - categorical_accuracy: 0.7441\n",
      "Epoch 00008: val_loss did not improve from 0.69015\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.7016 - categorical_accuracy: 0.7441 - val_loss: 0.7214 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6098 - categorical_accuracy: 0.7779\n",
      "Epoch 00009: val_loss improved from 0.69015 to 0.61000, saving model to model_init_2022-05-1511_43_15.303156/model-00009-0.60980-0.77794-0.61000-0.76000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.6098 - categorical_accuracy: 0.7779 - val_loss: 0.6100 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5392 - categorical_accuracy: 0.8103\n",
      "Epoch 00010: val_loss did not improve from 0.61000\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.5392 - categorical_accuracy: 0.8103 - val_loss: 0.6530 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4619 - categorical_accuracy: 0.8338\n",
      "Epoch 00011: val_loss did not improve from 0.61000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4619 - categorical_accuracy: 0.8338 - val_loss: 0.6743 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4059 - categorical_accuracy: 0.8338\n",
      "Epoch 00012: val_loss improved from 0.61000 to 0.47646, saving model to model_init_2022-05-1511_43_15.303156/model-00012-0.40588-0.83382-0.47646-0.81000.h5\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.4059 - categorical_accuracy: 0.8338 - val_loss: 0.4765 - val_categorical_accuracy: 0.8100 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3278 - categorical_accuracy: 0.8750\n",
      "Epoch 00013: val_loss did not improve from 0.47646\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3278 - categorical_accuracy: 0.8750 - val_loss: 0.5446 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3297 - categorical_accuracy: 0.8897\n",
      "Epoch 00014: val_loss did not improve from 0.47646\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.3297 - categorical_accuracy: 0.8897 - val_loss: 0.5786 - val_categorical_accuracy: 0.8100 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2354 - categorical_accuracy: 0.9147\n",
      "Epoch 00015: val_loss improved from 0.47646 to 0.46415, saving model to model_init_2022-05-1511_43_15.303156/model-00015-0.23535-0.91471-0.46415-0.86000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2354 - categorical_accuracy: 0.9147 - val_loss: 0.4642 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2280 - categorical_accuracy: 0.9235\n",
      "Epoch 00016: val_loss did not improve from 0.46415\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2280 - categorical_accuracy: 0.9235 - val_loss: 0.4742 - val_categorical_accuracy: 0.8300 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2618 - categorical_accuracy: 0.9088\n",
      "Epoch 00017: val_loss did not improve from 0.46415\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2618 - categorical_accuracy: 0.9088 - val_loss: 0.4909 - val_categorical_accuracy: 0.8300 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2782 - categorical_accuracy: 0.9015\n",
      "Epoch 00018: val_loss improved from 0.46415 to 0.34506, saving model to model_init_2022-05-1511_43_15.303156/model-00018-0.27820-0.90147-0.34506-0.87000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2782 - categorical_accuracy: 0.9015 - val_loss: 0.3451 - val_categorical_accuracy: 0.8700 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2163 - categorical_accuracy: 0.9162\n",
      "Epoch 00019: val_loss did not improve from 0.34506\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2163 - categorical_accuracy: 0.9162 - val_loss: 0.5163 - val_categorical_accuracy: 0.8400 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2169 - categorical_accuracy: 0.9279\n",
      "Epoch 00020: val_loss did not improve from 0.34506\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.2169 - categorical_accuracy: 0.9279 - val_loss: 0.4974 - val_categorical_accuracy: 0.8200 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf98547a30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ac88d",
   "metadata": {},
   "source": [
    "# Model 3 is overfitting\n",
    "- No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10\n",
    "\n",
    "- Keeping the same shape and increasing the no of frames we have observed the overfitting as compared to model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caabe65",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c2bcf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 12\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 12, 120, 120, 64)  5248      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 12, 120, 120, 64)  256      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 12, 120, 120, 64)  0         \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 6, 60, 120, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 6, 60, 120, 128)   221312    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 6, 60, 120, 128)  512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 6, 60, 120, 128)   0         \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 3, 30, 60, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_6 (Conv3D)           (None, 3, 30, 60, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 3, 30, 60, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 3, 30, 60, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 2, 15, 30, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 2, 15, 30, 256)    1769728   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 2, 15, 30, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2, 15, 30, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 8, 15, 256)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 30720)             0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30720)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               15729152  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,615,813\n",
      "Trainable params: 18,614,405\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#No of Epochs = 25 , batch_size = 50 ,shape = (120,120) , no of frames = 10\n",
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(5,28,2)),120,120,50,25)\n",
    "conv_model4=Conv3DModel()\n",
    "conv_model4=conv_model4.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1d0b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3122fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 3.8824 - categorical_accuracy: 0.3129Source path =  ../datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 71s 5s/step - loss: 3.8824 - categorical_accuracy: 0.3129 - val_loss: 9.1019 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3458 - categorical_accuracy: 0.5614\n",
      "Epoch 00002: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 1.3458 - categorical_accuracy: 0.5614 - val_loss: 7.2994 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1223 - categorical_accuracy: 0.6329\n",
      "Epoch 00003: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 1.1223 - categorical_accuracy: 0.6329 - val_loss: 4.6450 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9862 - categorical_accuracy: 0.6500\n",
      "Epoch 00004: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.9862 - categorical_accuracy: 0.6500 - val_loss: 5.6161 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9020 - categorical_accuracy: 0.6557\n",
      "Epoch 00005: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.9020 - categorical_accuracy: 0.6557 - val_loss: 3.1087 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7229 - categorical_accuracy: 0.7457\n",
      "Epoch 00006: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.7229 - categorical_accuracy: 0.7457 - val_loss: 2.1009 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6150 - categorical_accuracy: 0.7700\n",
      "Epoch 00007: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.6150 - categorical_accuracy: 0.7700 - val_loss: 1.5945 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6335 - categorical_accuracy: 0.7657\n",
      "Epoch 00008: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.6335 - categorical_accuracy: 0.7657 - val_loss: 2.1248 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5551 - categorical_accuracy: 0.8043\n",
      "Epoch 00009: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.5551 - categorical_accuracy: 0.8043 - val_loss: 1.0614 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4512 - categorical_accuracy: 0.8300\n",
      "Epoch 00010: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.4512 - categorical_accuracy: 0.8300 - val_loss: 1.0343 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3998 - categorical_accuracy: 0.8571\n",
      "Epoch 00011: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.3998 - categorical_accuracy: 0.8571 - val_loss: 0.9000 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3394 - categorical_accuracy: 0.8686\n",
      "Epoch 00012: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.3394 - categorical_accuracy: 0.8686 - val_loss: 0.9071 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3449 - categorical_accuracy: 0.8757\n",
      "Epoch 00013: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.3449 - categorical_accuracy: 0.8757 - val_loss: 0.6822 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3179 - categorical_accuracy: 0.8871\n",
      "Epoch 00014: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.3179 - categorical_accuracy: 0.8871 - val_loss: 0.4657 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3040 - categorical_accuracy: 0.8900\n",
      "Epoch 00015: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.3040 - categorical_accuracy: 0.8900 - val_loss: 0.5978 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2625 - categorical_accuracy: 0.9043\n",
      "Epoch 00016: val_loss did not improve from 0.34506\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.2625 - categorical_accuracy: 0.9043 - val_loss: 0.6334 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2406 - categorical_accuracy: 0.9029\n",
      "Epoch 00017: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.2406 - categorical_accuracy: 0.9029 - val_loss: 0.5440 - val_categorical_accuracy: 0.8600 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1991 - categorical_accuracy: 0.9229\n",
      "Epoch 00018: val_loss did not improve from 0.34506\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.1991 - categorical_accuracy: 0.9229 - val_loss: 0.4949 - val_categorical_accuracy: 0.8600 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1778 - categorical_accuracy: 0.9300\n",
      "Epoch 00019: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.1778 - categorical_accuracy: 0.9300 - val_loss: 0.5310 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
      "Epoch 20/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1476 - categorical_accuracy: 0.9443\n",
      "Epoch 00020: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.1476 - categorical_accuracy: 0.9443 - val_loss: 0.3672 - val_categorical_accuracy: 0.8900 - lr: 2.5000e-04\n",
      "Epoch 21/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1479 - categorical_accuracy: 0.9500\n",
      "Epoch 00021: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.1479 - categorical_accuracy: 0.9500 - val_loss: 0.5294 - val_categorical_accuracy: 0.8600 - lr: 2.5000e-04\n",
      "Epoch 22/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1562 - categorical_accuracy: 0.9414\n",
      "Epoch 00022: val_loss did not improve from 0.34506\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 67s 5s/step - loss: 0.1562 - categorical_accuracy: 0.9414 - val_loss: 0.5297 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
      "Epoch 23/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1632 - categorical_accuracy: 0.9314\n",
      "Epoch 00023: val_loss did not improve from 0.34506\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.1632 - categorical_accuracy: 0.9314 - val_loss: 0.5188 - val_categorical_accuracy: 0.8600 - lr: 1.2500e-04\n",
      "Epoch 24/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1540 - categorical_accuracy: 0.9329\n",
      "Epoch 00024: val_loss improved from 0.34506 to 0.28723, saving model to model_init_2022-05-1511_43_15.303156/model-00024-0.15396-0.93286-0.28723-0.91000.h5\n",
      "14/14 [==============================] - 68s 5s/step - loss: 0.1540 - categorical_accuracy: 0.9329 - val_loss: 0.2872 - val_categorical_accuracy: 0.9100 - lr: 1.2500e-04\n",
      "Epoch 25/25\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1281 - categorical_accuracy: 0.9514\n",
      "Epoch 00025: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 66s 5s/step - loss: 0.1281 - categorical_accuracy: 0.9514 - val_loss: 0.5218 - val_categorical_accuracy: 0.8700 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf98552520>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cf8cd",
   "metadata": {},
   "source": [
    "# Model 4 seems to be overfitting seems increase the image size decreasing the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe9efd",
   "metadata": {},
   "source": [
    "# Model 5\n",
    "\n",
    "taking image_height and image_width as 70,70 , batch size 50 and no of epochs 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6148c6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_16 (Conv3D)          (None, 18, 70, 70, 64)    5248      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 18, 70, 70, 64)   256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 18, 70, 70, 64)    0         \n",
      "                                                                 \n",
      " max_pooling3d_16 (MaxPoolin  (None, 9, 35, 70, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 9, 35, 70, 128)    221312    \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 9, 35, 70, 128)   512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 9, 35, 70, 128)    0         \n",
      "                                                                 \n",
      " max_pooling3d_17 (MaxPoolin  (None, 5, 18, 35, 128)   0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 5, 18, 35, 256)    884992    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 5, 18, 35, 256)   1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 5, 18, 35, 256)    0         \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 3, 9, 18, 256)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 3, 9, 18, 256)     1769728   \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 3, 9, 18, 256)    1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 3, 9, 18, 256)     0         \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 2, 5, 9, 256)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 23040)             0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 23040)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               11796992  \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,683,653\n",
      "Trainable params: 14,682,245\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],70,70,50,34)\n",
    "conv_model5=Conv3DModel()\n",
    "conv_model5=conv_model5.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
    "conv_model5.summary()\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e44ebab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9e6be64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ae6379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 50\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 4.7419 - categorical_accuracy: 0.3457Source path =  ../datasets/Project_data/val ; batch size = 50\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 4.7419 - categorical_accuracy: 0.3457 - val_loss: 9.7583 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.4842 - categorical_accuracy: 0.4943\n",
      "Epoch 00002: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 103s 8s/step - loss: 1.4842 - categorical_accuracy: 0.4943 - val_loss: 6.6219 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1513 - categorical_accuracy: 0.6143\n",
      "Epoch 00003: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 104s 8s/step - loss: 1.1513 - categorical_accuracy: 0.6143 - val_loss: 6.9907 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0693 - categorical_accuracy: 0.6300\n",
      "Epoch 00004: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 1.0693 - categorical_accuracy: 0.6300 - val_loss: 4.5945 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9458 - categorical_accuracy: 0.6714\n",
      "Epoch 00005: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.9458 - categorical_accuracy: 0.6714 - val_loss: 4.2001 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.8077 - categorical_accuracy: 0.7243\n",
      "Epoch 00006: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 105s 8s/step - loss: 0.8077 - categorical_accuracy: 0.7243 - val_loss: 3.2180 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.7530 - categorical_accuracy: 0.7257\n",
      "Epoch 00007: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.7530 - categorical_accuracy: 0.7257 - val_loss: 1.8526 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5688 - categorical_accuracy: 0.8071\n",
      "Epoch 00008: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.5688 - categorical_accuracy: 0.8071 - val_loss: 1.3130 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5064 - categorical_accuracy: 0.8000\n",
      "Epoch 00009: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.5064 - categorical_accuracy: 0.8000 - val_loss: 1.0132 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4722 - categorical_accuracy: 0.8286\n",
      "Epoch 00010: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.4722 - categorical_accuracy: 0.8286 - val_loss: 1.0852 - val_categorical_accuracy: 0.6400 - lr: 0.0010\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3931 - categorical_accuracy: 0.8557\n",
      "Epoch 00011: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.3931 - categorical_accuracy: 0.8557 - val_loss: 0.9097 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4034 - categorical_accuracy: 0.8586\n",
      "Epoch 00012: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.4034 - categorical_accuracy: 0.8586 - val_loss: 0.7568 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2768 - categorical_accuracy: 0.9029\n",
      "Epoch 00013: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.2768 - categorical_accuracy: 0.9029 - val_loss: 0.5974 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3114 - categorical_accuracy: 0.8843\n",
      "Epoch 00014: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.3114 - categorical_accuracy: 0.8843 - val_loss: 0.5922 - val_categorical_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2860 - categorical_accuracy: 0.8929\n",
      "Epoch 00015: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.2860 - categorical_accuracy: 0.8929 - val_loss: 0.5476 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2645 - categorical_accuracy: 0.8957\n",
      "Epoch 00016: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.2645 - categorical_accuracy: 0.8957 - val_loss: 0.5677 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1932 - categorical_accuracy: 0.9214\n",
      "Epoch 00017: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.1932 - categorical_accuracy: 0.9214 - val_loss: 0.6077 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2136 - categorical_accuracy: 0.9114\n",
      "Epoch 00018: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.2136 - categorical_accuracy: 0.9114 - val_loss: 0.5294 - val_categorical_accuracy: 0.7900 - lr: 5.0000e-04\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2244 - categorical_accuracy: 0.9300\n",
      "Epoch 00019: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.2244 - categorical_accuracy: 0.9300 - val_loss: 0.5682 - val_categorical_accuracy: 0.8100 - lr: 5.0000e-04\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1484 - categorical_accuracy: 0.9443\n",
      "Epoch 00020: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1484 - categorical_accuracy: 0.9443 - val_loss: 0.6305 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1829 - categorical_accuracy: 0.9343\n",
      "Epoch 00021: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.1829 - categorical_accuracy: 0.9343 - val_loss: 0.5907 - val_categorical_accuracy: 0.8100 - lr: 2.5000e-04\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1585 - categorical_accuracy: 0.9443\n",
      "Epoch 00022: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.1585 - categorical_accuracy: 0.9443 - val_loss: 0.4583 - val_categorical_accuracy: 0.8300 - lr: 2.5000e-04\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2123 - categorical_accuracy: 0.9257\n",
      "Epoch 00023: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.2123 - categorical_accuracy: 0.9257 - val_loss: 0.5860 - val_categorical_accuracy: 0.8100 - lr: 2.5000e-04\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1654 - categorical_accuracy: 0.9329\n",
      "Epoch 00024: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.1654 - categorical_accuracy: 0.9329 - val_loss: 0.5619 - val_categorical_accuracy: 0.7700 - lr: 2.5000e-04\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1079 - categorical_accuracy: 0.9586\n",
      "Epoch 00025: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.1079 - categorical_accuracy: 0.9586 - val_loss: 0.5438 - val_categorical_accuracy: 0.8200 - lr: 1.2500e-04\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1312 - categorical_accuracy: 0.9486\n",
      "Epoch 00026: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1312 - categorical_accuracy: 0.9486 - val_loss: 0.5219 - val_categorical_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1190 - categorical_accuracy: 0.9586\n",
      "Epoch 00027: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1190 - categorical_accuracy: 0.9586 - val_loss: 0.5303 - val_categorical_accuracy: 0.8100 - lr: 6.2500e-05\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1194 - categorical_accuracy: 0.9514\n",
      "Epoch 00028: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 99s 8s/step - loss: 0.1194 - categorical_accuracy: 0.9514 - val_loss: 0.6201 - val_categorical_accuracy: 0.8100 - lr: 6.2500e-05\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1376 - categorical_accuracy: 0.9443\n",
      "Epoch 00029: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.1376 - categorical_accuracy: 0.9443 - val_loss: 0.5273 - val_categorical_accuracy: 0.8100 - lr: 3.1250e-05\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1248 - categorical_accuracy: 0.9571\n",
      "Epoch 00030: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1248 - categorical_accuracy: 0.9571 - val_loss: 0.5204 - val_categorical_accuracy: 0.8200 - lr: 3.1250e-05\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1253 - categorical_accuracy: 0.9529\n",
      "Epoch 00031: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 101s 8s/step - loss: 0.1253 - categorical_accuracy: 0.9529 - val_loss: 0.5215 - val_categorical_accuracy: 0.8200 - lr: 1.5625e-05\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1144 - categorical_accuracy: 0.9614\n",
      "Epoch 00032: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 [==============================] - 102s 8s/step - loss: 0.1144 - categorical_accuracy: 0.9614 - val_loss: 0.6718 - val_categorical_accuracy: 0.8000 - lr: 1.5625e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1465 - categorical_accuracy: 0.9429\n",
      "Epoch 00033: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 100s 8s/step - loss: 0.1465 - categorical_accuracy: 0.9429 - val_loss: 0.5205 - val_categorical_accuracy: 0.8300 - lr: 1.0000e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1224 - categorical_accuracy: 0.9514\n",
      "Epoch 00034: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 103s 8s/step - loss: 0.1224 - categorical_accuracy: 0.9514 - val_loss: 0.4541 - val_categorical_accuracy: 0.8500 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fafb713e970>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d086ca",
   "metadata": {},
   "source": [
    "# Model 5 is clearly an overfit model can see that increasing no of frames and epochs causing the noise to be learned also from all the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f97ae",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "\n",
    "> Based on our experiment the final model will be model 2 - Less no of frames and reducing image size to 50,50 giving good results\n",
    "   \n",
    "# Model 2 No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1011a",
   "metadata": {},
   "source": [
    "# Model 6 CNN + RNN : CNN2D LSTM Model - TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605f119",
   "metadata": {},
   "source": [
    "# Taking image_height and image_width as 70,70 , batch size 50 and no of epochs 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04f42bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching Model architecture to Conv2D+LSTM\n",
    "# Conv2D_18, 70, 70, 16\n",
    "# LSTM_512\n",
    "# Dense_512_5\n",
    "\n",
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(512),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_2d_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "191bd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf0d6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_2d_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 18, 70, 70, 16)   448       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 18, 70, 70, 16)   64        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 18, 35, 35, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 18, 35, 35, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 18, 35, 35, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 18, 17, 17, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 18, 17, 17, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 18, 17, 17, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 18, 8, 8, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 18, 8, 8, 128)    73856     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 18, 8, 8, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 18, 4, 4, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 18, 4, 4, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 18, 4, 4, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 18, 2, 2, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               3147776   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,807,589\n",
      "Trainable params: 3,806,597\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30d919c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = generator(train_path, train_doc, 20)\n",
    "val_generator = generator(val_path, val_doc, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc0da402",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42952fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c9c67b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6374 - categorical_accuracy: 0.2357Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 43s 3s/step - loss: 1.6374 - categorical_accuracy: 0.2357 - val_loss: 1.6335 - val_categorical_accuracy: 0.2250 - lr: 0.0010\n",
      "Epoch 2/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.5488 - categorical_accuracy: 0.3071\n",
      "Epoch 00002: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.5488 - categorical_accuracy: 0.3071 - val_loss: 1.6343 - val_categorical_accuracy: 0.2250 - lr: 0.0010\n",
      "Epoch 3/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.3880 - categorical_accuracy: 0.4536\n",
      "Epoch 00003: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.3880 - categorical_accuracy: 0.4536 - val_loss: 1.6635 - val_categorical_accuracy: 0.1750 - lr: 0.0010\n",
      "Epoch 4/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2670 - categorical_accuracy: 0.5250\n",
      "Epoch 00004: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.2670 - categorical_accuracy: 0.5250 - val_loss: 1.5226 - val_categorical_accuracy: 0.3250 - lr: 5.0000e-04\n",
      "Epoch 5/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.2707 - categorical_accuracy: 0.5357\n",
      "Epoch 00005: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 44s 3s/step - loss: 1.2707 - categorical_accuracy: 0.5357 - val_loss: 1.5483 - val_categorical_accuracy: 0.2750 - lr: 5.0000e-04\n",
      "Epoch 6/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1933 - categorical_accuracy: 0.6179\n",
      "Epoch 00006: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.1933 - categorical_accuracy: 0.6179 - val_loss: 1.5776 - val_categorical_accuracy: 0.1750 - lr: 5.0000e-04\n",
      "Epoch 7/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1763 - categorical_accuracy: 0.5821\n",
      "Epoch 00007: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.1763 - categorical_accuracy: 0.5821 - val_loss: 1.4715 - val_categorical_accuracy: 0.2750 - lr: 2.5000e-04\n",
      "Epoch 8/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.1067 - categorical_accuracy: 0.6714\n",
      "Epoch 00008: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.1067 - categorical_accuracy: 0.6714 - val_loss: 1.4272 - val_categorical_accuracy: 0.2750 - lr: 2.5000e-04\n",
      "Epoch 9/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0806 - categorical_accuracy: 0.6750\n",
      "Epoch 00009: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 38s 3s/step - loss: 1.0806 - categorical_accuracy: 0.6750 - val_loss: 1.4087 - val_categorical_accuracy: 0.3500 - lr: 2.5000e-04\n",
      "Epoch 10/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0684 - categorical_accuracy: 0.6393\n",
      "Epoch 00010: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 1.0684 - categorical_accuracy: 0.6393 - val_loss: 1.3145 - val_categorical_accuracy: 0.4000 - lr: 2.5000e-04\n",
      "Epoch 11/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0556 - categorical_accuracy: 0.6893\n",
      "Epoch 00011: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.0556 - categorical_accuracy: 0.6893 - val_loss: 1.3259 - val_categorical_accuracy: 0.3500 - lr: 2.5000e-04\n",
      "Epoch 12/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0706 - categorical_accuracy: 0.6250\n",
      "Epoch 00012: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 [==============================] - 39s 3s/step - loss: 1.0706 - categorical_accuracy: 0.6250 - val_loss: 1.3196 - val_categorical_accuracy: 0.4500 - lr: 2.5000e-04\n",
      "Epoch 13/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0371 - categorical_accuracy: 0.6786\n",
      "Epoch 00013: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 42s 3s/step - loss: 1.0371 - categorical_accuracy: 0.6786 - val_loss: 1.1600 - val_categorical_accuracy: 0.6000 - lr: 1.2500e-04\n",
      "Epoch 14/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9967 - categorical_accuracy: 0.7107\n",
      "Epoch 00014: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9967 - categorical_accuracy: 0.7107 - val_loss: 1.2726 - val_categorical_accuracy: 0.5000 - lr: 1.2500e-04\n",
      "Epoch 15/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9679 - categorical_accuracy: 0.7607\n",
      "Epoch 00015: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9679 - categorical_accuracy: 0.7607 - val_loss: 1.2643 - val_categorical_accuracy: 0.4250 - lr: 1.2500e-04\n",
      "Epoch 16/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9975 - categorical_accuracy: 0.7143\n",
      "Epoch 00016: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.9975 - categorical_accuracy: 0.7143 - val_loss: 1.1744 - val_categorical_accuracy: 0.5500 - lr: 6.2500e-05\n",
      "Epoch 17/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0099 - categorical_accuracy: 0.6893\n",
      "Epoch 00017: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.0099 - categorical_accuracy: 0.6893 - val_loss: 1.2249 - val_categorical_accuracy: 0.4750 - lr: 6.2500e-05\n",
      "Epoch 18/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9190 - categorical_accuracy: 0.7821\n",
      "Epoch 00018: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9190 - categorical_accuracy: 0.7821 - val_loss: 1.1365 - val_categorical_accuracy: 0.6250 - lr: 3.1250e-05\n",
      "Epoch 19/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.0175 - categorical_accuracy: 0.7036\n",
      "Epoch 00019: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 1.0175 - categorical_accuracy: 0.7036 - val_loss: 1.1354 - val_categorical_accuracy: 0.6500 - lr: 3.1250e-05\n",
      "Epoch 20/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9914 - categorical_accuracy: 0.7429\n",
      "Epoch 00020: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.9914 - categorical_accuracy: 0.7429 - val_loss: 1.1656 - val_categorical_accuracy: 0.5750 - lr: 3.1250e-05\n",
      "Epoch 21/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9623 - categorical_accuracy: 0.7536\n",
      "Epoch 00021: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9623 - categorical_accuracy: 0.7536 - val_loss: 1.1267 - val_categorical_accuracy: 0.6000 - lr: 3.1250e-05\n",
      "Epoch 22/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9979 - categorical_accuracy: 0.7429\n",
      "Epoch 00022: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9979 - categorical_accuracy: 0.7429 - val_loss: 1.1164 - val_categorical_accuracy: 0.6750 - lr: 3.1250e-05\n",
      "Epoch 23/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9180 - categorical_accuracy: 0.7643\n",
      "Epoch 00023: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.9180 - categorical_accuracy: 0.7643 - val_loss: 1.1511 - val_categorical_accuracy: 0.6000 - lr: 3.1250e-05\n",
      "Epoch 24/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9579 - categorical_accuracy: 0.7536\n",
      "Epoch 00024: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 43s 3s/step - loss: 0.9579 - categorical_accuracy: 0.7536 - val_loss: 1.0246 - val_categorical_accuracy: 0.6750 - lr: 3.1250e-05\n",
      "Epoch 25/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9467 - categorical_accuracy: 0.7714\n",
      "Epoch 00025: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 39s 3s/step - loss: 0.9467 - categorical_accuracy: 0.7714 - val_loss: 1.0700 - val_categorical_accuracy: 0.5500 - lr: 3.1250e-05\n",
      "Epoch 26/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9683 - categorical_accuracy: 0.7321\n",
      "Epoch 00026: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.9683 - categorical_accuracy: 0.7321 - val_loss: 1.1287 - val_categorical_accuracy: 0.6000 - lr: 3.1250e-05\n",
      "Epoch 27/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9577 - categorical_accuracy: 0.7571\n",
      "Epoch 00027: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.9577 - categorical_accuracy: 0.7571 - val_loss: 1.0627 - val_categorical_accuracy: 0.6750 - lr: 1.5625e-05\n",
      "Epoch 28/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9651 - categorical_accuracy: 0.7536\n",
      "Epoch 00028: val_loss did not improve from 0.28723\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9651 - categorical_accuracy: 0.7536 - val_loss: 1.0683 - val_categorical_accuracy: 0.7000 - lr: 1.5625e-05\n",
      "Epoch 29/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9646 - categorical_accuracy: 0.7143\n",
      "Epoch 00029: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9646 - categorical_accuracy: 0.7143 - val_loss: 1.0867 - val_categorical_accuracy: 0.6500 - lr: 1.0000e-05\n",
      "Epoch 30/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9575 - categorical_accuracy: 0.7214\n",
      "Epoch 00030: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.9575 - categorical_accuracy: 0.7214 - val_loss: 1.1154 - val_categorical_accuracy: 0.6500 - lr: 1.0000e-05\n",
      "Epoch 31/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9211 - categorical_accuracy: 0.7607\n",
      "Epoch 00031: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 42s 3s/step - loss: 0.9211 - categorical_accuracy: 0.7607 - val_loss: 1.1205 - val_categorical_accuracy: 0.6250 - lr: 1.0000e-05\n",
      "Epoch 32/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9966 - categorical_accuracy: 0.7071\n",
      "Epoch 00032: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.9966 - categorical_accuracy: 0.7071 - val_loss: 1.1320 - val_categorical_accuracy: 0.5250 - lr: 1.0000e-05\n",
      "Epoch 33/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9250 - categorical_accuracy: 0.8036\n",
      "Epoch 00033: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 41s 3s/step - loss: 0.9250 - categorical_accuracy: 0.8036 - val_loss: 1.0702 - val_categorical_accuracy: 0.6250 - lr: 1.0000e-05\n",
      "Epoch 34/34\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.9558 - categorical_accuracy: 0.7679\n",
      "Epoch 00034: val_loss did not improve from 0.28723\n",
      "14/14 [==============================] - 40s 3s/step - loss: 0.9558 - categorical_accuracy: 0.7679 - val_loss: 1.1165 - val_categorical_accuracy: 0.6000 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf91ecbca0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a345e5",
   "metadata": {},
   "source": [
    "# MOdel 6 is clearly overfitting ,Lets change the no of frames, image size and check "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fbfd8",
   "metadata": {},
   "source": [
    "# MOdel 7  - no of frames 10 , epochs = 20 , img_height, img_width = (50,50) , no of batches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c75fae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 10\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81d3116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switching Model architecture to Conv2D+LSTM\n",
    "\n",
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(512),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_2d_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dd0b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c89fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_2d_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_16 (TimeDi  (None, 10, 50, 50, 16)   448       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 10, 50, 50, 16)   64        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 10, 25, 25, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 10, 25, 25, 32)   4640      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 10, 25, 25, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 10, 12, 12, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 10, 12, 12, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 10, 12, 12, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 10, 6, 6, 64)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 10, 6, 6, 128)    73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_26 (TimeDi  (None, 10, 6, 6, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 10, 3, 3, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_28 (TimeDi  (None, 10, 3, 3, 256)    295168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, 10, 3, 3, 256)    1024      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, 10, 1, 1, 256)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 10, 256)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 512)               1574912   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,234,725\n",
      "Trainable params: 2,233,733\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82063ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1e7c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e4de028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d2d40f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6153 - categorical_accuracy: 0.2574Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61459, saving model to model_init_2022-05-1511_43_15.303156/model-00001-1.61533-0.25735-1.61459-0.22000.h5\n",
      "34/34 [==============================] - 64s 2s/step - loss: 1.6153 - categorical_accuracy: 0.2574 - val_loss: 1.6146 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3982 - categorical_accuracy: 0.4529\n",
      "Epoch 00002: val_loss improved from 1.61459 to 1.57992, saving model to model_init_2022-05-1511_43_15.303156/model-00002-1.39819-0.45294-1.57992-0.23000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.3982 - categorical_accuracy: 0.4529 - val_loss: 1.5799 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2643 - categorical_accuracy: 0.5162\n",
      "Epoch 00003: val_loss improved from 1.57992 to 1.51611, saving model to model_init_2022-05-1511_43_15.303156/model-00003-1.26426-0.51618-1.51611-0.26000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.2643 - categorical_accuracy: 0.5162 - val_loss: 1.5161 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1124 - categorical_accuracy: 0.6515\n",
      "Epoch 00004: val_loss improved from 1.51611 to 1.41192, saving model to model_init_2022-05-1511_43_15.303156/model-00004-1.11236-0.65147-1.41192-0.34000.h5\n",
      "34/34 [==============================] - 63s 2s/step - loss: 1.1124 - categorical_accuracy: 0.6515 - val_loss: 1.4119 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9996 - categorical_accuracy: 0.6897\n",
      "Epoch 00005: val_loss improved from 1.41192 to 1.40195, saving model to model_init_2022-05-1511_43_15.303156/model-00005-0.99962-0.68971-1.40195-0.40000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.9996 - categorical_accuracy: 0.6897 - val_loss: 1.4020 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8840 - categorical_accuracy: 0.7471\n",
      "Epoch 00006: val_loss improved from 1.40195 to 1.19824, saving model to model_init_2022-05-1511_43_15.303156/model-00006-0.88398-0.74706-1.19824-0.50000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.8840 - categorical_accuracy: 0.7471 - val_loss: 1.1982 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7604 - categorical_accuracy: 0.8044\n",
      "Epoch 00007: val_loss improved from 1.19824 to 1.10692, saving model to model_init_2022-05-1511_43_15.303156/model-00007-0.76040-0.80441-1.10692-0.57000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.7604 - categorical_accuracy: 0.8044 - val_loss: 1.1069 - val_categorical_accuracy: 0.5700 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6637 - categorical_accuracy: 0.8485\n",
      "Epoch 00008: val_loss improved from 1.10692 to 0.95960, saving model to model_init_2022-05-1511_43_15.303156/model-00008-0.66369-0.84853-0.95960-0.61000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.6637 - categorical_accuracy: 0.8485 - val_loss: 0.9596 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5780 - categorical_accuracy: 0.8750\n",
      "Epoch 00009: val_loss improved from 0.95960 to 0.93636, saving model to model_init_2022-05-1511_43_15.303156/model-00009-0.57795-0.87500-0.93636-0.65000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.5780 - categorical_accuracy: 0.8750 - val_loss: 0.9364 - val_categorical_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4841 - categorical_accuracy: 0.9044\n",
      "Epoch 00010: val_loss did not improve from 0.93636\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4841 - categorical_accuracy: 0.9044 - val_loss: 0.9418 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4181 - categorical_accuracy: 0.9309\n",
      "Epoch 00011: val_loss improved from 0.93636 to 0.82899, saving model to model_init_2022-05-1511_43_15.303156/model-00011-0.41815-0.93088-0.82899-0.66000.h5\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.4181 - categorical_accuracy: 0.9309 - val_loss: 0.8290 - val_categorical_accuracy: 0.6600 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3308 - categorical_accuracy: 0.9500\n",
      "Epoch 00012: val_loss improved from 0.82899 to 0.78596, saving model to model_init_2022-05-1511_43_15.303156/model-00012-0.33080-0.95000-0.78596-0.70000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3308 - categorical_accuracy: 0.9500 - val_loss: 0.7860 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2951 - categorical_accuracy: 0.9647\n",
      "Epoch 00013: val_loss improved from 0.78596 to 0.75239, saving model to model_init_2022-05-1511_43_15.303156/model-00013-0.29509-0.96471-0.75239-0.71000.h5\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.2951 - categorical_accuracy: 0.9647 - val_loss: 0.7524 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2279 - categorical_accuracy: 0.9853\n",
      "Epoch 00014: val_loss improved from 0.75239 to 0.74317, saving model to model_init_2022-05-1511_43_15.303156/model-00014-0.22795-0.98529-0.74317-0.66000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2279 - categorical_accuracy: 0.9853 - val_loss: 0.7432 - val_categorical_accuracy: 0.6600 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1923 - categorical_accuracy: 0.9897\n",
      "Epoch 00015: val_loss improved from 0.74317 to 0.66532, saving model to model_init_2022-05-1511_43_15.303156/model-00015-0.19228-0.98971-0.66532-0.73000.h5\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1923 - categorical_accuracy: 0.9897 - val_loss: 0.6653 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1480 - categorical_accuracy: 0.9926\n",
      "Epoch 00016: val_loss did not improve from 0.66532\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1480 - categorical_accuracy: 0.9926 - val_loss: 0.7107 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1637 - categorical_accuracy: 0.9838\n",
      "Epoch 00017: val_loss did not improve from 0.66532\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.1637 - categorical_accuracy: 0.9838 - val_loss: 0.7179 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1217 - categorical_accuracy: 0.9971\n",
      "Epoch 00018: val_loss did not improve from 0.66532\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1217 - categorical_accuracy: 0.9971 - val_loss: 0.7755 - val_categorical_accuracy: 0.6800 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1117 - categorical_accuracy: 0.9956\n",
      "Epoch 00019: val_loss did not improve from 0.66532\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1117 - categorical_accuracy: 0.9956 - val_loss: 0.6814 - val_categorical_accuracy: 0.6800 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1017 - categorical_accuracy: 0.9985\n",
      "Epoch 00020: val_loss did not improve from 0.66532\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1017 - categorical_accuracy: 0.9985 - val_loss: 0.6709 - val_categorical_accuracy: 0.7100 - lr: 2.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf2c081550>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd0c3f",
   "metadata": {},
   "source": [
    "# Model 7 is also clearly overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fc843",
   "metadata": {},
   "source": [
    "# Model 8 CONV2D + GRU\n",
    "\n",
    "Changed the no of layers , no of frames are 18 , image_height and image_witdth = (50,50) , batch_size 20 , no of epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2afbaf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],50,50,20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cef78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "model = Sequential()    \n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
    "                                  input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a09e6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d1b745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 18, 50, 50, 16)   448       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 18, 50, 50, 16)   64        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 18, 25, 25, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 18, 25, 25, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 18, 25, 25, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 18, 12, 12, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 18, 12, 12, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 18, 12, 12, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 18, 6, 6, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 18, 6, 6, 128)    73856     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 18, 6, 6, 128)    512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 18, 3, 3, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 18, 1152)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                233856    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,741\n",
      "Trainable params: 336,261\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b84d0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca72a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf2d5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22db1f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 16:22:02.895023: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - ETA: 0s - loss: 1.7063 - categorical_accuracy: 0.2603Source path =  ../datasets/Project_data/val ; batch size = 20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73767, saving model to model_init_2022-05-1516_19_13.658833/model-00001-1.70625-0.26029-1.73767-0.25000.h5\n",
      "34/34 [==============================] - 114s 3s/step - loss: 1.7063 - categorical_accuracy: 0.2603 - val_loss: 1.7377 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4768 - categorical_accuracy: 0.3647\n",
      "Epoch 00002: val_loss improved from 1.73767 to 1.66238, saving model to model_init_2022-05-1516_19_13.658833/model-00002-1.47680-0.36471-1.66238-0.29000.h5\n",
      "34/34 [==============================] - 111s 3s/step - loss: 1.4768 - categorical_accuracy: 0.3647 - val_loss: 1.6624 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2475 - categorical_accuracy: 0.4824\n",
      "Epoch 00003: val_loss did not improve from 1.66238\n",
      "34/34 [==============================] - 111s 3s/step - loss: 1.2475 - categorical_accuracy: 0.4824 - val_loss: 1.6680 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0654 - categorical_accuracy: 0.5838\n",
      "Epoch 00004: val_loss improved from 1.66238 to 1.50430, saving model to model_init_2022-05-1516_19_13.658833/model-00004-1.06539-0.58382-1.50430-0.35000.h5\n",
      "34/34 [==============================] - 108s 3s/step - loss: 1.0654 - categorical_accuracy: 0.5838 - val_loss: 1.5043 - val_categorical_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9348 - categorical_accuracy: 0.6676\n",
      "Epoch 00005: val_loss improved from 1.50430 to 1.21254, saving model to model_init_2022-05-1516_19_13.658833/model-00005-0.93478-0.66765-1.21254-0.52000.h5\n",
      "34/34 [==============================] - 110s 3s/step - loss: 0.9348 - categorical_accuracy: 0.6676 - val_loss: 1.2125 - val_categorical_accuracy: 0.5200 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8828 - categorical_accuracy: 0.6794\n",
      "Epoch 00006: val_loss improved from 1.21254 to 1.07352, saving model to model_init_2022-05-1516_19_13.658833/model-00006-0.88280-0.67941-1.07352-0.59000.h5\n",
      "34/34 [==============================] - 113s 3s/step - loss: 0.8828 - categorical_accuracy: 0.6794 - val_loss: 1.0735 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7490 - categorical_accuracy: 0.7574\n",
      "Epoch 00007: val_loss improved from 1.07352 to 1.03617, saving model to model_init_2022-05-1516_19_13.658833/model-00007-0.74905-0.75735-1.03617-0.54000.h5\n",
      "34/34 [==============================] - 111s 3s/step - loss: 0.7490 - categorical_accuracy: 0.7574 - val_loss: 1.0362 - val_categorical_accuracy: 0.5400 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6738 - categorical_accuracy: 0.7853\n",
      "Epoch 00008: val_loss improved from 1.03617 to 0.92043, saving model to model_init_2022-05-1516_19_13.658833/model-00008-0.67384-0.78529-0.92043-0.62000.h5\n",
      "34/34 [==============================] - 110s 3s/step - loss: 0.6738 - categorical_accuracy: 0.7853 - val_loss: 0.9204 - val_categorical_accuracy: 0.6200 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6395 - categorical_accuracy: 0.8118\n",
      "Epoch 00009: val_loss improved from 0.92043 to 0.83255, saving model to model_init_2022-05-1516_19_13.658833/model-00009-0.63948-0.81176-0.83255-0.71000.h5\n",
      "34/34 [==============================] - 111s 3s/step - loss: 0.6395 - categorical_accuracy: 0.8118 - val_loss: 0.8325 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5738 - categorical_accuracy: 0.8191\n",
      "Epoch 00010: val_loss did not improve from 0.83255\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.5738 - categorical_accuracy: 0.8191 - val_loss: 0.8980 - val_categorical_accuracy: 0.6600 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5205 - categorical_accuracy: 0.8574\n",
      "Epoch 00011: val_loss did not improve from 0.83255\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "34/34 [==============================] - 109s 3s/step - loss: 0.5205 - categorical_accuracy: 0.8574 - val_loss: 0.8364 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4674 - categorical_accuracy: 0.8824\n",
      "Epoch 00012: val_loss improved from 0.83255 to 0.74736, saving model to model_init_2022-05-1516_19_13.658833/model-00012-0.46738-0.88235-0.74736-0.75000.h5\n",
      "34/34 [==============================] - 112s 3s/step - loss: 0.4674 - categorical_accuracy: 0.8824 - val_loss: 0.7474 - val_categorical_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4159 - categorical_accuracy: 0.9059\n",
      "Epoch 00013: val_loss did not improve from 0.74736\n",
      "34/34 [==============================] - 112s 3s/step - loss: 0.4159 - categorical_accuracy: 0.9059 - val_loss: 0.7938 - val_categorical_accuracy: 0.7300 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3936 - categorical_accuracy: 0.9176\n",
      "Epoch 00014: val_loss did not improve from 0.74736\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "34/34 [==============================] - 106s 3s/step - loss: 0.3936 - categorical_accuracy: 0.9176 - val_loss: 0.7523 - val_categorical_accuracy: 0.7100 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3766 - categorical_accuracy: 0.9176\n",
      "Epoch 00015: val_loss did not improve from 0.74736\n",
      "34/34 [==============================] - 110s 3s/step - loss: 0.3766 - categorical_accuracy: 0.9176 - val_loss: 0.7671 - val_categorical_accuracy: 0.7400 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3595 - categorical_accuracy: 0.9235\n",
      "Epoch 00016: val_loss did not improve from 0.74736\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "34/34 [==============================] - 113s 3s/step - loss: 0.3595 - categorical_accuracy: 0.9235 - val_loss: 0.7567 - val_categorical_accuracy: 0.7400 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3739 - categorical_accuracy: 0.9191\n",
      "Epoch 00017: val_loss improved from 0.74736 to 0.73368, saving model to model_init_2022-05-1516_19_13.658833/model-00017-0.37386-0.91912-0.73368-0.75000.h5\n",
      "34/34 [==============================] - 109s 3s/step - loss: 0.3739 - categorical_accuracy: 0.9191 - val_loss: 0.7337 - val_categorical_accuracy: 0.7500 - lr: 1.2500e-04\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3532 - categorical_accuracy: 0.9250\n",
      "Epoch 00018: val_loss improved from 0.73368 to 0.68212, saving model to model_init_2022-05-1516_19_13.658833/model-00018-0.35321-0.92500-0.68212-0.79000.h5\n",
      "34/34 [==============================] - 108s 3s/step - loss: 0.3532 - categorical_accuracy: 0.9250 - val_loss: 0.6821 - val_categorical_accuracy: 0.7900 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3353 - categorical_accuracy: 0.9353\n",
      "Epoch 00019: val_loss did not improve from 0.68212\n",
      "34/34 [==============================] - 110s 3s/step - loss: 0.3353 - categorical_accuracy: 0.9353 - val_loss: 0.8201 - val_categorical_accuracy: 0.6900 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3238 - categorical_accuracy: 0.9412\n",
      "Epoch 00020: val_loss did not improve from 0.68212\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "34/34 [==============================] - 113s 3s/step - loss: 0.3238 - categorical_accuracy: 0.9412 - val_loss: 0.7565 - val_categorical_accuracy: 0.7500 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f05702d4670>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d17de",
   "metadata": {},
   "source": [
    "# Model 9 Using Transfer Learning - MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c431d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of images we will be feeding in the input for a video 18\n"
     ]
    }
   ],
   "source": [
    "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],120,120,5,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34a60a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
    "from keras.applications import mobilenet\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(TimeDistributed(mobilenet_transfer,input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "665aa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b1fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_10 (TimeDi  (None, 18, 3, 3, 1024)   3228864   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 18, 3, 3, 1024)   4096      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 18, 1, 1, 1024)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 18, 1024)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               443136    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7ecd1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4bb55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07ac767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07654204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../datasets/Project_data/train ; batch size = 5\n",
      "Epoch 1/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0803 - categorical_accuracy: 0.6015Source path =  ../datasets/Project_data/val ; batch size = 5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66000, saving model to model_init_2022-05-1603_29_14.160876/model-00001-1.08034-0.60150-0.66000-0.74000.h5\n",
      "133/133 [==============================] - 100s 723ms/step - loss: 1.0803 - categorical_accuracy: 0.6015 - val_loss: 0.6600 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5030 - categorical_accuracy: 0.8346\n",
      "Epoch 00002: val_loss improved from 0.66000 to 0.37334, saving model to model_init_2022-05-1603_29_14.160876/model-00002-0.50305-0.83459-0.37334-0.87000.h5\n",
      "133/133 [==============================] - 95s 721ms/step - loss: 0.5030 - categorical_accuracy: 0.8346 - val_loss: 0.3733 - val_categorical_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5150 - categorical_accuracy: 0.8286\n",
      "Epoch 00003: val_loss did not improve from 0.37334\n",
      "133/133 [==============================] - 96s 730ms/step - loss: 0.5150 - categorical_accuracy: 0.8286 - val_loss: 0.5354 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.4677 - categorical_accuracy: 0.8556\n",
      "Epoch 00004: val_loss did not improve from 0.37334\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "133/133 [==============================] - 96s 726ms/step - loss: 0.4677 - categorical_accuracy: 0.8556 - val_loss: 0.5679 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2676 - categorical_accuracy: 0.9308\n",
      "Epoch 00005: val_loss did not improve from 0.37334\n",
      "133/133 [==============================] - 95s 718ms/step - loss: 0.2676 - categorical_accuracy: 0.9308 - val_loss: 0.6342 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1318 - categorical_accuracy: 0.9639\n",
      "Epoch 00006: val_loss improved from 0.37334 to 0.25650, saving model to model_init_2022-05-1603_29_14.160876/model-00006-0.13178-0.96391-0.25650-0.93000.h5\n",
      "133/133 [==============================] - 95s 719ms/step - loss: 0.1318 - categorical_accuracy: 0.9639 - val_loss: 0.2565 - val_categorical_accuracy: 0.9300 - lr: 5.0000e-04\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.1252 - categorical_accuracy: 0.9564\n",
      "Epoch 00007: val_loss did not improve from 0.25650\n",
      "133/133 [==============================] - 95s 717ms/step - loss: 0.1252 - categorical_accuracy: 0.9564 - val_loss: 0.4801 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0866 - categorical_accuracy: 0.9714\n",
      "Epoch 00008: val_loss did not improve from 0.25650\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "133/133 [==============================] - 99s 748ms/step - loss: 0.0866 - categorical_accuracy: 0.9714 - val_loss: 0.3082 - val_categorical_accuracy: 0.8900 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0752 - categorical_accuracy: 0.9789\n",
      "Epoch 00009: val_loss did not improve from 0.25650\n",
      "133/133 [==============================] - 97s 734ms/step - loss: 0.0752 - categorical_accuracy: 0.9789 - val_loss: 0.4665 - val_categorical_accuracy: 0.7900 - lr: 2.5000e-04\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0312 - categorical_accuracy: 0.9925\n",
      "Epoch 00010: val_loss improved from 0.25650 to 0.13412, saving model to model_init_2022-05-1603_29_14.160876/model-00010-0.03121-0.99248-0.13412-0.98000.h5\n",
      "133/133 [==============================] - 96s 724ms/step - loss: 0.0312 - categorical_accuracy: 0.9925 - val_loss: 0.1341 - val_categorical_accuracy: 0.9800 - lr: 2.5000e-04\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0343 - categorical_accuracy: 0.9880\n",
      "Epoch 00011: val_loss did not improve from 0.13412\n",
      "133/133 [==============================] - 97s 730ms/step - loss: 0.0343 - categorical_accuracy: 0.9880 - val_loss: 0.1360 - val_categorical_accuracy: 0.9800 - lr: 2.5000e-04\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0254 - categorical_accuracy: 0.9925\n",
      "Epoch 00012: val_loss did not improve from 0.13412\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "133/133 [==============================] - 96s 728ms/step - loss: 0.0254 - categorical_accuracy: 0.9925 - val_loss: 0.3117 - val_categorical_accuracy: 0.9300 - lr: 2.5000e-04\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0100 - categorical_accuracy: 0.9985\n",
      "Epoch 00013: val_loss did not improve from 0.13412\n",
      "133/133 [==============================] - 94s 712ms/step - loss: 0.0100 - categorical_accuracy: 0.9985 - val_loss: 0.2974 - val_categorical_accuracy: 0.9300 - lr: 1.2500e-04\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0071 - categorical_accuracy: 0.9985\n",
      "Epoch 00014: val_loss improved from 0.13412 to 0.13376, saving model to model_init_2022-05-1603_29_14.160876/model-00014-0.00712-0.99850-0.13376-0.97000.h5\n",
      "133/133 [==============================] - 96s 730ms/step - loss: 0.0071 - categorical_accuracy: 0.9985 - val_loss: 0.1338 - val_categorical_accuracy: 0.9700 - lr: 1.2500e-04\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.0188 - categorical_accuracy: 0.9970\n",
      "Epoch 00015: val_loss did not improve from 0.13376\n",
      "133/133 [==============================] - 97s 736ms/step - loss: 0.0188 - categorical_accuracy: 0.9970 - val_loss: 0.2233 - val_categorical_accuracy: 0.9400 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b2343b3a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f00ca",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "- # Model Statistics\n",
    "\n",
    "- # Conv3D\n",
    "\n",
    "- Model 1 : No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
    "- - - - Training Accuracy : , Validation Accuracy : , \n",
    "- - - - Model Analysis : Model 1 is giving the out of memory error hence we are reducing the batch size instead of 64 we will experiment with less batch size\n",
    "\n",
    "- Model 2 : No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
    "\n",
    "- - - - Training Accuracy : .9574 , Validation Accuracy : .93 , \n",
    "- - - - Model Analysis : Training and validation Accuracy are good so that we can conclude that with above set of parameters model is giving good results\n",
    "\n",
    "- Model 3 : No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10\n",
    "\n",
    "- - - - Training Accuracy : 0.9279 , Validation Accuracy : .82 \n",
    "- - - - Model Analysis : This Model is clearly Overfitting\n",
    "\n",
    "- Model 4 : No of Epochs = 25 , batch_size = 50 ,shape = (120,120) , no of frames = 10\n",
    "\n",
    "- - - - Training Accuracy : .95 , Validation Accuracy : .87 \n",
    "- - - - Model Analysis : Model 4 seems to be overfitting seems increase the image size decreasing the accuracy\n",
    "\n",
    "- Model 5 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "\n",
    "- - - - Training Accuracy : .95 , Validation Accuracy : .85 \n",
    "- - - - Model Analysis : Model 5 is clearly an overfit model can see that increasing no of frames and epochs causing the noise to be learned also from all the frames\n",
    "\n",
    "- # CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
    "\n",
    "- Model 6 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18 \n",
    "\n",
    "- - - - Training Accuracy : .76 , Validation Accuracy : .60 \n",
    "- - - - Model Analysis : Model 6 is Overfitting\n",
    "\n",
    "- Model 7 : No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 10 \n",
    "\n",
    "- - - - Training Accuracy : .99 , Validation Accuracy : .67 \n",
    "- - - - Model Analysis : Model 7 is overfitting\n",
    "\n",
    "- # CONV2D + GRU\n",
    "\n",
    "- Model 8 : No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 18\n",
    "\n",
    "- - - - Training Accuracy : .94, Validation Accuracy : .75 \n",
    "- - - - Model Analysis : Model 8 is. overfitting\n",
    "\n",
    "- # Transfer Learning Using MobileNet\n",
    "\n",
    "-  Model 9 : No of epochs = 15 , batch_size = 5 , shape  (120,120) , no of frames  = 18\n",
    "\n",
    "- - - - Training Accuracy : .99 , Validation Accuracy : .98 \n",
    "- - - - Model Analysis : This is so far the best model that we got in terms of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6c95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc752c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
