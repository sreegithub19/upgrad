{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using RNN - Word Level\n",
    "\n",
    "To generate text using RNN, we need a to convert raw text to a supervised learning problem format.\n",
    "\n",
    "Take, for example, the following corpus:\n",
    "\n",
    "\"Her brother shook his head incredulously. He was not aware of the situation at all.\"\n",
    "\n",
    "First we need to divide the data into tabular format containing input (X) and output (y) sequences. In case of a character level model, the X and y will look like this:\n",
    "\n",
    "|      X                |  Y      |\n",
    "|-----------------------|---------|\n",
    "|    < word1 >< word2 > | < word3 > |\n",
    "|    Her brother        |  shook  |\n",
    "|    brother shook      |  his    |\n",
    "|    shook his          |  head   |\n",
    "|    his head           | incredulously |\n",
    "|    head incredulously |    .    |\n",
    "|    ..                 |    .    |\n",
    "|    situation at       |  all    |\n",
    "|    at all             |    .    |\n",
    "\n",
    "Note that in the above problem, the sequence length of **X is two words** and that of **y is one word**. Hence, this is a many-to-one architecture. We can, however, change the number of input words to any number depending on the problem.\n",
    "\n",
    "A model is trained on such data. To generate text, we simply give the model any two words using which it predicts the next word. Then it appends the predicted word to the input sequence (to the extreme right of the sequence) and discards the first word (word on extreme left of the sequence). Then it predicts again using the new sequence and the cycle continues until a fix number of iterations. An example is shown below:\n",
    "\n",
    "Seed text: \"Did I\"\n",
    "\n",
    "|      X                                            |  Y                       |\n",
    "|---------------------------------------------------|--------------------------|\n",
    "|                        Did I                      |    < predicted word 1 >  |\n",
    "|               I < predicted word 1 >              |    < predicted word 2 >  |\n",
    "|       < predicted word 1 > < predicted word 2 >   |    < predicted word 3 >  |\n",
    "|       < predicted word 2 > < predicted word 3 >   |    < predicted word 4 >  |\n",
    "|                      ...                          |            ...           | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "1. Preprocess data\n",
    "2. Build LSTM model\n",
    "3. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "n5R1APadwANc",
    "outputId": "3553d1b6-cb0c-4adf-efac-333da6e7f547",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PXzkqw1ipoPg"
   },
   "outputs": [],
   "source": [
    "# download ebook\n",
    "url = \"https://www.gutenberg.org/files/24869/24869-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Ramayana\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\r\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\r\n",
      "the terms of the Project Gutenberg License included with this eBook or\r\n",
      "online at http://www.gutenberg.org/license\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Title: The Ramayana\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Release Date: March 18, 2008 [Ebook #24869]\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "Character set encoding: UTF-8\r\n",
      "\r\n",
      "\r\n",
      "***START OF THE PROJECT GUTENBERG EBOOK THE RA\n"
     ]
    }
   ],
   "source": [
    "# let's look at the text\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18370\n"
     ]
    }
   ],
   "source": [
    "# subset the book from the first chapter, that ism INVOCATION - everything before first chapter is irrelevant data\n",
    "start_index = re.search(\"invocation.\\(1\\)\", data, re.I)\n",
    "print(start_index.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how does the text look like\n",
    "data = data[start_index.start():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOCATION.(1)\r\n",
      "\r\n",
      "\r\n",
      "Praise to Válmíki,(2)bird of charming song,(3)\r\n",
      "  Who mounts on Poesy’s sublimest spray,\r\n",
      "And sweetly sings with accent clear and strong\r\n",
      "  Ráma, aye Ráma, in his deathless lay.\r\n",
      "\r\n",
      "Where breathes the man can listen to the strain\r\n",
      "  That flows in music from Válmíki’s tongue,\r\n",
      "Nor feel his feet the path of bliss attain\r\n",
      "  When Ráma’s glory by the saint is sung!\r\n",
      "\r\n",
      "The stream Rámáyan leaves its sacred fount\r\n",
      "  The whole wide world from sin and stain to free.(4)\r\n",
      "The Prince of He\n"
     ]
    }
   ],
   "source": [
    "# let's look at the text\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to clean text data\n",
    "def clean_document(document, char_filter = r\"[^\\w]\"):\n",
    "    '''\n",
    "    input:\n",
    "    document          :  string\n",
    "    char_filter       :  regex pattern - removes those characters from the text that match the pattern\n",
    "\n",
    "    output: clean document\n",
    "    '''\n",
    "    \n",
    "    # convert words to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenise words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # strip whitespace from all words\n",
    "    words = [word.strip() for word in words]\n",
    "\n",
    "    # join back words to get document\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    # remove unwanted characters\n",
    "    document = re.sub(char_filter, \" \", document)\n",
    "\n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    document = re.sub(r\"\\s+\", \" \", document)\n",
    "\n",
    "    # strip whitespace from document\n",
    "    document = document.strip()\n",
    "\n",
    "    return document\n",
    "\n",
    "data = clean_document(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in document: 410371\n"
     ]
    }
   ],
   "source": [
    "# length of text\n",
    "words = word_tokenize(data)\n",
    "print(\"Number of words in document: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PBkfQnPGqV_4",
    "outputId": "5d32cb08-61b0-4695-a4fe-4508b4b3e898"
   },
   "outputs": [],
   "source": [
    "# use Keras' Tokenizer() function to encode text to integers\n",
    "word_tokeniser = Tokenizer()\n",
    "word_tokeniser.fit_on_texts([data])\n",
    "encoded_words = word_tokeniser.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 17667\n"
     ]
    }
   ],
   "source": [
    "# check the size of the vocabulary\n",
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences\n",
    "\n",
    "In each training sample, X will have a sequence of 5 words and y will have the sixth word. In other words, this means that use previous five words of a sequence to predict next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hwJLfNn3qX56",
    "outputId": "bef2447b-77d1-43f3-e64b-a098ed2e87e1"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "MAX_SEQ_LENGTH = 5  # X will have five words, y will have the sixth word\n",
    "\n",
    "for i in range(MAX_SEQ_LENGTH, len(encoded_words)):\n",
    "    sequence = encoded_words[i-MAX_SEQ_LENGTH:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples: 410241\n",
      "\n",
      "Sample sequences: \n",
      "[[6821  813  857    4 1060 1928]\n",
      " [ 813  857    4 1060 1928  397]\n",
      " [ 857    4 1060 1928  397    3]]\n"
     ]
    }
   ],
   "source": [
    "print('Total number of training samples: {}'.format(len(sequences)))\n",
    "print('\\nSample sequences: \\n{}'.format(sequences[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N3baVAiOqZpj"
   },
   "outputs": [],
   "source": [
    "# divide the sequence into X and y\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:,:-1]  # assign all but last words of a sequence to X\n",
    "y = sequences[:,-1]   # assign last word of each sequence to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of the first data point: [6821  813  857    4 1060] \n",
      "\n",
      "Output of the first data point: [ 1928 ]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first training example\n",
    "print(\"Input of the first data point:\", X[0], \"\\n\")\n",
    "print(\"Output of the first data point: [\", y[0], \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410241,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HX_Npp_cVWyW",
    "outputId": "af76e422-420e-4b83-cd42-6b1dda6011ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410241, 5)\n",
      "(410241, 17667)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 410241 sequences (data points) in total.\n",
    "\n",
    "Remember that to use an RNN data has to be of the shape (#samples, #timesteps, #features)\n",
    "\n",
    "In X, the third dimension, that is, number of features is missing because we're going to use the Keras' Embedding Layer. Hence we don't need to explicitly reshape the data to incorporate the third dimension. That will be done automatically by Keras.\n",
    "\n",
    "In y, the second dimension is missing, that is, the number of timesteps because y is not a sequence, it's just a single word. The number of features are represented by a one-hot encoded vector whose length is the VOCABULARY_SIZE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence length: 5\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "print('Input sequence length: {}'.format(MAX_SEQ_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model architecture\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH))\n",
    "\n",
    "# lstm layer 1\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 100)            1766700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 128)            117248    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17667)             2279043   \n",
      "=================================================================\n",
      "Total params: 4,294,575\n",
      "Trainable params: 4,294,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5134
    },
    "colab_type": "code",
    "id": "cvY3R74rqbOu",
    "outputId": "68541bf7-bc2f-4fa6-c6c9-8787a7da8e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 84s - loss: 6.9920 - acc: 0.0679\n",
      "Epoch 2/10\n",
      " - 77s - loss: 6.3825 - acc: 0.0923\n",
      "Epoch 3/10\n",
      " - 78s - loss: 5.9852 - acc: 0.1105\n",
      "Epoch 4/10\n",
      " - 78s - loss: 5.6994 - acc: 0.1229\n",
      "Epoch 5/10\n",
      " - 77s - loss: 5.4749 - acc: 0.1329\n",
      "Epoch 6/10\n",
      " - 77s - loss: 5.2867 - acc: 0.1409\n",
      "Epoch 7/10\n",
      " - 77s - loss: 5.1200 - acc: 0.1491\n",
      "Epoch 8/10\n",
      " - 78s - loss: 4.9698 - acc: 0.1577\n",
      "Epoch 9/10\n",
      " - 77s - loss: 4.8298 - acc: 0.1663\n",
      "Epoch 10/10\n",
      " - 77s - loss: 4.6983 - acc: 0.1774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f71bfa9abe0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(X, y, epochs=10, verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings to represent the input words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec download link (Size ~ 1.5GB): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "path = 'storage/word-embeddings/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# load word2vec using the following function present in the gensim library\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign word vectors from word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokeniser.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (17667, 300)\n"
     ]
    }
   ],
   "source": [
    "# check embedding dimension\n",
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model architecture\n",
    "\n",
    "model_wv = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model_wv.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH,\n",
    "                    weights = [embedding_weights], trainable=True))\n",
    "\n",
    "# lstm layer 1\n",
    "model_wv.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "# when using multiple LSTM layers, set return_sequences to True at the previous layer\n",
    "# because the current layer expects a sequential intput rather than a single input\n",
    "model_wv.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model_wv.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 300)            5300100   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 128)            219648    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 17667)             2279043   \n",
      "=================================================================\n",
      "Total params: 7,930,375\n",
      "Trainable params: 7,930,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model_wv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model_wv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5134
    },
    "colab_type": "code",
    "id": "cvY3R74rqbOu",
    "outputId": "68541bf7-bc2f-4fa6-c6c9-8787a7da8e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "410241/410241 [==============================] - 435s 1ms/step - loss: 6.4059 - acc: 0.0981\n",
      "Epoch 2/10\n",
      "410241/410241 [==============================] - 437s 1ms/step - loss: 5.7361 - acc: 0.1302\n",
      "Epoch 3/10\n",
      "410241/410241 [==============================] - 437s 1ms/step - loss: 5.4051 - acc: 0.1482\n",
      "Epoch 4/10\n",
      "410241/410241 [==============================] - 437s 1ms/step - loss: 5.1437 - acc: 0.1628\n",
      "Epoch 5/10\n",
      "410241/410241 [==============================] - 437s 1ms/step - loss: 4.9080 - acc: 0.1776\n",
      "Epoch 6/10\n",
      "410241/410241 [==============================] - 437s 1ms/step - loss: 4.6920 - acc: 0.1936\n",
      "Epoch 7/10\n",
      "410241/410241 [==============================] - 438s 1ms/step - loss: 4.4895 - acc: 0.2107\n",
      "Epoch 8/10\n",
      "410241/410241 [==============================] - 438s 1ms/step - loss: 4.3055 - acc: 0.2290\n",
      "Epoch 9/10\n",
      "410241/410241 [==============================] - 437s 1ms/step - loss: 4.1407 - acc: 0.2472\n",
      "Epoch 10/10\n",
      "410241/410241 [==============================] - 438s 1ms/step - loss: 3.9920 - acc: 0.2657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69749bda90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model_wv.fit(X, y, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2NiH8h5oqkX5"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed, n_words):\n",
    "    \n",
    "    text = seed\n",
    "    \n",
    "    # generate n_words\n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        # encode text as integers\n",
    "        encoded_words = word_tokeniser.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # pad sequences\n",
    "        padded_words = pad_sequences([encoded_words], maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "        \n",
    "        # predict next word\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "        \n",
    "        # convert predicted index to its word\n",
    "        next_word = \"\"\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if i == prediction:\n",
    "                next_word = word\n",
    "                break\n",
    "        \n",
    "        # append predicted word to text\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some text generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rama never told anyone about the earth and the sun is a substitution to the work and the ganges and the ganges is the capital of the gods and the gandharvas and the gandharvas is the capital of the gods and the gandharvas and the gandharvas is the capital of the gods and the gandharvas and the gandharvas is the capital of the gods and the gandharvas and the gandharvas is the capital of the gods and the gandharvas and the gandharvas is the capital of the gods and the gandharvas and the gandharvas is the capital of the gods and the gandharvas and the\n"
     ]
    }
   ],
   "source": [
    "# text generation using first model - model without word embeddings\n",
    "seed_text = \"rama never told anyone about\"\n",
    "num_words = 100\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rama never told anyone about in careful watch to him he cried the king to whom the king has done and all the people s speech and thus to raghu s son he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he\n"
     ]
    }
   ],
   "source": [
    "# text generation using second model - model with word embeddings\n",
    "seed_text = \"rama never told anyone about\"\n",
    "num_words = 100\n",
    "print(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you doing and be not only thou to thee and i my lord and guide the king of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who rules the worlds of men and he who\n"
     ]
    }
   ],
   "source": [
    "# text generation using first model - model without word embeddings\n",
    "seed_text = \"how are you doing\"\n",
    "num_words = 100\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you doing and may be proved in the words of ráma s weal and woe and pain and woe no more to see the maithil dame he ceased and lakshmaṇ s care with joy inspired and thus the king began to hear the words of ráma remembered the words he said and thus to lakshmaṇ spake the king to whom the king has done and all the people s speech and thus to raghu s son he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke and thus to lakshmaṇ s speech he spoke\n"
     ]
    }
   ],
   "source": [
    "# text generation using second model - model with word embeddings\n",
    "seed_text = \"how are you doing\"\n",
    "num_words = 100\n",
    "print(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "word_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
