{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e477037-49da-4f8c-a127-177f6eb32bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is inference step to execute the hosted model with custom input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc78ea61-4cbc-4365-b86b-f10b0764cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), \n",
    "                         header=None)\n",
    "\n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example, includes the ring label\n",
    "            df.columns = feature_columns_names + [label_column]\n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd2c04c-86e4-455d-8e3b-9f01c59dab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fd17c61-727a-4021-b443-310258b73a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the text input for which infernece needs to be performed\n",
    "text = \"credit card was stolen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00d1c3a4-8307-49fb-b073-2460f3c272ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my loan application is not approved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d2bb7fd-9348-40bc-ba5d-58bfa6db9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre process the input text \n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27126c41-a04a-41c2-aa8c-132fef22c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to trim the text \n",
    "import re, nltk, spacy, string\n",
    "pd.options.mode.chained_assignment = None  \n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36b29223-1126-48c5-9d07-fca608f819f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Lemmatize the text\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "def lemmatizer(text):        \n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3cae92ac-0353-44f2-8bcd-41daa603f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "746d1dd9-4b75-4db5-a77b-3349721251fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = clean_text(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "585c13cd-e28f-439b-8e00-3119484fabc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Chunking in NLP is a process to take small pieces of information and group them into large units. The primary use of Chunking is making groups of \"noun phrases.\n",
    "#Here we are using only noun, singular as we have already lemmatized the texts.\n",
    "import pandas as pd\n",
    "!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "\n",
    "def pos_tag(text):\n",
    "    try:\n",
    "        return TextBlob(text).tags\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return ' '.join([ word for (word,tag) in blob.tags if tag == \"NN\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72fd75f1-ef83-46e6-8556-591e0c4d3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad83fc4b-0382-4d3d-841c-c5d6631eed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = get_adjectives(text_clean)\n",
    "text_clean=text_clean.replace('-PRON-', '')\n",
    "text_clean=text_clean.replace('xxxx', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "46773a42-ef6f-497e-bd8f-1c09d3acbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "data = io.StringIO(text_clean)\n",
    "df = pd.read_csv(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93514e63-69ab-4d5d-8149-e688fddbfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the count vector used during training \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "\n",
    "tfidf = transformer.fit_transform(loaded_vec.fit_transform(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "279c1125-3809-4eca-83e2-bc9d38f098f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=pd.DataFrame(tfidf.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "df203d30-468e-4ff9-94b6-ee7e03df6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=tfidf.to_csv(header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9d376f74-476e-4369-a543-ed4f7e150c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = tfidf.iloc[:1].to_csv(header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d418f83e-69dc-4f16-ac81-e4a2c05a1c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Provide the end point where the model is hosted\n",
    "\n",
    "endpoint_name=\"TicketClassificationNLP-staging\"\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "predictor = Predictor(endpoint_name=endpoint_name1)\n",
    "\n",
    "p = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "print(p.decode(\"utf-8\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1822acb7-5634-4ee6-a5b6-b0127c9a9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_names = {0:\"Bank Account services\", 1:\"Credit card or prepaid card\", 2:\"Others\", 3:\"Theft/Dispute Reporting\", 4:\"Mortgage/Loan\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "973fe994-673d-4aa5-b388-bca566b5080f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mortgage/Loan'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert class out put to mapped names\n",
    "Topic_names[int(float(p.decode(\"utf-8\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2c55e39-704f-4e12-a3b0-d8d23e5020fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name1 = 'NLPComplaintClassification-staging'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ad336-bd08-4762-a3a9-55236862ce77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
