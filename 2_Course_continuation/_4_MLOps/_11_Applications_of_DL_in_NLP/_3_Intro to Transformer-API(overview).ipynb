{"cells":[{"cell_type":"markdown","metadata":{"id":"knvKMneiKJ2H"},"source":["# Introduction to Huggingface Library\n","\n","### Introduction to Huggingface\n","Huggingface is an open-source library that provides state-of-the-art Natural Language Processing (NLP) tools and pre-trained models for a wide range of NLP tasks. The library is built on top of PyTorch and TensorFlow and provides a unified API for working with various models. Huggingface is widely used in the NLP community for research and development purposes.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"29nVVrGdKJ2Q"},"source":["### Installing the transformers\n","\n","To use Huggingface, you need to install the transformers library. You can do this by running the following command in your Colab notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"gnzmEmZBKJ2T"},"outputs":[],"source":["!pip install datasets transformers[sentencepiece] tensorflow\n","\n","!pip install transformers\n"]},{"cell_type":"markdown","source":["This will install the latest version of the library."],"metadata":{"id":"nTa6131Bm7aQ"}},{"cell_type":"markdown","source":["### Pipelines\n","Pipelines in Huggingface are a simple and intuitive way to use pre-trained models for various NLP tasks such as text classification, question answering, and text generation. Pipelines provide a high-level API for working with pre-trained models without the need for extensive coding.\n"],"metadata":{"id":"kthljjOZnAhm"}},{"cell_type":"markdown","metadata":{"id":"JQvj-qPdKJ2d"},"source":["### Using the pipelines\n","To use a pre-trained model for a specific NLP task, you can simply create a pipeline object and specify the task you want to perform. Here's an example of how to use the text classification pipeline:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["d327ddcf259148a08fb9edc59d2ceb66","1a87c10f6db3436ba9d9d4f13d1b810a","b29c759304ec4241b9207bf94a43098f","d01eeb56af424bed8d6017c79d692ef4","818b28edbcb247ff96ce01e8442d035f","f264a70b5fb946b0bfaab26bfc23d114","468b86823a6c4634ae32ec2cfcf9c67b","85bfcc93f3334955bbce2ce55221b200","999e0dc52e474302965b187498f6378f","61e5ad3b46844668a95c8453ade0731b","253452c3306a465793a888f190d102c0","d751cd72fe5548378c66f9db3ac0afcd","7007886831ed4abaa11476442c7815b3","e792bc0079b74e799094653fa5c857f1","9f74f6c367f9446a84933677e77321aa","9d933b92faf64569be89f58ba4b238cd"]},"id":"JjVVVPH5KJ2e","outputId":"a980bf1b-9b90-4121-f9e8-ff56ea8d7774"},"outputs":[{"name":"stderr","output_type":"stream","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"name":"stdout","output_type":"stream","text":["Moving 0 files to the new cache system\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d327ddcf259148a08fb9edc59d2ceb66","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d751cd72fe5548378c66f9db3ac0afcd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7007886831ed4abaa11476442c7815b3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e792bc0079b74e799094653fa5c857f1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f74f6c367f9446a84933677e77321aa","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d933b92faf64569be89f58ba4b238cd","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n"]},{"data":{"text/plain":["[{'generated_text': \"In the galaxy far far away, a young lady is riding on a rocket ship. She is the princess of Y'Kron's ship Y'Kron the Avatar, in charge of the princess by Y'Kron itself. She leads her\"}]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\")\n","generator(\"In the galaxy far far\")"]},{"cell_type":"markdown","metadata":{"id":"ZlSaSJbTKJ2f"},"source":["## Specifying a custom model in the pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["218b2b7de03644aa91c549a6114c7b9c","bda8e5de147444d785fab6f1b54ad048","d231c75b7e064c779a20ac9c60e4188a","d5a8fa009ea84a648fd905198ebb609f","7e88d1f825b2414287c1a77ca622a22d"]},"id":"0iOikAXWKJ2g","outputId":"185971c1-9c4b-4cd3-d706-94f280f90a5a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"218b2b7de03644aa91c549a6114c7b9c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bda8e5de147444d785fab6f1b54ad048","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n","\n","All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d231c75b7e064c779a20ac9c60e4188a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5a8fa009ea84a648fd905198ebb609f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e88d1f825b2414287c1a77ca622a22d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"]},{"data":{"text/plain":["[{'generated_text': 'In the galaxy far far away, and far all over the galaxy.\\n\\n\\n“\\n—\\nAll you\\n“\\n�'},\n"," {'generated_text': \"In the galaxy far far, far away, the next time you see that light, just like that of the sun, it's in darkness. But\"}]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n","generator(\n","    \"In the galaxy far far\",\n","    max_length=30,\n","    num_return_sequences=2,\n",")"]},{"cell_type":"markdown","metadata":{"id":"CwTR8keKKJ2i"},"source":["&nbsp;"]},{"cell_type":"markdown","metadata":{"id":"4lv8I0PbKJ2j"},"source":["## Available Pipelines"]},{"cell_type":"markdown","metadata":{"id":"lcl3MCrfKJ2k"},"source":["\n","- AudioClassificationPipeline\n","- AutomaticSpeechRecognitionPipeline\n","- ConversationalPipeline\n","- FeatureExtractionPipeline\n","- FillMaskPipeline\n","- ImageClassificationPipeline\n","- ImageSegmentationPipeline\n","- ObjectDetectionPipeline\n","- QuestionAnsweringPipeline\n","- SummarizationPipeline\n","- TableQuestionAnsweringPipeline\n","- TextClassificationPipeline\n","- TextGenerationPipeline\n","- Text2TextGenerationPipeline\n","- TokenClassificationPipeline\n","- TranslationPipeline\n","- VisualQuestionAnsweringPipeline\n","- ZeroShotClassificationPipeline\n","- ZeroShotImageClassificationPipeline"]},{"cell_type":"markdown","metadata":{"id":"pSNoGu8eKJ2l"},"source":["## Another example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKOWiQJCKJ2l","outputId":"48beda36-e271-4c1f-e1e3-9c42d89ed58a"},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n","All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n","\n","All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"]},{"data":{"text/plain":["[{'score': 0.5679107308387756,\n","  'token': 1798,\n","  'token_str': ' hear',\n","  'sequence': 'You are going to hear about a wonderful library today.'},\n"," {'score': 0.22818315029144287,\n","  'token': 1532,\n","  'token_str': ' learn',\n","  'sequence': 'You are going to learn about a wonderful library today.'}]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","unmasker = pipeline(\"fill-mask\")\n","unmasker(\"You are going to <mask> about a wonderful library today.\", top_k=2)"]},{"cell_type":"markdown","metadata":{"id":"ZU84mJylKJ2m"},"source":["# Looking inside the pipeline with Tensorflow API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLOKoNqPKJ2n","outputId":"f3d6cdd5-91e9-49d0-d080-b14199f87dca"},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n","Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_210']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["[{'label': 'NEGATIVE', 'score': 0.9839025139808655},\n"," {'label': 'POSITIVE', 'score': 0.9998325109481812}]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","input_sentences = [\n","        \"I don't like this movie\",\n","        \"Upgrad is helping me learn new and wonderful things.\",\n","        \n","    ]\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\n","    input_sentences\n",")"]},{"cell_type":"markdown","metadata":{"id":"-DXydOOTKJ2p"},"source":["## Tokenizing the input sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9fDjoOKKJ2p"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6clMi1PbKJ2q","outputId":"82be67d6-afea-4651-a83f-4af7ab662239"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>,\n"," 'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n","array([[  101,   146,  1274,   112,   189,  1176,  1142,  2523,   102,\n","            0,     0,     0],\n","       [  101,  3725, 20561,  1110,  4395,  1143,  3858,  1207,  1105,\n","         7310,  1614,   102]])>,\n"," 'token_type_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n","array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n"]}],"source":["\n","inputs = tokenizer(input_sentences, padding=True, truncation=True, max_length = 12, return_tensors=\"tf\",)\n","pp.pprint(inputs)"]},{"cell_type":"markdown","metadata":{"id":"wPmw5oRIKJ2r"},"source":["## Classifying the input sentences into positive and negative sentiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozFrCxFXKJ2s","outputId":"772ad3b6-621d-446f-bd12-47e10d291c8b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_230']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import TFAutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = TFAutoModelForSequenceClassification.from_pretrained(model)\n","outputs = model(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wViavRN-KJ2t","outputId":"5f97b42b-105a-499f-c856-bedcb51e8113"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorShape([2, 2])\n"]}],"source":["pp.pprint(outputs.logits.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a_hOy_uKJ2t","outputId":"e4cc7397-aa1a-49fc-ec97-7ad0c2109a8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n","array([[ 2.2426069, -1.8702551],\n","       [-4.191452 ,  4.5031376]], dtype=float32)>\n"]}],"source":["pp.pprint(outputs.logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8nNgueLKJ2v","outputId":"31c7c357-16d9-441b-d25a-2e21932944e9"},"outputs":[{"data":{"text/plain":["tf.Tensor(\n","[[4.01951671e-02 9.59804833e-01]\n"," [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import tensorflow as tf\n","\n","predictions = tf.math.softmax(outputs.logits, axis=-1)\n","pp.pprint(predictions)"]},{"cell_type":"markdown","metadata":{"id":"QftJaOYLKJ2w"},"source":["# Exploring the Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"it_IVwFNKJ2w","outputId":"0951bbca-7201-4852-ad42-4f99da70ce99"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Learning', 'NLP', 'is', 'so', 'much', 'rewarding']\n"]}],"source":["tokenized_text = \"Learning NLP is so much rewarding\".split()\n","pp.pprint(tokenized_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zq61pEE3KJ2x"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4y-G-CelKJ2y","outputId":"03bdd278-1694-4e40-d476-14ea5438c668"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Learning NLP is so much rewarding\")"]},{"cell_type":"markdown","metadata":{"id":"jEgqeg23KJ20"},"source":["## Breaking the tokenizer functions down"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9MG8agZKJ21","outputId":"2206c3e2-ddf6-4638-e0d5-137e47eca0e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Learning', 'NL', '##P', 'is', 'so', 'much', 'reward', '##ing']\n"]}],"source":["tokens = tokenizer.tokenize(\"Learning NLP is so much rewarding\", )\n","pp.pprint(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjbnRP6CKJ21","outputId":"536bed26-5f59-42ed-9540-8ddd64df77f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158]\n"]}],"source":["ids = tokenizer.convert_tokens_to_ids(tokens)\n","pp.pprint(ids)"]},{"cell_type":"markdown","metadata":{"id":"2Snnn47mKJ22"},"source":["## Final touch-ups"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzPKWshAKJ23","outputId":"5dfc7c8b-a900-4531-e360-04438d839caf"},"outputs":[{"name":"stderr","output_type":"stream","text":["Keyword arguments {'add_special_tokens': True} not recognized.\n"]},{"name":"stdout","output_type":"stream","text":["['Learning', 'NL', '##P', 'is', 'so', 'much', 'reward', '##ing']\n","[9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158]\n","[101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102]\n"]}],"source":["tokens = tokenizer.tokenize(\"Learning NLP is so much rewarding\", add_special_tokens = True )\n","pp.pprint(tokens)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","pp.pprint(ids)\n","\n","final_ids = tokenizer.build_inputs_with_special_tokens(ids)\n","pp.pprint(final_ids)"]},{"cell_type":"markdown","metadata":{"id":"zTS7ET-DKJ24"},"source":["## Decoding the tokens ang getting back the string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQAmd08jKJ25","outputId":"be162621-2157-419a-ff8d-f5600e1247e1"},"outputs":[{"data":{"text/plain":["'[CLS] Learning NLP is so much rewarding [SEP]'"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(final_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhIIajB8KJ25","outputId":"a1eec02b-28a7-403d-dcf5-d8dc2b3065e9"},"outputs":[{"data":{"text/plain":["'Learning NLP is so much rewarding'"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(final_ids, skip_special_tokens=True)"]},{"cell_type":"markdown","metadata":{"id":"yp07lMdGKJ26"},"source":["## Handling Multiple sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIeraF_gKJ26"},"outputs":[],"source":["tokenized_output = tokenizer([\"Learning NLP is so much rewarding\",\"Another test sentence\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_jk8y8DKJ27","outputId":"15a670f6-4707-41f4-9bdd-51ddc8858ddc"},"outputs":[{"data":{"text/plain":["[[101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102],\n"," [101, 2543, 2774, 5650, 102]]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_output['input_ids']"]},{"cell_type":"markdown","metadata":{"id":"I91YbP0pKJ27"},"source":["### Padding"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"LZpXf__lKJ27","outputId":"fd2d8dee-b3c5-424f-bd79-a532d61f7cc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]],\n"," 'input_ids': [[101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102],\n","               [101, 2543, 2774, 5650, 102, 0, 0, 0, 0, 0]],\n"," 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n","\n","\n","{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","                    [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n"," 'input_ids': [[101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","               [101, 2543, 2774, 5650, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n"," 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n","\n","\n","{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0]],\n"," 'input_ids': [[101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102], [101, 2543, 2774, 5650, 102, 0]],\n"," 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]}\n"]}],"source":["sequences = [\"Learning NLP is so much rewarding\",\"Another test sentence\"]\n","# Will pad the sequences up to the maximum sequence length\n","model_inputs = tokenizer(sequences, padding=\"longest\")\n","pp.pprint(model_inputs)\n","# Will pad the sequences up to the model max length\n","# (512 for BERT or DistilBERT)\n","print('\\n')\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","pp.pprint(model_inputs)\n","\n","# Will pad the sequences up to the specified max length\n","print('\\n')\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=6)\n","pp.pprint(model_inputs)\n"]},{"cell_type":"markdown","metadata":{"id":"HOK_MxVgKJ28"},"source":["### Truncation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blcQ7k8LKJ28","outputId":"534dbc15-02fe-4fdf-87a8-076c4d51c8c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],\n"," 'input_ids': [[101, 9681, 21239, 2101, 1110, 1177, 1277, 10703, 1158, 102], [101, 2543, 2774, 5650, 102]],\n"," 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]}\n","\n","\n","{'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],\n"," 'input_ids': [[101, 9681, 21239, 2101, 1110, 102], [101, 2543, 2774, 5650, 102]],\n"," 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]}\n","\n","\n"]}],"source":["# Will truncate the sequences that are longer than the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, truncation=True)\n","pp.pprint(model_inputs)\n","print(\"\\n\")\n","# Will truncate the sequences that are longer than the specified max length\n","model_inputs = tokenizer(sequences, max_length=6, truncation=True)\n","pp.pprint(model_inputs)\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"sLH6yupKKJ29"},"source":["### Different Output Types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETTNjzjCKJ29","outputId":"51e8d4f6-d2bd-44d1-8fcb-82cdffd53096"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'attention_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])>,\n"," 'input_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n","array([[  101,  9681, 21239,  2101,  1110,  1177,  1277, 10703,  1158,\n","          102],\n","       [  101,  2543,  2774,  5650,   102,     0,     0,     0,     0,\n","            0]])>,\n"," 'token_type_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n","array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n","\n","\n","{'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]),\n"," 'input_ids': array([[  101,  9681, 21239,  2101,  1110,  1177,  1277, 10703,  1158,\n","          102],\n","       [  101,  2543,  2774,  5650,   102,     0,     0,     0,     0,\n","            0]]),\n"," 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","\n","\n"]}],"source":["# sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Returns PyTorch tensors\n","# model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n","\n","# Returns TensorFlow tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n","pp.pprint(model_inputs)\n","print(\"\\n\")\n","\n","# Returns NumPy arrays\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n","pp.pprint(model_inputs)\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kykRlnfLKJ2-","outputId":"c5991da4-e587-42d5-8975-99a26ec7ca3b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_250']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import tensorflow as tf\n","from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\"Learning NLP is so much rewarding\",\"Another test sentence\"]\n","\n","tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n","output = model(**tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5meMp9a-KJ3E","outputId":"f8e7cf74-4c6a-43c8-cb19-17fa3bd6e57c"},"outputs":[{"data":{"text/plain":["{'input_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n","array([[  101,  4083, 17953,  2361,  2003,  2061,  2172, 10377,  2075,\n","          102],\n","       [  101,  2178,  3231,  6251,   102,     0,     0,     0,     0,\n","            0]])>, 'attention_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])>}"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"381SVMyoKJ3E","outputId":"0d7fa404-ba7d-4d47-9aa9-b7228d596f90"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n","array([[1.20997014e-04, 9.99879003e-01],\n","       [9.92225170e-01, 7.77476979e-03]], dtype=float32)>"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["tf.math.softmax(output.logits, axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8ASe6rPKJ3F"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["I91YbP0pKJ27","HOK_MxVgKJ28","sLH6yupKKJ29"]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"1a87c10f6db3436ba9d9d4f13d1b810a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f264a70b5fb946b0bfaab26bfc23d114","placeholder":"​","style":"IPY_MODEL_468b86823a6c4634ae32ec2cfcf9c67b","value":""}},"253452c3306a465793a888f190d102c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"468b86823a6c4634ae32ec2cfcf9c67b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61e5ad3b46844668a95c8453ade0731b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"818b28edbcb247ff96ce01e8442d035f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85bfcc93f3334955bbce2ce55221b200":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"999e0dc52e474302965b187498f6378f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b29c759304ec4241b9207bf94a43098f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85bfcc93f3334955bbce2ce55221b200","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_999e0dc52e474302965b187498f6378f","value":0}},"d01eeb56af424bed8d6017c79d692ef4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61e5ad3b46844668a95c8453ade0731b","placeholder":"​","style":"IPY_MODEL_253452c3306a465793a888f190d102c0","value":" 0/0 [00:00&lt;?, ?it/s]"}},"d327ddcf259148a08fb9edc59d2ceb66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a87c10f6db3436ba9d9d4f13d1b810a","IPY_MODEL_b29c759304ec4241b9207bf94a43098f","IPY_MODEL_d01eeb56af424bed8d6017c79d692ef4"],"layout":"IPY_MODEL_818b28edbcb247ff96ce01e8442d035f"}},"f264a70b5fb946b0bfaab26bfc23d114":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}