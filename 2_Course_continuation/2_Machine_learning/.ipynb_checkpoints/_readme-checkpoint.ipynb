{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f608d662",
   "metadata": {},
   "source": [
    "### Structure of the course\n",
    "<img src=\"_readme/GetImage.png\" />\n",
    "<img src=\"https://images.squarespace-cdn.com/content/v1/58dc1a1ee4fcb51cbb80a096/24c2f0fb-e667-42d6-a6fb-b2c4ad79ed41/ML-CheatSheet-en.png\" />\n",
    "<img src=\"https://www.xponance.com/wp-content/uploads/2021/09/Chart1-and-Table1-1024x590.png\" />\n",
    "\n",
    "### Agenda: \n",
    "    Module 1: Linear Regression (LR) \n",
    "        Intro to SLR (Simple LR) \n",
    "            Intro to ML \n",
    "            Regression line \n",
    "            Best-fit line \n",
    "        SLR in Python \n",
    "            Assumptions of SLR \n",
    "            Reading and understanding the data \n",
    "            Hypothesis testing in LR \n",
    "            Building a linear model \n",
    "            Residual analysis and Predictions \n",
    "            LR using SKLearn \n",
    "\n",
    "        MLR (Multi-Linear Regression) \n",
    "            New considerations \n",
    "            Multicollinearity \n",
    "            Dealing with Categorical variables \n",
    "            Model Assessment and Comparison \n",
    "            Feature selection \n",
    "       MLR in Python: \n",
    "           Reading and Understanding data \n",
    "           Data preparation \n",
    "           Initial steps \n",
    "           Building the model \n",
    "           Residual analysis and Predictions \n",
    "           Variable selection using RFE (Recursive Feature Elimination)\n",
    "           \n",
    "### Notes\n",
    "1. SLR and MLR (Single LR and Multi LR):<br>\n",
    "SLR:  \n",
    "\n",
    "GitHub - ContentUpgrad/Linear-Regression: \n",
    "\n",
    "https://github.com/ContentUpgrad/Linear-Regression \n",
    "\n",
    "Read this  (https://www.mathsisfun.com/equation_of_line.html ) link to revise the physical significance of an equation of a straight line, and also to understand how to find the slope and intercept of a straight line from its graph. \n",
    "\n",
    "Why does the test statistic for β1 follow a t-distribution instead of a normal distribution? (Hypothesis testing in linear regression part 2) \n",
    "\n",
    "The calculation of F-statistic is a complex task and is not required. Hence it is out of the scope of this course. But interested students can check out this link : https://en.wikipedia.org/wiki/F-test. \n",
    "\n",
    " \n",
    "\n",
    "MLR: \n",
    "\n",
    "Y=β0+β1X1+β2X2+...+βpXp+ϵ \n",
    "\n",
    "In the link below, you can understand more about the overfitting concept. \n",
    "\n",
    "Overfitting : https://elitedatascience.com/overfitting-in-machine-learning \n",
    "\n",
    "Partial Least Squares (PLS) : https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/partial-least-squares-regression/what-is-partial-least-squares-regression/ \n",
    "\n",
    "2. R-squared vs Adjusted R-squared:<br>\n",
    " The major difference between R-squared and Adjusted R-squared is that R-squared doesn't penalise the model for having more number of variables. Thus, if you keep on adding variables to the model, the R-squared will always increase (or remain the same in the case when the value of correlation between that variable and the dependent variable is zero). Thus, R-squared assumes that any variable added to the model will increase the predictive power. \n",
    "\n",
    "Adjusted R-squared on the other hand, penalises models based on the number of variables present in it. So if you add a variable and the Adjusted R-squared drops, you can be certain that that variable is insignificant to the model and shouldn't be used. So in the case of multiple linear regression, you should always look at the adjusted R-squared value in order to keep redundant variables out from your regression model. \n",
    "\n",
    "3. Additional Reading \n",
    "\n",
    "To know more about dummy variables (https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-dummy-coding/) \n",
    "\n",
    "Why it's necessary to create dummy variables (https://stats.stackexchange.com/questions/89533/convert-a-categorical-variable-to-a-numerical-variable-prior-to-regression) \n",
    "\n",
    "When to Normalise data and when to standardise? (https://stackoverflow.com/questions/32108179/linear-regression-normalization-vs-standardization) \n",
    "\n",
    "Various scaling techniques (https://en.wikipedia.org/wiki/Feature_scaling) \n",
    " \n",
    "\n",
    "The following links provide a detail study on AIC and other parameters used in automatic feature selection : \n",
    "\n",
    "AIC : https://en.wikipedia.org/wiki/Akaike_information_criterion \n",
    "\n",
    "BIC : https://en.wikipedia.org/wiki/Bayesian_information_criterion \n",
    "\n",
    "Mallows' CP : https://en.wikipedia.org/wiki/Mallows%27s_Cp \n",
    "\n",
    " \n",
    "### Instructors notes link\n",
    " \n",
    "https://github.com/ContentUpgrad/Linear-Regression\n",
    "           \n",
    "### Assignment -2 (Linear regression (Bike sharing) assignment)\n",
    "\n",
    "Linear regression assignment:\n",
    "https://www.dropbox.com/scl/fo/vcj45zteqhi0qr9pl1xr8/h?dl=0&rlkey=aglrkxuyir3dob9t7j3fjesql\n",
    "\n",
    "https://www.kaggle.com/code/lakshmi25npathi/bike-rental-count-prediction-using-python/notebook\n",
    "\n",
    "Shared by professor: \n",
    "https://drive.google.com/drive/folders/1VjrHKtgjLzWk9W_ImvJnyuw37XCr-iQS\n",
    "\n",
    "<br>Location of assignment: http://localhost:8889/tree/2_Course_continuation/2_Machine_learning/1_Linear_regression/2_Bike_sharing_assignment\n",
    "\n",
    "\n",
    "\n",
    "### Logistic regression\n",
    "1. \n",
    "<img src=\"_readme/GetImage_1.png\" />\n",
    "2. \n",
    "<img src=\"_readme/GetImage_2.png\" />\n",
    "3. \n",
    "<img src=\"_readme/GetImage_3.png\" />\n",
    "4. Logistic function:\n",
    "<img src=\"_readme/GetImage_4.png\" />\n",
    "5. Odds and log odds:\n",
    "<img src=\"_readme/GetImage_5.png\" />\n",
    "<img src=\"_readme/GetImage_6.png\" />\n",
    "6. Confusion matrix:\n",
    "<img src=\"_readme/GetImage_7.jpg\" />\n",
    "\n",
    "\n",
    "7. Shared by professor:\n",
    "https://drive.google.com/drive/folders/1KGdUhqSBOjEmOeKdkmNL5s328eagkSTP\n",
    "\n",
    "8. Notes:<br>\n",
    "    i. The formula for standardising a value in a dataset is given by: (X−μ)/ σ <br>\n",
    "    ii. For a variable to be insignificant, the p-value should be greater than 0.05. <br>\n",
    "    iii. <u>Likelihood function:</u>\n",
    "\n",
    "    So, the best fitting combination of β0 and β1 will be the one which maximises the product: \n",
    "    (1−P1)(1−P2)(1−P3)(1−P4)(1−P6)(P5)(P7)(P8)(P9)(P10) \n",
    "    This product is called the likelihood function. It is the product of: <br>\n",
    "    [(1−Pi)(1−Pi)------ for all non-diabetics --------] * [(Pi)(Pi) -------- for all diabetics -------]\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b620df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
